{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6","title":"Introduction"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"SparkCore/ShuffleService/","text":"Shuffle Service BlockTranserService Code Improve Spark shuffle server responsiveness to non-ChunkFetch requests push-based shuffle Use remote storage for persisting shuffle data Shuffle Service \u00b6 BlockTranserService \u00b6 Code \u00b6 /** * The BlockTransferService that used for fetching a set of blocks at time. Each instance of * BlockTransferService contains both client and server inside. */ private[spark] abstract class BlockTransferService extends BlockStoreClient { /** * A BlockTransferService that uses Netty to fetch a set of blocks at time. */ private[spark] class NettyBlockTransferService( conf: SparkConf, securityManager: SecurityManager, bindAddress: String, override val hostName: String, _port: Int, numCores: Int, driverEndPointRef: RpcEndpointRef = null) extends BlockTransferService { override def init(blockDataManager: BlockDataManager): Unit = { val rpcHandler = new NettyBlockRpcServer(conf.getAppId, serializer, blockDataManager) var serverBootstrap: Option[TransportServerBootstrap] = None var clientBootstrap: Option[TransportClientBootstrap] = None this.transportConf = SparkTransportConf.fromSparkConf(conf, \"shuffle\", numCores) if (authEnabled) { serverBootstrap = Some(new AuthServerBootstrap(transportConf, securityManager)) clientBootstrap = Some(new AuthClientBootstrap(transportConf, conf.getAppId, securityManager)) } transportContext = new TransportContext(transportConf, rpcHandler) clientFactory = transportContext.createClientFactory(clientBootstrap.toSeq.asJava) server = createServer(serverBootstrap.toList) appId = conf.getAppId if (hostName.equals(bindAddress)) { logger.info(s\"Server created on $hostName:${server.getPort}\") } else { logger.info(s\"Server created on $hostName $bindAddress:${server.getPort}\") } } Improve Spark shuffle server responsiveness to non-ChunkFetch requests \u00b6 SPARK-24355 Improve Spark shuffle server responsiveness to non-ChunkFetch requests SPARK-30512 Use a dedicated boss event group loop in the netty pipeline for external shuffle service SPARK-30623 Spark external shuffle allow disable of separate event loop group What changes were proposed in this pull request? Fix the regression caused by PR #22173. The original PR changes the logic of handling ChunkFetchReqeust from async to sync, that\u2019s causes the shuffle benchmark regression. This PR fixes the regression back to the async mode by reusing the config spark.shuffle.server.chunkFetchHandlerThreadsPercent . When the user sets the config, ChunkFetchReqeust will be processed in a separate event loop group, otherwise, the code path is exactly the same as before. Performance regression described in [comment](https://github.com/apache/spark/pull/22173#issuecomment-572459561 org.apache.spark.network.server.ChunkFetchRequestHandler#respond /** * The invocation to channel.writeAndFlush is async, and the actual I/O on the * channel will be handled by the EventLoop the channel is registered to. So even * though we are processing the ChunkFetchRequest in a separate thread pool, the actual I/O, * which is the potentially blocking call that could deplete server handler threads, is still * being processed by TransportServer's default EventLoopGroup. * * When syncModeEnabled is true, Spark will throttle the max number of threads that channel I/O * for sending response to ChunkFetchRequest, the thread calling channel.writeAndFlush will wait * for the completion of sending response back to client by invoking await(). This will throttle * the rate at which threads from ChunkFetchRequest dedicated EventLoopGroup submit channel I/O * requests to TransportServer's default EventLoopGroup, thus making sure that we can reserve * some threads in TransportServer's default EventLoopGroup for handling other RPC messages. */ private ChannelFuture respond( final Channel channel, final Encodable result) throws InterruptedException { final SocketAddress remoteAddress = channel.remoteAddress(); ChannelFuture channelFuture; if (syncModeEnabled) { channelFuture = channel.writeAndFlush(result).await(); } else { channelFuture = channel.writeAndFlush(result); } return channelFuture.addListener((ChannelFutureListener) future -> { if (future.isSuccess()) { logger.trace(\"Sent result {} to client {}\", result, remoteAddress); } else { logger.error(String.format(\"Error sending result %s to %s; closing connection\", result, remoteAddress), future.cause()); channel.close(); } }); } Q: Why await() needed? I think await does\u2019t provide any benefit and could be removed. When the chunk fetch event loop runs channel.writeAndFlush(result) This adds a WriteAndFlushTask in the pendingQueue of the default server-IO thread registered with that channel. The code in NioEventLoop.run() itself throttles the number of tasks that can be run at a time from its pending queue. Here is the code: final long ioStartTime = System.nanoTime(); try { processSelectedKeys(); } finally { // Ensure we always run tasks. final long ioTime = System.nanoTime() - ioStartTime; runAllTasks(ioTime * (100 - ioRatio) / ioRatio); } } Here it records how much time it took to perform the IO operations, that is, execute processSelectedKeys(). runAllTasks, which is the method that processes the tasks from pendingQueue, will be performed for the same amount of time. runAllTasks() does process 64 tasks and then checks the time. // Check timeout every 64 tasks because nanoTime() is relatively expensive. // XXX: Hard-coded value - will make it configurable if it is really a problem. if ((runTasks & 0x3F) == 0) { lastExecutionTime = ScheduledFutureTask.nanoTime(); if (lastExecutionTime >= deadline) { break; } } This ensures that the default server-IO thread always gets time to process the ready channels. Its not always busy processing WriteAndFlushTask Answer: I removed the await and tested with our internal stress testing framework. I started seeing SASL requests timing out. In this test, I observed more than 2 minutes delay between channel registration and when the first bytes are read from the channel. 2020-01-24 22:53:34,019 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] REGISTERED 2020-01-24 22:53:34,019 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] ACTIVE 2020-01-24 22:55:05,207 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] READ: 48B 2020-01-24 22:55:05,207 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] WRITE: org.apache.spark.network.protocol.MessageWithHeader@27e59ee9 2020-01-24 22:55:05,207 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] FLUSH 2020-01-24 22:55:05,207 INFO org.apache.spark.network.server.OutgoingChannelHandler: OUTPUT request 5929104419960968526 channel d475f5ff request_rec 1579906505207 transport_rec 1579906505207 flush 1579906505207 receive-transport 0 transport-flush 0 total 0 Since there is a delay in reading the channel, I suspect this is because the hardcoding in netty code SingleThreadEventExecutor.runAllTask() that checks time only after 64 tasks. WriteAndFlush tasks are bulky tasks. With await there will be just 1 WriteAndFlushTask per channel in the IO thread\u2019s pending queue and the rest of the tasks will be smaller tasks. However, without await there are more WriteAndFlush tasks per channel in the IO thread\u2019s queue. Since it processes 64 tasks and then checks time, this time increases with more WriteAndFlush tasks. / Check timeout every 64 tasks because nanoTime() is relatively expensive. // XXX: Hard-coded value - will make it configurable if it is really a problem. if ((runTasks & 0x3F) == 0) { lastExecutionTime = ScheduledFutureTask.nanoTime(); if (lastExecutionTime >= deadline) { break; } } I can test this theory by lowering this number in a fork of netty and building spark against it. However, for now we can\u2019t remove await(). Note: This test was with a dedicated boss event loop group which is why we don\u2019t see any delay in channel registration. push-based shuffle \u00b6 SPARK-30602 SPIP: Support push-based shuffle to improve shuffle efficiency doc: SPIP: Spark Push-Based Shuffle Consolidated reference PR for Push-based shuffle SPARK-32917 Add support for executors to push shuffle blocks after successful map task completion PR SPARK-32917 Use remote storage for persisting shuffle data \u00b6 architecture discussion - Use remote storage for persisting shuffle data SPIP: `SPARK-25299 - An API For Writing Shuffle Data To Remote Storage","title":"ShuffleService"},{"location":"SparkCore/ShuffleService/#shuffle-service","text":"","title":"Shuffle Service"},{"location":"SparkCore/ShuffleService/#blocktranserservice","text":"","title":"BlockTranserService"},{"location":"SparkCore/ShuffleService/#code","text":"/** * The BlockTransferService that used for fetching a set of blocks at time. Each instance of * BlockTransferService contains both client and server inside. */ private[spark] abstract class BlockTransferService extends BlockStoreClient { /** * A BlockTransferService that uses Netty to fetch a set of blocks at time. */ private[spark] class NettyBlockTransferService( conf: SparkConf, securityManager: SecurityManager, bindAddress: String, override val hostName: String, _port: Int, numCores: Int, driverEndPointRef: RpcEndpointRef = null) extends BlockTransferService { override def init(blockDataManager: BlockDataManager): Unit = { val rpcHandler = new NettyBlockRpcServer(conf.getAppId, serializer, blockDataManager) var serverBootstrap: Option[TransportServerBootstrap] = None var clientBootstrap: Option[TransportClientBootstrap] = None this.transportConf = SparkTransportConf.fromSparkConf(conf, \"shuffle\", numCores) if (authEnabled) { serverBootstrap = Some(new AuthServerBootstrap(transportConf, securityManager)) clientBootstrap = Some(new AuthClientBootstrap(transportConf, conf.getAppId, securityManager)) } transportContext = new TransportContext(transportConf, rpcHandler) clientFactory = transportContext.createClientFactory(clientBootstrap.toSeq.asJava) server = createServer(serverBootstrap.toList) appId = conf.getAppId if (hostName.equals(bindAddress)) { logger.info(s\"Server created on $hostName:${server.getPort}\") } else { logger.info(s\"Server created on $hostName $bindAddress:${server.getPort}\") } }","title":"Code"},{"location":"SparkCore/ShuffleService/#improve-spark-shuffle-server-responsiveness-to-non-chunkfetch-requests","text":"SPARK-24355 Improve Spark shuffle server responsiveness to non-ChunkFetch requests SPARK-30512 Use a dedicated boss event group loop in the netty pipeline for external shuffle service SPARK-30623 Spark external shuffle allow disable of separate event loop group What changes were proposed in this pull request? Fix the regression caused by PR #22173. The original PR changes the logic of handling ChunkFetchReqeust from async to sync, that\u2019s causes the shuffle benchmark regression. This PR fixes the regression back to the async mode by reusing the config spark.shuffle.server.chunkFetchHandlerThreadsPercent . When the user sets the config, ChunkFetchReqeust will be processed in a separate event loop group, otherwise, the code path is exactly the same as before. Performance regression described in [comment](https://github.com/apache/spark/pull/22173#issuecomment-572459561 org.apache.spark.network.server.ChunkFetchRequestHandler#respond /** * The invocation to channel.writeAndFlush is async, and the actual I/O on the * channel will be handled by the EventLoop the channel is registered to. So even * though we are processing the ChunkFetchRequest in a separate thread pool, the actual I/O, * which is the potentially blocking call that could deplete server handler threads, is still * being processed by TransportServer's default EventLoopGroup. * * When syncModeEnabled is true, Spark will throttle the max number of threads that channel I/O * for sending response to ChunkFetchRequest, the thread calling channel.writeAndFlush will wait * for the completion of sending response back to client by invoking await(). This will throttle * the rate at which threads from ChunkFetchRequest dedicated EventLoopGroup submit channel I/O * requests to TransportServer's default EventLoopGroup, thus making sure that we can reserve * some threads in TransportServer's default EventLoopGroup for handling other RPC messages. */ private ChannelFuture respond( final Channel channel, final Encodable result) throws InterruptedException { final SocketAddress remoteAddress = channel.remoteAddress(); ChannelFuture channelFuture; if (syncModeEnabled) { channelFuture = channel.writeAndFlush(result).await(); } else { channelFuture = channel.writeAndFlush(result); } return channelFuture.addListener((ChannelFutureListener) future -> { if (future.isSuccess()) { logger.trace(\"Sent result {} to client {}\", result, remoteAddress); } else { logger.error(String.format(\"Error sending result %s to %s; closing connection\", result, remoteAddress), future.cause()); channel.close(); } }); } Q: Why await() needed? I think await does\u2019t provide any benefit and could be removed. When the chunk fetch event loop runs channel.writeAndFlush(result) This adds a WriteAndFlushTask in the pendingQueue of the default server-IO thread registered with that channel. The code in NioEventLoop.run() itself throttles the number of tasks that can be run at a time from its pending queue. Here is the code: final long ioStartTime = System.nanoTime(); try { processSelectedKeys(); } finally { // Ensure we always run tasks. final long ioTime = System.nanoTime() - ioStartTime; runAllTasks(ioTime * (100 - ioRatio) / ioRatio); } } Here it records how much time it took to perform the IO operations, that is, execute processSelectedKeys(). runAllTasks, which is the method that processes the tasks from pendingQueue, will be performed for the same amount of time. runAllTasks() does process 64 tasks and then checks the time. // Check timeout every 64 tasks because nanoTime() is relatively expensive. // XXX: Hard-coded value - will make it configurable if it is really a problem. if ((runTasks & 0x3F) == 0) { lastExecutionTime = ScheduledFutureTask.nanoTime(); if (lastExecutionTime >= deadline) { break; } } This ensures that the default server-IO thread always gets time to process the ready channels. Its not always busy processing WriteAndFlushTask Answer: I removed the await and tested with our internal stress testing framework. I started seeing SASL requests timing out. In this test, I observed more than 2 minutes delay between channel registration and when the first bytes are read from the channel. 2020-01-24 22:53:34,019 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] REGISTERED 2020-01-24 22:53:34,019 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] ACTIVE 2020-01-24 22:55:05,207 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] READ: 48B 2020-01-24 22:55:05,207 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] WRITE: org.apache.spark.network.protocol.MessageWithHeader@27e59ee9 2020-01-24 22:55:05,207 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] FLUSH 2020-01-24 22:55:05,207 INFO org.apache.spark.network.server.OutgoingChannelHandler: OUTPUT request 5929104419960968526 channel d475f5ff request_rec 1579906505207 transport_rec 1579906505207 flush 1579906505207 receive-transport 0 transport-flush 0 total 0 Since there is a delay in reading the channel, I suspect this is because the hardcoding in netty code SingleThreadEventExecutor.runAllTask() that checks time only after 64 tasks. WriteAndFlush tasks are bulky tasks. With await there will be just 1 WriteAndFlushTask per channel in the IO thread\u2019s pending queue and the rest of the tasks will be smaller tasks. However, without await there are more WriteAndFlush tasks per channel in the IO thread\u2019s queue. Since it processes 64 tasks and then checks time, this time increases with more WriteAndFlush tasks. / Check timeout every 64 tasks because nanoTime() is relatively expensive. // XXX: Hard-coded value - will make it configurable if it is really a problem. if ((runTasks & 0x3F) == 0) { lastExecutionTime = ScheduledFutureTask.nanoTime(); if (lastExecutionTime >= deadline) { break; } } I can test this theory by lowering this number in a fork of netty and building spark against it. However, for now we can\u2019t remove await(). Note: This test was with a dedicated boss event loop group which is why we don\u2019t see any delay in channel registration.","title":"Improve Spark shuffle server responsiveness to non-ChunkFetch requests"},{"location":"SparkCore/ShuffleService/#push-based-shuffle","text":"SPARK-30602 SPIP: Support push-based shuffle to improve shuffle efficiency doc: SPIP: Spark Push-Based Shuffle Consolidated reference PR for Push-based shuffle SPARK-32917 Add support for executors to push shuffle blocks after successful map task completion PR SPARK-32917","title":"push-based shuffle"},{"location":"SparkCore/ShuffleService/#use-remote-storage-for-persisting-shuffle-data","text":"architecture discussion - Use remote storage for persisting shuffle data SPIP: `SPARK-25299 - An API For Writing Shuffle Data To Remote Storage","title":"Use remote storage for persisting shuffle data"},{"location":"SparkCore/TaskScheduling/","text":"TaskSchedulerImpl resourceOffers handle failed task TaskSchedulerImpl \u00b6 resourceOffers \u00b6 org.apache.spark.scheduler.TaskSchedulerImpl#resourceOffers=> org.apache.spark.scheduler.TaskSchedulerImpl#resourceOfferSingleTaskSet=> org.apache.spark.scheduler.TaskSetManager#resourceOffer org.apache.spark.scheduler.TaskSchedulerImpl#resourceOffers /** * Called by cluster manager to offer resources on workers. We respond by asking our active task * sets for tasks in order of priority. We fill each node with tasks in a round-robin manner so * that tasks are balanced across the cluster. */ def resourceOffers( offers: IndexedSeq[WorkerOffer], isAllFreeResources: Boolean = true): Seq[Seq[TaskDescription]] = synchronized { ... val shuffledOffers = shuffleOffers(filteredOffers) ===> shuffle WorkerOffers to avoid always placing tasks on the same workers // Build a list of tasks to assign to each worker. // Note the size estimate here might be off with different ResourceProfiles but should be // close estimate val tasks = shuffledOffers.map(o => new ArrayBuffer[TaskDescription](o.cores / CPUS_PER_TASK)) val availableResources = shuffledOffers.map(_.resources).toArray val availableCpus = shuffledOffers.map(o => o.cores).toArray val resourceProfileIds = shuffledOffers.map(o => o.resourceProfileId).toArray val sortedTaskSets = rootPool.getSortedTaskSetQueue for (taskSet <- sortedTaskSets) { logDebug(\"parentName: %s, name: %s, runningTasks: %s\".format( taskSet.parent.name, taskSet.name, taskSet.runningTasks)) if (newExecAvail) { taskSet.executorAdded() } } // Take each TaskSet in our scheduling order, and then offer it to each node in increasing order // of locality levels so that it gets a chance to launch local tasks on all of them. // NOTE: the preferredLocality order: PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY for (taskSet <- sortedTaskSets) { ... } else { var launchedAnyTask = false var noDelaySchedulingRejects = true var globalMinLocality: Option[TaskLocality] = None for (currentMaxLocality <- taskSet.myLocalityLevels) { var launchedTaskAtCurrentMaxLocality = false do { val (noDelayScheduleReject, minLocality) = resourceOfferSingleTaskSet( taskSet, currentMaxLocality, shuffledOffers, availableCpus, availableResources, tasks) launchedTaskAtCurrentMaxLocality = minLocality.isDefined launchedAnyTask |= launchedTaskAtCurrentMaxLocality noDelaySchedulingRejects &= noDelayScheduleReject globalMinLocality = minTaskLocality(globalMinLocality, minLocality) } while (launchedTaskAtCurrentMaxLocality) } /** * Shuffle offers around to avoid always placing tasks on the same workers. Exposed to allow * overriding in tests, so it can be deterministic. */ protected def shuffleOffers(offers: IndexedSeq[WorkerOffer]): IndexedSeq[WorkerOffer] = { Random.shuffle(offers) } org.apache.spark.scheduler.TaskSchedulerImpl#resourceOfferSingleTaskSet /** * Offers resources to a single [[TaskSetManager]] at a given max allowed [[TaskLocality]]. * * @param taskSet task set manager to offer resources to * @param maxLocality max locality to allow when scheduling * @param shuffledOffers shuffled resource offers to use for scheduling, * remaining resources are tracked by below fields as tasks are scheduled * @param availableCpus remaining cpus per offer, * value at index 'i' corresponds to shuffledOffers[i] * @param availableResources remaining resources per offer, * value at index 'i' corresponds to shuffledOffers[i] * @param tasks tasks scheduled per offer, value at index 'i' corresponds to shuffledOffers[i] * @return tuple of (no delay schedule rejects?, option of min locality of launched task) */ private def resourceOfferSingleTaskSet( taskSet: TaskSetManager, maxLocality: TaskLocality, shuffledOffers: Seq[WorkerOffer], availableCpus: Array[Int], availableResources: Array[Map[String, Buffer[String]]], tasks: IndexedSeq[ArrayBuffer[TaskDescription]]) : (Boolean, Option[TaskLocality]) = { var noDelayScheduleRejects = true var minLaunchedLocality: Option[TaskLocality] = None // nodes and executors that are excluded for the entire application have already been // filtered out by this point for (i <- shuffledOffers.indices) { ======> round robin to assign a task per offer/executor val execId = shuffledOffers(i).executorId val host = shuffledOffers(i).host val taskSetRpID = taskSet.taskSet.resourceProfileId // check whether the task can be scheduled to the executor base on resource profile. if (sc.resourceProfileManager .canBeScheduled(taskSetRpID, shuffledOffers(i).resourceProfileId)) { val taskResAssignmentsOpt = resourcesMeetTaskRequirements(taskSet, availableCpus(i), availableResources(i)) ===> Check whether the resources from the WorkerOffer are enough to run at least one task. taskResAssignmentsOpt.foreach { taskResAssignments => try { val prof = sc.resourceProfileManager.resourceProfileFromId(taskSetRpID) val taskCpus = ResourceProfile.getTaskCpusOrDefaultForProfile(prof, conf) val (taskDescOption, didReject, index) = taskSet.resourceOffer(execId, host, maxLocality, taskCpus, taskResAssignments) ===> Respond to an offer of a single executor from the scheduler by finding a task noDelayScheduleRejects &= !didReject for (task <- taskDescOption) { val (locality, resources) = if (task != null) { tasks(i) += task addRunningTask(task.taskId, execId, taskSet) (taskSet.taskInfos(task.taskId).taskLocality, task.resources) } else { assert(taskSet.isBarrier, \"TaskDescription can only be null for barrier task\") val barrierTask = taskSet.barrierPendingLaunchTasks(index) barrierTask.assignedOfferIndex = i barrierTask.assignedCores = taskCpus (barrierTask.taskLocality, barrierTask.assignedResources) } minLaunchedLocality = minTaskLocality(minLaunchedLocality, Some(locality)) availableCpus(i) -= taskCpus assert(availableCpus(i) >= 0) resources.foreach { case (rName, rInfo) => // Remove the first n elements from availableResources addresses, these removed // addresses are the same as that we allocated in taskResourceAssignments since it's // synchronized. We don't remove the exact addresses allocated because the current // approach produces the identical result with less time complexity. availableResources(i)(rName).remove(0, rInfo.addresses.size) } } } catch { case e: TaskNotSerializableException => logError(s\"Resource offer failed, task set ${taskSet.name} was not serializable\") // Do not offer resources for this task, but don't throw an error to allow other // task sets to be submitted. return (noDelayScheduleRejects, minLaunchedLocality) } } } } (noDelayScheduleRejects, minLaunchedLocality) } org.apache.spark.scheduler.TaskSetManager#resourceOffer /** * Respond to an offer of a single executor from the scheduler by finding a task * * NOTE: this function is either called with a maxLocality which * would be adjusted by delay scheduling algorithm or it will be with a special * NO_PREF locality which will be not modified * * @param execId the executor Id of the offered resource * @param host the host Id of the offered resource * @param maxLocality the maximum locality we want to schedule the tasks at * @param taskCpus the number of CPUs for the task * @param taskResourceAssignments the resource assignments for the task * * @return Triple containing: * (TaskDescription of launched task if any, * rejected resource due to delay scheduling?, * dequeued task index) */ @throws[TaskNotSerializableException] def resourceOffer( execId: String, host: String, maxLocality: TaskLocality.TaskLocality, taskCpus: Int = sched.CPUS_PER_TASK, taskResourceAssignments: Map[String, ResourceInformation] = Map.empty) : (Option[TaskDescription], Boolean, Int) org.apache.spark.scheduler.TaskDescription /** * Description of a task that gets passed onto executors to be executed, usually created by * `TaskSetManager.resourceOffer`. * * TaskDescriptions and the associated Task need to be serialized carefully for two reasons: * * (1) When a TaskDescription is received by an Executor, the Executor needs to first get the * list of JARs and files and add these to the classpath, and set the properties, before * deserializing the Task object (serializedTask). This is why the Properties are included * in the TaskDescription, even though they're also in the serialized task. * (2) Because a TaskDescription is serialized and sent to an executor for each task, efficient * serialization (both in terms of serialization time and serialized buffer size) is * important. For this reason, we serialize TaskDescriptions ourselves with the * TaskDescription.encode and TaskDescription.decode methods. This results in a smaller * serialized size because it avoids serializing unnecessary fields in the Map objects * (which can introduce significant overhead when the maps are small). */ private[spark] class TaskDescription( val taskId: Long, val attemptNumber: Int, ===> how many times this task has been attempted (0 for the first attempt) val executorId: String, val name: String, val index: Int, // Index within this task's TaskSet val partitionId: Int, val addedFiles: Map[String, Long], val addedJars: Map[String, Long], val addedArchives: Map[String, Long], val properties: Properties, val cpus: Int, val resources: immutable.Map[String, ResourceInformation], val serializedTask: ByteBuffer) { assert(cpus > 0, \"CPUs per task should be > 0\") override def toString: String = s\"TaskDescription($name)\" } handle failed task \u00b6 org.apache.spark.scheduler.TaskSchedulerImpl#handleFailedTask=> org.apache.spark.scheduler.TaskSetManager#handleFailedTask=> org.apache.spark.scheduler.DAGScheduler#taskEnded org.apache.spark.scheduler.TaskSetManager#handleFailedTask /** * Marks the task as failed, re-adds it to the list of pending tasks, and notifies the * DAG Scheduler. */ def handleFailedTask(tid: Long, state: TaskState, reason: TaskFailedReason): Unit = { val info = taskInfos(tid) // SPARK-37300: when the task was already finished state, just ignore it, // so that there won't cause copiesRunning wrong result. if (info.finished) { return } removeRunningTask(tid) info.markFinished(state, clock.getTimeMillis()) val index = info.index copiesRunning(index) -= 1 var accumUpdates: Seq[AccumulatorV2[_, _]] = Seq.empty var metricPeaks: Array[Long] = Array.empty val failureReason = s\"Lost ${taskName(tid)} (${info.host} \" + s\"executor ${info.executorId}): ${reason.toErrorString}\" val failureException: Option[Throwable] = reason match { case fetchFailed: FetchFailed => logWarning(failureReason) if (!successful(index)) { successful(index) = true tasksSuccessful += 1 } isZombie = true if (fetchFailed.bmAddress != null) { healthTracker.foreach(_.updateExcludedForFetchFailure( fetchFailed.bmAddress.host, fetchFailed.bmAddress.executorId)) } None case ef: ExceptionFailure => // ExceptionFailure's might have accumulator updates accumUpdates = ef.accums metricPeaks = ef.metricPeaks.toArray val task = taskName(tid) if (ef.className == classOf[NotSerializableException].getName) { // If the task result wasn't serializable, there's no point in trying to re-execute it. logError(s\"$task had a not serializable result: ${ef.description}; not retrying\") sched.dagScheduler.taskEnded(tasks(index), reason, null, accumUpdates, metricPeaks, info) abort(s\"$task had a not serializable result: ${ef.description}\") return } if (ef.className == classOf[TaskOutputFileAlreadyExistException].getName) { // If we can not write to output file in the task, there's no point in trying to // re-execute it. logError(\"Task %s in stage %s (TID %d) can not write to output file: %s; not retrying\" .format(info.id, taskSet.id, tid, ef.description)) sched.dagScheduler.taskEnded(tasks(index), reason, null, accumUpdates, metricPeaks, info) abort(\"Task %s in stage %s (TID %d) can not write to output file: %s\".format( info.id, taskSet.id, tid, ef.description)) return } val key = ef.description val now = clock.getTimeMillis() val (printFull, dupCount) = { if (recentExceptions.contains(key)) { val (dupCount, printTime) = recentExceptions(key) if (now - printTime > EXCEPTION_PRINT_INTERVAL) { recentExceptions(key) = (0, now) (true, 0) } else { recentExceptions(key) = (dupCount + 1, printTime) (false, dupCount + 1) } } else { recentExceptions(key) = (0, now) (true, 0) } } if (printFull) { logWarning(failureReason) } else { logInfo( s\"Lost $task on ${info.host}, executor ${info.executorId}: \" + s\"${ef.className} (${ef.description}) [duplicate $dupCount]\") } ef.exception case tk: TaskKilled => // TaskKilled might have accumulator updates accumUpdates = tk.accums metricPeaks = tk.metricPeaks.toArray logWarning(failureReason) None case e: ExecutorLostFailure if !e.exitCausedByApp => logInfo(s\"${taskName(tid)} failed because while it was being computed, its executor \" + \"exited for a reason unrelated to the task. Not counting this failure towards the \" + \"maximum number of failures for the task.\") None case e: TaskFailedReason => // TaskResultLost and others logWarning(failureReason) None } if (tasks(index).isBarrier) { isZombie = true } sched.dagScheduler.taskEnded(tasks(index), reason, null, accumUpdates, metricPeaks, info) if (!isZombie && reason.countTowardsTaskFailures) { assert (null != failureReason) taskSetExcludelistHelperOpt.foreach(_.updateExcludedForFailedTask( info.host, info.executorId, index, failureReason)) numFailures(index) += 1 if (numFailures(index) >= maxTaskFailures) { logError(\"Task %d in stage %s failed %d times; aborting job\".format( index, taskSet.id, maxTaskFailures)) abort(\"Task %d in stage %s failed %d times, most recent failure: %s\\nDriver stacktrace:\" .format(index, taskSet.id, maxTaskFailures, failureReason), failureException) return } } if (successful(index)) { logInfo(s\"${taskName(info.taskId)} failed, but the task will not\" + \" be re-executed (either because the task failed with a shuffle data fetch failure,\" + \" so the previous stage needs to be re-run, or because a different copy of the task\" + \" has already succeeded).\") } else { addPendingTask(index) } maybeFinishTaskSet() }","title":"TaskScheduling"},{"location":"SparkCore/TaskScheduling/#taskschedulerimpl","text":"","title":"TaskSchedulerImpl"},{"location":"SparkCore/TaskScheduling/#resourceoffers","text":"org.apache.spark.scheduler.TaskSchedulerImpl#resourceOffers=> org.apache.spark.scheduler.TaskSchedulerImpl#resourceOfferSingleTaskSet=> org.apache.spark.scheduler.TaskSetManager#resourceOffer org.apache.spark.scheduler.TaskSchedulerImpl#resourceOffers /** * Called by cluster manager to offer resources on workers. We respond by asking our active task * sets for tasks in order of priority. We fill each node with tasks in a round-robin manner so * that tasks are balanced across the cluster. */ def resourceOffers( offers: IndexedSeq[WorkerOffer], isAllFreeResources: Boolean = true): Seq[Seq[TaskDescription]] = synchronized { ... val shuffledOffers = shuffleOffers(filteredOffers) ===> shuffle WorkerOffers to avoid always placing tasks on the same workers // Build a list of tasks to assign to each worker. // Note the size estimate here might be off with different ResourceProfiles but should be // close estimate val tasks = shuffledOffers.map(o => new ArrayBuffer[TaskDescription](o.cores / CPUS_PER_TASK)) val availableResources = shuffledOffers.map(_.resources).toArray val availableCpus = shuffledOffers.map(o => o.cores).toArray val resourceProfileIds = shuffledOffers.map(o => o.resourceProfileId).toArray val sortedTaskSets = rootPool.getSortedTaskSetQueue for (taskSet <- sortedTaskSets) { logDebug(\"parentName: %s, name: %s, runningTasks: %s\".format( taskSet.parent.name, taskSet.name, taskSet.runningTasks)) if (newExecAvail) { taskSet.executorAdded() } } // Take each TaskSet in our scheduling order, and then offer it to each node in increasing order // of locality levels so that it gets a chance to launch local tasks on all of them. // NOTE: the preferredLocality order: PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY for (taskSet <- sortedTaskSets) { ... } else { var launchedAnyTask = false var noDelaySchedulingRejects = true var globalMinLocality: Option[TaskLocality] = None for (currentMaxLocality <- taskSet.myLocalityLevels) { var launchedTaskAtCurrentMaxLocality = false do { val (noDelayScheduleReject, minLocality) = resourceOfferSingleTaskSet( taskSet, currentMaxLocality, shuffledOffers, availableCpus, availableResources, tasks) launchedTaskAtCurrentMaxLocality = minLocality.isDefined launchedAnyTask |= launchedTaskAtCurrentMaxLocality noDelaySchedulingRejects &= noDelayScheduleReject globalMinLocality = minTaskLocality(globalMinLocality, minLocality) } while (launchedTaskAtCurrentMaxLocality) } /** * Shuffle offers around to avoid always placing tasks on the same workers. Exposed to allow * overriding in tests, so it can be deterministic. */ protected def shuffleOffers(offers: IndexedSeq[WorkerOffer]): IndexedSeq[WorkerOffer] = { Random.shuffle(offers) } org.apache.spark.scheduler.TaskSchedulerImpl#resourceOfferSingleTaskSet /** * Offers resources to a single [[TaskSetManager]] at a given max allowed [[TaskLocality]]. * * @param taskSet task set manager to offer resources to * @param maxLocality max locality to allow when scheduling * @param shuffledOffers shuffled resource offers to use for scheduling, * remaining resources are tracked by below fields as tasks are scheduled * @param availableCpus remaining cpus per offer, * value at index 'i' corresponds to shuffledOffers[i] * @param availableResources remaining resources per offer, * value at index 'i' corresponds to shuffledOffers[i] * @param tasks tasks scheduled per offer, value at index 'i' corresponds to shuffledOffers[i] * @return tuple of (no delay schedule rejects?, option of min locality of launched task) */ private def resourceOfferSingleTaskSet( taskSet: TaskSetManager, maxLocality: TaskLocality, shuffledOffers: Seq[WorkerOffer], availableCpus: Array[Int], availableResources: Array[Map[String, Buffer[String]]], tasks: IndexedSeq[ArrayBuffer[TaskDescription]]) : (Boolean, Option[TaskLocality]) = { var noDelayScheduleRejects = true var minLaunchedLocality: Option[TaskLocality] = None // nodes and executors that are excluded for the entire application have already been // filtered out by this point for (i <- shuffledOffers.indices) { ======> round robin to assign a task per offer/executor val execId = shuffledOffers(i).executorId val host = shuffledOffers(i).host val taskSetRpID = taskSet.taskSet.resourceProfileId // check whether the task can be scheduled to the executor base on resource profile. if (sc.resourceProfileManager .canBeScheduled(taskSetRpID, shuffledOffers(i).resourceProfileId)) { val taskResAssignmentsOpt = resourcesMeetTaskRequirements(taskSet, availableCpus(i), availableResources(i)) ===> Check whether the resources from the WorkerOffer are enough to run at least one task. taskResAssignmentsOpt.foreach { taskResAssignments => try { val prof = sc.resourceProfileManager.resourceProfileFromId(taskSetRpID) val taskCpus = ResourceProfile.getTaskCpusOrDefaultForProfile(prof, conf) val (taskDescOption, didReject, index) = taskSet.resourceOffer(execId, host, maxLocality, taskCpus, taskResAssignments) ===> Respond to an offer of a single executor from the scheduler by finding a task noDelayScheduleRejects &= !didReject for (task <- taskDescOption) { val (locality, resources) = if (task != null) { tasks(i) += task addRunningTask(task.taskId, execId, taskSet) (taskSet.taskInfos(task.taskId).taskLocality, task.resources) } else { assert(taskSet.isBarrier, \"TaskDescription can only be null for barrier task\") val barrierTask = taskSet.barrierPendingLaunchTasks(index) barrierTask.assignedOfferIndex = i barrierTask.assignedCores = taskCpus (barrierTask.taskLocality, barrierTask.assignedResources) } minLaunchedLocality = minTaskLocality(minLaunchedLocality, Some(locality)) availableCpus(i) -= taskCpus assert(availableCpus(i) >= 0) resources.foreach { case (rName, rInfo) => // Remove the first n elements from availableResources addresses, these removed // addresses are the same as that we allocated in taskResourceAssignments since it's // synchronized. We don't remove the exact addresses allocated because the current // approach produces the identical result with less time complexity. availableResources(i)(rName).remove(0, rInfo.addresses.size) } } } catch { case e: TaskNotSerializableException => logError(s\"Resource offer failed, task set ${taskSet.name} was not serializable\") // Do not offer resources for this task, but don't throw an error to allow other // task sets to be submitted. return (noDelayScheduleRejects, minLaunchedLocality) } } } } (noDelayScheduleRejects, minLaunchedLocality) } org.apache.spark.scheduler.TaskSetManager#resourceOffer /** * Respond to an offer of a single executor from the scheduler by finding a task * * NOTE: this function is either called with a maxLocality which * would be adjusted by delay scheduling algorithm or it will be with a special * NO_PREF locality which will be not modified * * @param execId the executor Id of the offered resource * @param host the host Id of the offered resource * @param maxLocality the maximum locality we want to schedule the tasks at * @param taskCpus the number of CPUs for the task * @param taskResourceAssignments the resource assignments for the task * * @return Triple containing: * (TaskDescription of launched task if any, * rejected resource due to delay scheduling?, * dequeued task index) */ @throws[TaskNotSerializableException] def resourceOffer( execId: String, host: String, maxLocality: TaskLocality.TaskLocality, taskCpus: Int = sched.CPUS_PER_TASK, taskResourceAssignments: Map[String, ResourceInformation] = Map.empty) : (Option[TaskDescription], Boolean, Int) org.apache.spark.scheduler.TaskDescription /** * Description of a task that gets passed onto executors to be executed, usually created by * `TaskSetManager.resourceOffer`. * * TaskDescriptions and the associated Task need to be serialized carefully for two reasons: * * (1) When a TaskDescription is received by an Executor, the Executor needs to first get the * list of JARs and files and add these to the classpath, and set the properties, before * deserializing the Task object (serializedTask). This is why the Properties are included * in the TaskDescription, even though they're also in the serialized task. * (2) Because a TaskDescription is serialized and sent to an executor for each task, efficient * serialization (both in terms of serialization time and serialized buffer size) is * important. For this reason, we serialize TaskDescriptions ourselves with the * TaskDescription.encode and TaskDescription.decode methods. This results in a smaller * serialized size because it avoids serializing unnecessary fields in the Map objects * (which can introduce significant overhead when the maps are small). */ private[spark] class TaskDescription( val taskId: Long, val attemptNumber: Int, ===> how many times this task has been attempted (0 for the first attempt) val executorId: String, val name: String, val index: Int, // Index within this task's TaskSet val partitionId: Int, val addedFiles: Map[String, Long], val addedJars: Map[String, Long], val addedArchives: Map[String, Long], val properties: Properties, val cpus: Int, val resources: immutable.Map[String, ResourceInformation], val serializedTask: ByteBuffer) { assert(cpus > 0, \"CPUs per task should be > 0\") override def toString: String = s\"TaskDescription($name)\" }","title":"resourceOffers"},{"location":"SparkCore/TaskScheduling/#handle-failed-task","text":"org.apache.spark.scheduler.TaskSchedulerImpl#handleFailedTask=> org.apache.spark.scheduler.TaskSetManager#handleFailedTask=> org.apache.spark.scheduler.DAGScheduler#taskEnded org.apache.spark.scheduler.TaskSetManager#handleFailedTask /** * Marks the task as failed, re-adds it to the list of pending tasks, and notifies the * DAG Scheduler. */ def handleFailedTask(tid: Long, state: TaskState, reason: TaskFailedReason): Unit = { val info = taskInfos(tid) // SPARK-37300: when the task was already finished state, just ignore it, // so that there won't cause copiesRunning wrong result. if (info.finished) { return } removeRunningTask(tid) info.markFinished(state, clock.getTimeMillis()) val index = info.index copiesRunning(index) -= 1 var accumUpdates: Seq[AccumulatorV2[_, _]] = Seq.empty var metricPeaks: Array[Long] = Array.empty val failureReason = s\"Lost ${taskName(tid)} (${info.host} \" + s\"executor ${info.executorId}): ${reason.toErrorString}\" val failureException: Option[Throwable] = reason match { case fetchFailed: FetchFailed => logWarning(failureReason) if (!successful(index)) { successful(index) = true tasksSuccessful += 1 } isZombie = true if (fetchFailed.bmAddress != null) { healthTracker.foreach(_.updateExcludedForFetchFailure( fetchFailed.bmAddress.host, fetchFailed.bmAddress.executorId)) } None case ef: ExceptionFailure => // ExceptionFailure's might have accumulator updates accumUpdates = ef.accums metricPeaks = ef.metricPeaks.toArray val task = taskName(tid) if (ef.className == classOf[NotSerializableException].getName) { // If the task result wasn't serializable, there's no point in trying to re-execute it. logError(s\"$task had a not serializable result: ${ef.description}; not retrying\") sched.dagScheduler.taskEnded(tasks(index), reason, null, accumUpdates, metricPeaks, info) abort(s\"$task had a not serializable result: ${ef.description}\") return } if (ef.className == classOf[TaskOutputFileAlreadyExistException].getName) { // If we can not write to output file in the task, there's no point in trying to // re-execute it. logError(\"Task %s in stage %s (TID %d) can not write to output file: %s; not retrying\" .format(info.id, taskSet.id, tid, ef.description)) sched.dagScheduler.taskEnded(tasks(index), reason, null, accumUpdates, metricPeaks, info) abort(\"Task %s in stage %s (TID %d) can not write to output file: %s\".format( info.id, taskSet.id, tid, ef.description)) return } val key = ef.description val now = clock.getTimeMillis() val (printFull, dupCount) = { if (recentExceptions.contains(key)) { val (dupCount, printTime) = recentExceptions(key) if (now - printTime > EXCEPTION_PRINT_INTERVAL) { recentExceptions(key) = (0, now) (true, 0) } else { recentExceptions(key) = (dupCount + 1, printTime) (false, dupCount + 1) } } else { recentExceptions(key) = (0, now) (true, 0) } } if (printFull) { logWarning(failureReason) } else { logInfo( s\"Lost $task on ${info.host}, executor ${info.executorId}: \" + s\"${ef.className} (${ef.description}) [duplicate $dupCount]\") } ef.exception case tk: TaskKilled => // TaskKilled might have accumulator updates accumUpdates = tk.accums metricPeaks = tk.metricPeaks.toArray logWarning(failureReason) None case e: ExecutorLostFailure if !e.exitCausedByApp => logInfo(s\"${taskName(tid)} failed because while it was being computed, its executor \" + \"exited for a reason unrelated to the task. Not counting this failure towards the \" + \"maximum number of failures for the task.\") None case e: TaskFailedReason => // TaskResultLost and others logWarning(failureReason) None } if (tasks(index).isBarrier) { isZombie = true } sched.dagScheduler.taskEnded(tasks(index), reason, null, accumUpdates, metricPeaks, info) if (!isZombie && reason.countTowardsTaskFailures) { assert (null != failureReason) taskSetExcludelistHelperOpt.foreach(_.updateExcludedForFailedTask( info.host, info.executorId, index, failureReason)) numFailures(index) += 1 if (numFailures(index) >= maxTaskFailures) { logError(\"Task %d in stage %s failed %d times; aborting job\".format( index, taskSet.id, maxTaskFailures)) abort(\"Task %d in stage %s failed %d times, most recent failure: %s\\nDriver stacktrace:\" .format(index, taskSet.id, maxTaskFailures, failureReason), failureException) return } } if (successful(index)) { logInfo(s\"${taskName(info.taskId)} failed, but the task will not\" + \" be re-executed (either because the task failed with a shuffle data fetch failure,\" + \" so the previous stage needs to be re-run, or because a different copy of the task\" + \" has already succeeded).\") } else { addPendingTask(index) } maybeFinishTaskSet() }","title":"handle failed task"},{"location":"SparkCore/YarnAllocator/","text":"Yarn Allocator Code YarnAllocator Yarn Allocator \u00b6 Code \u00b6 YarnAllocator \u00b6 org.apache.spark.deploy.yarn.YarnAllocator /** * YarnAllocator is charged with requesting containers from the YARN ResourceManager and deciding * what to do with containers when YARN fulfills these requests. * * This class makes use of YARN's AMRMClient APIs. We interact with the AMRMClient in three ways: * * Making our resource needs known, which updates local bookkeeping about containers requested. * * Calling \"allocate\", which syncs our local container requests with the RM, and returns any * containers that YARN has granted to us. This also functions as a heartbeat. * * Processing the containers granted to us to possibly launch executors inside of them. * * The public methods of this class are thread-safe. All methods that mutate state are * synchronized. */ private[yarn] class YarnAllocator( driverUrl: String, driverRef: RpcEndpointRef, conf: YarnConfiguration, sparkConf: SparkConf, amClient: AMRMClient[ContainerRequest], appAttemptId: ApplicationAttemptId, securityMgr: SecurityManager, localResources: Map[String, LocalResource], resolver: SparkRackResolver, clock: Clock = new SystemClock) extends Logging {","title":"YarnAllocator"},{"location":"SparkCore/YarnAllocator/#yarn-allocator","text":"","title":"Yarn Allocator"},{"location":"SparkCore/YarnAllocator/#code","text":"","title":"Code"},{"location":"SparkCore/YarnAllocator/#yarnallocator","text":"org.apache.spark.deploy.yarn.YarnAllocator /** * YarnAllocator is charged with requesting containers from the YARN ResourceManager and deciding * what to do with containers when YARN fulfills these requests. * * This class makes use of YARN's AMRMClient APIs. We interact with the AMRMClient in three ways: * * Making our resource needs known, which updates local bookkeeping about containers requested. * * Calling \"allocate\", which syncs our local container requests with the RM, and returns any * containers that YARN has granted to us. This also functions as a heartbeat. * * Processing the containers granted to us to possibly launch executors inside of them. * * The public methods of this class are thread-safe. All methods that mutate state are * synchronized. */ private[yarn] class YarnAllocator( driverUrl: String, driverRef: RpcEndpointRef, conf: YarnConfiguration, sparkConf: SparkConf, amClient: AMRMClient[ContainerRequest], appAttemptId: ApplicationAttemptId, securityMgr: SecurityManager, localResources: Map[String, LocalResource], resolver: SparkRackResolver, clock: Clock = new SystemClock) extends Logging {","title":"YarnAllocator"},{"location":"SparkSQL/AQE/","text":"Adaptive execution in Spark Jira QueryExecution#preparations InsertAdaptiveSparkPlan AdaptiveSparkPlanExec QueryStageExec ShuffleQueryStageExec BroadcastQueryStageExec reuseQueryStage Adaptive coalesce partitions Adaptive execution in Spark \u00b6 Jira \u00b6 SPARK-31412 Feature requirement (with subtasks list) Design Doc SPARK-23128 The basic framework for the new Adaptive Query Execution SPARK-28177 Adjust post shuffle partition number in adaptive execution SPARK-29544 Optimize skewed join at runtime with new Adaptive Execution SPARK-31865 Fix complex AQE query stage not reused SPARK-35552 Make query stage materialized more readable SPARK-9850 Adaptive execution in Spark (original idea) Design Doc SPARK-9851 Support submitting map stages individually in DAGScheduler SPARK document Adaptive Query Execution QueryExecution#preparations \u00b6 org.apache.spark.sql.execution.QueryExecution#preparations private[execution] def preparations( sparkSession: SparkSession, adaptiveExecutionRule: Option[InsertAdaptiveSparkPlan] = None, subquery: Boolean): Seq[Rule[SparkPlan]] = { // `AdaptiveSparkPlanExec` is a leaf node. If inserted, all the following rules will be no-op // as the original plan is hidden behind `AdaptiveSparkPlanExec`. adaptiveExecutionRule.toSeq ++ InsertAdaptiveSparkPlan \u00b6 org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan /** * This rule wraps the query plan with an [[AdaptiveSparkPlanExec]], which executes the query plan * and re-optimize the plan during execution based on runtime data statistics. * * Note that this rule is stateful and thus should not be reused across query executions. */ case class InsertAdaptiveSparkPlan( adaptiveExecutionContext: AdaptiveExecutionContext) extends Rule[SparkPlan] { override def apply(plan: SparkPlan): SparkPlan = applyInternal(plan, false) private def applyInternal(plan: SparkPlan, isSubquery: Boolean): SparkPlan = plan match { case _ if !conf.adaptiveExecutionEnabled => plan case _: ExecutedCommandExec => plan case _: CommandResultExec => plan case c: DataWritingCommandExec => c.copy(child = apply(c.child)) case c: V2CommandExec => c.withNewChildren(c.children.map(apply)) case _ if shouldApplyAQE(plan, isSubquery) => if (supportAdaptive(plan)) { try { // Plan sub-queries recursively and pass in the shared stage cache for exchange reuse. // Fall back to non-AQE mode if AQE is not supported in any of the sub-queries. val subqueryMap = buildSubqueryMap(plan) val planSubqueriesRule = PlanAdaptiveSubqueries(subqueryMap) val preprocessingRules = Seq( planSubqueriesRule) // Run pre-processing rules. val newPlan = AdaptiveSparkPlanExec.applyPhysicalRules(plan, preprocessingRules) logDebug(s\"Adaptive execution enabled for plan: $plan\") AdaptiveSparkPlanExec(newPlan, adaptiveExecutionContext, preprocessingRules, isSubquery) } catch { case SubqueryAdaptiveNotSupportedException(subquery) => logWarning(s\"${SQLConf.ADAPTIVE_EXECUTION_ENABLED.key} is enabled \" + s\"but is not supported for sub-query: $subquery.\") plan } } else { logDebug(s\"${SQLConf.ADAPTIVE_EXECUTION_ENABLED.key} is enabled \" + s\"but is not supported for query: $plan.\") plan } case _ => plan } // AQE is only useful when the query has exchanges or sub-queries. This method returns true if // one of the following conditions is satisfied: // - The config ADAPTIVE_EXECUTION_FORCE_APPLY is true. // - The input query is from a sub-query. When this happens, it means we've already decided to // apply AQE for the main query and we must continue to do it. // - The query contains exchanges. // - The query may need to add exchanges. It's an overkill to run `EnsureRequirements` here, so // we just check `SparkPlan.requiredChildDistribution` and see if it's possible that the // the query needs to add exchanges later. // - The query contains sub-query. private def shouldApplyAQE(plan: SparkPlan, isSubquery: Boolean): Boolean = { conf.getConf(SQLConf.ADAPTIVE_EXECUTION_FORCE_APPLY) || isSubquery || { plan.exists { case _: Exchange => true case p if !p.requiredChildDistribution.forall(_ == UnspecifiedDistribution) => true case p => p.expressions.exists(_.exists { case _: SubqueryExpression => true case _ => false }) } } } AdaptiveSparkPlanExec \u00b6 org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec /** * A root node to execute the query plan adaptively. It splits the query plan into independent * stages and executes them in order according to their dependencies. The query stage * materializes its output at the end. When one stage completes, the data statistics of the * materialized output will be used to optimize the remainder of the query. * * To create query stages, we traverse the query tree bottom up. When we hit an exchange node, * and if all the child query stages of this exchange node are materialized, we create a new * query stage for this exchange node. The new stage is then materialized asynchronously once it * is created. * * When one query stage finishes materialization, the rest query is re-optimized and planned based * on the latest statistics provided by all materialized stages. Then we traverse the query plan * again and create more stages if possible. After all stages have been materialized, we execute * the rest of the plan. */ case class AdaptiveSparkPlanExec( inputPlan: SparkPlan, @transient context: AdaptiveExecutionContext, @transient preprocessingRules: Seq[Rule[SparkPlan]], @transient isSubquery: Boolean, @transient override val supportsColumnar: Boolean = false) extends LeafExecNode { override def doExecute(): RDD[InternalRow] = { withFinalPlanUpdate(_.execute()) } private def withFinalPlanUpdate[T](fun: SparkPlan => T): T = { val plan = getFinalPhysicalPlan() val result = fun(plan) finalPlanUpdate result } ==================== getFinalPhysicalPlan ========================= private def getFinalPhysicalPlan(): SparkPlan = lock.synchronized { if (isFinalPlan) return currentPhysicalPlan // In case of this adaptive plan being executed out of `withActive` scoped functions, e.g., // `plan.queryExecution.rdd`, we need to set active session here as new plan nodes can be // created in the middle of the execution. context.session.withActive { val executionId = getExecutionId // Use inputPlan logicalLink here in case some top level physical nodes may be removed // during `initialPlan` var currentLogicalPlan = inputPlan.logicalLink.get var result = createQueryStages(currentPhysicalPlan) ===> createQueryStages val events = new LinkedBlockingQueue[StageMaterializationEvent]() val errors = new mutable.ArrayBuffer[Throwable]() var stagesToReplace = Seq.empty[QueryStageExec] while (!result.allChildStagesMaterialized) { ===> wait till all child stages materialized currentPhysicalPlan = result.newPlan if (result.newStages.nonEmpty) { stagesToReplace = result.newStages ++ stagesToReplace executionId.foreach(onUpdatePlan(_, result.newStages.map(_.plan))) // SPARK-33933: we should submit tasks of broadcast stages first, to avoid waiting // for tasks to be scheduled and leading to broadcast timeout. // This partial fix only guarantees the start of materialization for BroadcastQueryStage // is prior to others, but because the submission of collect job for broadcasting is // running in another thread, the issue is not completely resolved. val reorderedNewStages = result.newStages .sortWith { case (_: BroadcastQueryStageExec, _: BroadcastQueryStageExec) => false case (_: BroadcastQueryStageExec, _) => true case _ => false } ==================== stage.materialize() is run as Future async ========================= // Start materialization of all new stages and fail fast if any stages failed eagerly reorderedNewStages.foreach { stage => try { stage.materialize().onComplete { res => ===> materialize() is done async if (res.isSuccess) { events.offer(StageSuccess(stage, res.get)) ===> put into events queue } else { events.offer(StageFailure(stage, res.failed.get)) } }(AdaptiveSparkPlanExec.executionContext) } catch { case e: Throwable => cleanUpAndThrowException(Seq(e), Some(stage.id)) } } ========================================================================================== } ====== Wait on the next completed stage ====== // Wait on the next completed stage, which indicates new stats are available and probably // new stages can be created. There might be other stages that finish at around the same // time, so we process those stages too in order to reduce re-planning. val nextMsg = events.take() ===> block wait on event queue val rem = new util.ArrayList[StageMaterializationEvent]() events.drainTo(rem) (Seq(nextMsg) ++ rem.asScala).foreach { case StageSuccess(stage, res) => stage.resultOption.set(Some(res)) case StageFailure(stage, ex) => errors.append(ex) } // In case of errors, we cancel all running stages and throw exception. if (errors.nonEmpty) { cleanUpAndThrowException(errors.toSeq, None) } ======= re-optimizing and re-planning ====== // Try re-optimizing and re-planning. Adopt the new plan if its cost is equal to or less // than that of the current plan; otherwise keep the current physical plan together with // the current logical plan since the physical plan's logical links point to the logical // plan it has originated from. // Meanwhile, we keep a list of the query stages that have been created since last plan // update, which stands for the \"semantic gap\" between the current logical and physical // plans. And each time before re-planning, we replace the corresponding nodes in the // current logical plan with logical query stages to make it semantically in sync with // the current physical plan. Once a new plan is adopted and both logical and physical // plans are updated, we can clear the query stage list because at this point the two plans // are semantically and physically in sync again. val logicalPlan = replaceWithQueryStagesInLogicalPlan(currentLogicalPlan, stagesToReplace) val afterReOptimize = reOptimize(logicalPlan) if (afterReOptimize.isDefined) { val (newPhysicalPlan, newLogicalPlan) = afterReOptimize.get val origCost = costEvaluator.evaluateCost(currentPhysicalPlan) val newCost = costEvaluator.evaluateCost(newPhysicalPlan) if (newCost < origCost || (newCost == origCost && currentPhysicalPlan != newPhysicalPlan)) { logOnLevel(\"Plan changed:\\n\" + sideBySide(currentPhysicalPlan.treeString, newPhysicalPlan.treeString).mkString(\"\\n\")) cleanUpTempTags(newPhysicalPlan) currentPhysicalPlan = newPhysicalPlan currentLogicalPlan = newLogicalPlan stagesToReplace = Seq.empty[QueryStageExec] } } // Now that some stages have finished, we can try creating new stages. result = createQueryStages(currentPhysicalPlan) } // Run the final plan when there's no more unfinished stages. currentPhysicalPlan = applyPhysicalRules( optimizeQueryStage(result.newPlan, isFinalStage = true), postStageCreationRules(supportsColumnar), Some((planChangeLogger, \"AQE Post Stage Creation\"))) isFinalPlan = true executionId.foreach(onUpdatePlan(_, Seq(currentPhysicalPlan))) currentPhysicalPlan } } /** * This method is called recursively to traverse the plan tree bottom-up and create a new query * stage or try reusing an existing stage if the current node is an [[Exchange]] node and all of * its child stages have been materialized. * * With each call, it returns: * 1) The new plan replaced with [[QueryStageExec]] nodes where new stages are created. * 2) Whether the child query stages (if any) of the current node have all been materialized. * 3) A list of the new query stages that have been created. */ private def createQueryStages(plan: SparkPlan): CreateStageResult = plan match { case e: Exchange => // First have a quick check in the `stageCache` without having to traverse down the node. context.stageCache.get(e.canonicalized) match { case Some(existingStage) if conf.exchangeReuseEnabled => val stage = reuseQueryStage(existingStage, e) val isMaterialized = stage.isMaterialized CreateStageResult( newPlan = stage, allChildStagesMaterialized = isMaterialized, newStages = if (isMaterialized) Seq.empty else Seq(stage)) case _ => val result = createQueryStages(e.child) val newPlan = e.withNewChildren(Seq(result.newPlan)).asInstanceOf[Exchange] // Create a query stage only when all the child query stages are ready. if (result.allChildStagesMaterialized) { var newStage = newQueryStage(newPlan) if (conf.exchangeReuseEnabled) { // Check the `stageCache` again for reuse. If a match is found, ditch the new stage // and reuse the existing stage found in the `stageCache`, otherwise update the // `stageCache` with the new stage. val queryStage = context.stageCache.getOrElseUpdate( newStage.plan.canonicalized, newStage) if (queryStage.ne(newStage)) { newStage = reuseQueryStage(queryStage, e) } } val isMaterialized = newStage.isMaterialized CreateStageResult( newPlan = newStage, allChildStagesMaterialized = isMaterialized, newStages = if (isMaterialized) Seq.empty else Seq(newStage)) } else { CreateStageResult(newPlan = newPlan, allChildStagesMaterialized = false, newStages = result.newStages) } } case q: QueryStageExec => CreateStageResult(newPlan = q, allChildStagesMaterialized = q.isMaterialized, newStages = Seq.empty) case _ => if (plan.children.isEmpty) { CreateStageResult(newPlan = plan, allChildStagesMaterialized = true, newStages = Seq.empty) } else { val results = plan.children.map(createQueryStages) CreateStageResult( newPlan = plan.withNewChildren(results.map(_.newPlan)), allChildStagesMaterialized = results.forall(_.allChildStagesMaterialized), newStages = results.flatMap(_.newStages)) } } private def newQueryStage(e: Exchange): QueryStageExec = { val optimizedPlan = optimizeQueryStage(e.child, isFinalStage = false) val queryStage = e match { case s: ShuffleExchangeLike => val newShuffle = applyPhysicalRules( s.withNewChildren(Seq(optimizedPlan)), postStageCreationRules(outputsColumnar = s.supportsColumnar), Some((planChangeLogger, \"AQE Post Stage Creation\"))) if (!newShuffle.isInstanceOf[ShuffleExchangeLike]) { throw new IllegalStateException( \"Custom columnar rules cannot transform shuffle node to something else.\") } ShuffleQueryStageExec(currentStageId, newShuffle, s.canonicalized) case b: BroadcastExchangeLike => val newBroadcast = applyPhysicalRules( b.withNewChildren(Seq(optimizedPlan)), postStageCreationRules(outputsColumnar = b.supportsColumnar), Some((planChangeLogger, \"AQE Post Stage Creation\"))) if (!newBroadcast.isInstanceOf[BroadcastExchangeLike]) { throw new IllegalStateException( \"Custom columnar rules cannot transform broadcast node to something else.\") } BroadcastQueryStageExec(currentStageId, newBroadcast, b.canonicalized) } currentStageId += 1 setLogicalLinkForNewQueryStage(queryStage, e) queryStage } rules @transient private val costEvaluator = conf.getConf(SQLConf.ADAPTIVE_CUSTOM_COST_EVALUATOR_CLASS) match { case Some(className) => CostEvaluator.instantiate(className, session.sparkContext.getConf) case _ => SimpleCostEvaluator(conf.getConf(SQLConf.ADAPTIVE_FORCE_OPTIMIZE_SKEWED_JOIN)) } // A list of physical plan rules to be applied before creation of query stages. The physical // plan should reach a final status of query stages (i.e., no more addition or removal of // Exchange nodes) after running these rules. @transient private val queryStagePreparationRules: Seq[Rule[SparkPlan]] = { // For cases like `df.repartition(a, b).select(c)`, there is no distribution requirement for // the final plan, but we do need to respect the user-specified repartition. Here we ask // `EnsureRequirements` to not optimize out the user-specified repartition-by-col to work // around this case. val ensureRequirements = EnsureRequirements(requiredDistribution.isDefined, requiredDistribution) Seq( RemoveRedundantProjects, ensureRequirements, ValidateSparkPlan, ReplaceHashWithSortAgg, RemoveRedundantSorts, DisableUnnecessaryBucketedScan, OptimizeSkewedJoin(ensureRequirements) ) ++ context.session.sessionState.adaptiveRulesHolder.queryStagePrepRules } // A list of physical optimizer rules to be applied to a new stage before its execution. These // optimizations should be stage-independent. @transient private val queryStageOptimizerRules: Seq[Rule[SparkPlan]] = Seq( PlanAdaptiveDynamicPruningFilters(this), ReuseAdaptiveSubquery(context.subqueryCache), OptimizeSkewInRebalancePartitions, CoalesceShufflePartitions(context.session), // `OptimizeShuffleWithLocalRead` needs to make use of 'AQEShuffleReadExec.partitionSpecs' // added by `CoalesceShufflePartitions`, and must be executed after it. OptimizeShuffleWithLocalRead ) // This rule is stateful as it maintains the codegen stage ID. We can't create a fresh one every // time and need to keep it in a variable. @transient private val collapseCodegenStagesRule: Rule[SparkPlan] = CollapseCodegenStages() // A list of physical optimizer rules to be applied right after a new stage is created. The input // plan to these rules has exchange as its root node. private def postStageCreationRules(outputsColumnar: Boolean) = Seq( ApplyColumnarRulesAndInsertTransitions( context.session.sessionState.columnarRules, outputsColumnar), collapseCodegenStagesRule ) @transient val initialPlan = context.session.withActive { applyPhysicalRules( inputPlan, queryStagePreparationRules, Some((planChangeLogger, \"AQE Preparations\"))) } @volatile private var currentPhysicalPlan = initialPlan // The logical plan optimizer for re-optimizing the current logical plan. @transient private val optimizer = new AQEOptimizer(conf, session.sessionState.adaptiveRulesHolder.runtimeOptimizerRules) private def optimizeQueryStage(plan: SparkPlan, isFinalStage: Boolean): SparkPlan = { val optimized = queryStageOptimizerRules.foldLeft(plan) { case (latestPlan, rule) => val applied = rule.apply(latestPlan) val result = rule match { case _: AQEShuffleReadRule if !applied.fastEquals(latestPlan) => val distribution = if (isFinalStage) { // If `requiredDistribution` is None, it means `EnsureRequirements` will not optimize // out the user-specified repartition, thus we don't have a distribution requirement // for the final plan. requiredDistribution.getOrElse(UnspecifiedDistribution) } else { UnspecifiedDistribution } if (ValidateRequirements.validate(applied, distribution)) { applied } else { logDebug(s\"Rule ${rule.ruleName} is not applied as it breaks the \" + \"distribution requirement of the query plan.\") latestPlan } case _ => applied } planChangeLogger.logRule(rule.ruleName, latestPlan, result) result } planChangeLogger.logBatch(\"AQE Query Stage Optimization\", plan, optimized) optimized } /** * Re-optimize and run physical planning on the current logical plan based on the latest stats. */ private def reOptimize(logicalPlan: LogicalPlan): Option[(SparkPlan, LogicalPlan)] = { try { logicalPlan.invalidateStatsCache() val optimized = optimizer.execute(logicalPlan) val sparkPlan = context.session.sessionState.planner.plan(ReturnAnswer(optimized)).next() val newPlan = applyPhysicalRules( sparkPlan, preprocessingRules ++ queryStagePreparationRules, Some((planChangeLogger, \"AQE Replanning\"))) // When both enabling AQE and DPP, `PlanAdaptiveDynamicPruningFilters` rule will // add the `BroadcastExchangeExec` node manually in the DPP subquery, // not through `EnsureRequirements` rule. Therefore, when the DPP subquery is complicated // and need to be re-optimized, AQE also need to manually insert the `BroadcastExchangeExec` // node to prevent the loss of the `BroadcastExchangeExec` node in DPP subquery. // Here, we also need to avoid to insert the `BroadcastExchangeExec` node when the newPlan is // already the `BroadcastExchangeExec` plan after apply the `LogicalQueryStageStrategy` rule. val finalPlan = inputPlan match { case b: BroadcastExchangeLike if (!newPlan.isInstanceOf[BroadcastExchangeLike]) => b.withNewChildren(Seq(newPlan)) case _ => newPlan } Some((finalPlan, optimized)) } catch { case e: InvalidAQEPlanException[_] => logOnLevel(s\"Re-optimize - ${e.getMessage()}:\\n${e.plan}\") None } } QueryStageExec \u00b6 org.apache.spark.sql.execution.adaptive.QueryStageExec /** * A query stage is an independent subgraph of the query plan. Query stage materializes its output * before proceeding with further operators of the query plan. The data statistics of the * materialized output can be used to optimize subsequent query stages. * * There are 2 kinds of query stages: * 1. Shuffle query stage. This stage materializes its output to shuffle files, and Spark launches * another job to execute the further operators. * 2. Broadcast query stage. This stage materializes its output to an array in driver JVM. Spark * broadcasts the array before executing the further operators. */ abstract class QueryStageExec extends LeafExecNode { @transient @volatile protected var _resultOption = new AtomicReference[Option[Any]](None) private[adaptive] def resultOption: AtomicReference[Option[Any]] = _resultOption def isMaterialized: Boolean = resultOption.get().isDefined /** * Compute the statistics of the query stage if executed, otherwise None. */ def computeStats(): Option[Statistics] = if (isMaterialized) { val runtimeStats = getRuntimeStatistics val dataSize = runtimeStats.sizeInBytes.max(0) val numOutputRows = runtimeStats.rowCount.map(_.max(0)) Some(Statistics(dataSize, numOutputRows, isRuntime = true)) } else { None } /** * Materialize this query stage, to prepare for the execution, like submitting map stages, * broadcasting data, etc. The caller side can use the returned [[Future]] to wait until this * stage is ready. */ def doMaterialize(): Future[Any] ====== materialize() is called by org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#getFinalPhysicalPlan === /** * Materialize this query stage, to prepare for the execution, like submitting map stages, * broadcasting data, etc. The caller side can use the returned [[Future]] to wait until this * stage is ready. */ final def materialize(): Future[Any] = { logDebug(s\"Materialize query stage ${this.getClass.getSimpleName}: $id\") doMaterialize() } ========================================================================================================= ShuffleQueryStageExec \u00b6 org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#getFinalPhysicalPlan => org.apache.spark.sql.execution.adaptive.QueryStageExec#materialize => org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec#doMaterialize=> org.apache.spark.sql.execution.exchange.ShuffleExchangeLike#submitShuffleJob=> org.apache.spark.sql.execution.SparkPlan#executeQuery=> org.apache.spark.sql.execution.exchange.ShuffleExchangeExec#mapOutputStatisticsFuture=> org.apache.spark.SparkContext#submitMapStage=> org.apache.spark.scheduler.DAGScheduler#submitMapStage=> org.apache.spark.scheduler.JobWaiter run test test(\"SPARK-37063: OptimizeSkewInRebalancePartitions support optimize non-root node\") /** * A shuffle query stage whose child is a [[ShuffleExchangeLike]] or [[ReusedExchangeExec]]. * * @param id the query stage id. * @param plan the underlying plan. * @param _canonicalized the canonicalized plan before applying query stage optimizer rules. */ case class ShuffleQueryStageExec( override val id: Int, override val plan: SparkPlan, override val _canonicalized: SparkPlan) extends QueryStageExec { @transient val shuffle = plan match { case s: ShuffleExchangeLike => s case ReusedExchangeExec(_, s: ShuffleExchangeLike) => s case _ => throw new IllegalStateException(s\"wrong plan for shuffle stage:\\n ${plan.treeString}\") } @transient private lazy val shuffleFuture = shuffle.submitShuffleJob ===> org.apache.spark.sql.execution.exchange.ShuffleExchangeLike#submitShuffleJob override def doMaterialize(): Future[Any] = shuffleFuture override def getRuntimeStatistics: Statistics = shuffle.runtimeStatistics org.apache.spark.sql.execution.exchange.ShuffleExchangeLike#submitShuffleJob /** * The asynchronous job that materializes the shuffle. It also does the preparations work, * such as waiting for the subqueries. */ final def submitShuffleJob: Future[MapOutputStatistics] = executeQuery { mapOutputStatisticsFuture } ===> org.apache.spark.sql.execution.exchange.ShuffleExchangeExec#mapOutputStatisticsFuture // 'mapOutputStatisticsFuture' is only needed when enable AQE. @transient override lazy val mapOutputStatisticsFuture: Future[MapOutputStatistics] = { if (inputRDD.getNumPartitions == 0) { Future.successful(null) } else { sparkContext.submitMapStage(shuffleDependency) } } ===> org.apache.spark.sql.execution.SparkPlan#executeQuery /** * Executes a query after preparing the query and adding query plan information to created RDDs * for visualization. */ protected final def executeQuery[T](query: => T): T = { RDDOperationScope.withScope(sparkContext, nodeName, false, true) { prepare() waitForSubqueries() query } } ===> org.apache.spark.SparkContext#submitMapStage /** * Submit a map stage for execution. This is currently an internal API only, but might be * promoted to DeveloperApi in the future. */ private[spark] def submitMapStage[K, V, C](dependency: ShuffleDependency[K, V, C]) : SimpleFutureAction[MapOutputStatistics] = { assertNotStopped() val callSite = getCallSite() var result: MapOutputStatistics = null val waiter = dagScheduler.submitMapStage( dependency, (r: MapOutputStatistics) => { result = r }, callSite, localProperties.get) new SimpleFutureAction[MapOutputStatistics](waiter, result) } ===> org.apache.spark.scheduler.DAGScheduler#submitMapStage /** * Submit a shuffle map stage to run independently and get a JobWaiter object back. The waiter * can be used to block until the job finishes executing or can be used to cancel the job. * This method is used for adaptive query planning, to run map stages and look at statistics * about their outputs before submitting downstream stages. * * @param dependency the ShuffleDependency to run a map stage for * @param callback function called with the result of the job, which in this case will be a * single MapOutputStatistics object showing how much data was produced for each partition * @param callSite where in the user program this job was submitted * @param properties scheduler properties to attach to this job, e.g. fair scheduler pool name */ def submitMapStage[K, V, C]( dependency: ShuffleDependency[K, V, C], callback: MapOutputStatistics => Unit, callSite: CallSite, properties: Properties): JobWaiter[MapOutputStatistics] = { val rdd = dependency.rdd val jobId = nextJobId.getAndIncrement() if (rdd.partitions.length == 0) { throw SparkCoreErrors.cannotRunSubmitMapStageOnZeroPartitionRDDError() } // SPARK-23626: `RDD.getPartitions()` can be slow, so we eagerly compute // `.partitions` on every RDD in the DAG to ensure that `getPartitions()` // is evaluated outside of the DAGScheduler's single-threaded event loop: eagerlyComputePartitionsForRddAndAncestors(rdd) // We create a JobWaiter with only one \"task\", which will be marked as complete when the whole // map stage has completed, and will be passed the MapOutputStatistics for that stage. // This makes it easier to avoid race conditions between the user code and the map output // tracker that might result if we told the user the stage had finished, but then they queries // the map output tracker and some node failures had caused the output statistics to be lost. val waiter = new JobWaiter[MapOutputStatistics]( this, jobId, 1, (_: Int, r: MapOutputStatistics) => callback(r)) eventProcessLoop.post(MapStageSubmitted( jobId, dependency, callSite, waiter, Utils.cloneProperties(properties))) waiter } ===> org.apache.spark.scheduler.JobWaiter /** * An object that waits for a DAGScheduler job to complete. As tasks finish, it passes their * results to the given handler function. */ private[spark] class JobWaiter[T]( dagScheduler: DAGScheduler, val jobId: Int, totalTasks: Int, resultHandler: (Int, T) => Unit) extends JobListener with Logging { stack in test(\"SPARK-37063: OptimizeSkewInRebalancePartitions support optimize non-root node\") at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:291) at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:244) at org.apache.spark.sql.execution.SparkPlan$$Lambda$2339.1114370802.apply(Unknown Source:-1) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243) at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:73) at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:72) at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:120) at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:185) - locked <0x362c> (a org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec) at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:185) at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:187) at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:286) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:284) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2336.1706125628.apply(Unknown Source:-1) at scala.collection.Iterator.foreach(Iterator.scala:943) at scala.collection.Iterator.foreach$(Iterator.scala:943) at scala.collection.AbstractIterator.foreach(Iterator.scala:1431) at scala.collection.IterableLike.foreach(IterableLike.scala:74) at scala.collection.IterableLike.foreach$(IterableLike.scala:73) at scala.collection.AbstractIterable.foreach(Iterable.scala:56) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:284) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2295.312068212.apply(Unknown Source:-1) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256) - locked <0x389c> (a java.lang.Object) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374) at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4384) at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3625) at org.apache.spark.sql.Dataset$$Lambda$2283.289194317.apply(Unknown Source:-1) at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4374) at org.apache.spark.sql.Dataset$$Lambda$2290.1482151715.apply(Unknown Source:-1) at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:529) at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4372) at org.apache.spark.sql.Dataset$$Lambda$2284.280804333.apply(Unknown Source:-1) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118) at org.apache.spark.sql.execution.SQLExecution$$$Lambda$1713.806842585.apply(Unknown Source:-1) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103) at org.apache.spark.sql.execution.SQLExecution$$$Lambda$1703.1020198427.apply(Unknown Source:-1) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4372) at org.apache.spark.sql.Dataset.collect(Dataset.scala:3625) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.runAdaptiveAndVerifyResult(AdaptiveQueryExecSuite.scala:83) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.checkRebalance$1(AdaptiveQueryExecSuite.scala:2417) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.$anonfun$new$273(AdaptiveQueryExecSuite.scala:2430) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite$$Lambda$2074.1374673778.apply$mcV$sp(Unknown Source:-1) at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf(SQLHelper.scala:54) at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf$(SQLHelper.scala:38) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.org$apache$spark$sql$test$SQLTestUtilsBase$$super$withSQLConf(AdaptiveQueryExecSuite.scala:51) at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf(SQLTestUtils.scala:247) at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf$(SQLTestUtils.scala:245) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.withSQLConf(AdaptiveQueryExecSuite.scala:51) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.$anonfun$new$270(AdaptiveQueryExecSuite.scala:2429) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite$$Lambda$2046.860717660.apply$mcV$sp(Unknown Source:-1) at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf(SQLHelper.scala:54) at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf$(SQLHelper.scala:38) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.org$apache$spark$sql$test$SQLTestUtilsBase$$super$withSQLConf(AdaptiveQueryExecSuite.scala:51) at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf(SQLTestUtils.scala:247) at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf$(SQLTestUtils.scala:245) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.withSQLConf(AdaptiveQueryExecSuite.scala:51) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.$anonfun$new$269(AdaptiveQueryExecSuite.scala:2411) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite$$Lambda$2044.1413677222.apply$mcV$sp(Unknown Source:-1) BroadcastQueryStageExec \u00b6 /** * A broadcast query stage whose child is a [[BroadcastExchangeLike]] or [[ReusedExchangeExec]]. * * @param id the query stage id. * @param plan the underlying plan. * @param _canonicalized the canonicalized plan before applying query stage optimizer rules. */ case class BroadcastQueryStageExec( override val id: Int, override val plan: SparkPlan, override val _canonicalized: SparkPlan) extends QueryStageExec { @transient val broadcast = plan match { case b: BroadcastExchangeLike => b case ReusedExchangeExec(_, b: BroadcastExchangeLike) => b case _ => throw new IllegalStateException(s\"wrong plan for broadcast stage:\\n ${plan.treeString}\") } override def doMaterialize(): Future[Any] = { broadcast.submitBroadcastJob } override def getRuntimeStatistics: Statistics = broadcast.runtimeStatistics reuseQueryStage \u00b6 org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#createQueryStages => reuseQueryStage org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#reuseQueryStage private def reuseQueryStage(existing: QueryStageExec, exchange: Exchange): QueryStageExec = { val queryStage = existing.newReuseInstance(currentStageId, exchange.output) currentStageId += 1 setLogicalLinkForNewQueryStage(queryStage, exchange) queryStage } org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec#newReuseInstance override def newReuseInstance(newStageId: Int, newOutput: Seq[Attribute]): QueryStageExec = { val reuse = BroadcastQueryStageExec( newStageId, ReusedExchangeExec(newOutput, broadcast), _canonicalized) reuse._resultOption = this._resultOption reuse } org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec#newReuseInstance override def newReuseInstance(newStageId: Int, newOutput: Seq[Attribute]): QueryStageExec = { val reuse = ShuffleQueryStageExec( newStageId, ReusedExchangeExec(newOutput, shuffle), _canonicalized) reuse._resultOption = this._resultOption reuse } Adaptive coalesce partitions \u00b6 SQLConf val COALESCE_PARTITIONS_ENABLED = buildConf(\"spark.sql.adaptive.coalescePartitions.enabled\") .doc(s\"When true and '${ADAPTIVE_EXECUTION_ENABLED.key}' is true, Spark will coalesce \" + \"contiguous shuffle partitions according to the target size (specified by \" + s\"'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}'), to avoid too many small tasks.\") .version(\"3.0.0\") .booleanConf .createWithDefault(true) val COALESCE_PARTITIONS_PARALLELISM_FIRST = buildConf(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\") .doc(\"When true, Spark does not respect the target size specified by \" + s\"'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}' (default 64MB) when coalescing contiguous \" + \"shuffle partitions, but adaptively calculate the target size according to the default \" + \"parallelism of the Spark cluster. The calculated size is usually smaller than the \" + \"configured target size. This is to maximize the parallelism and avoid performance \" + \"regression when enabling adaptive query execution. It's recommended to set this config \" + \"to false and respect the configured target size.\") .version(\"3.2.0\") .booleanConf .createWithDefault(true) val COALESCE_PARTITIONS_MIN_PARTITION_SIZE = buildConf(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\") .doc(\"The minimum size of shuffle partitions after coalescing. This is useful when the \" + \"adaptively calculated target size is too small during partition coalescing.\") .version(\"3.2.0\") .bytesConf(ByteUnit.BYTE) .checkValue(_ > 0, \"minPartitionSize must be positive\") .createWithDefaultString(\"1MB\") val COALESCE_PARTITIONS_MIN_PARTITION_NUM = buildConf(\"spark.sql.adaptive.coalescePartitions.minPartitionNum\") .internal() .doc(\"(deprecated) The suggested (not guaranteed) minimum number of shuffle partitions \" + \"after coalescing. If not set, the default value is the default parallelism of the \" + \"Spark cluster. This configuration only has an effect when \" + s\"'${ADAPTIVE_EXECUTION_ENABLED.key}' and \" + s\"'${COALESCE_PARTITIONS_ENABLED.key}' are both true.\") .version(\"3.0.0\") .intConf .checkValue(_ > 0, \"The minimum number of partitions must be positive.\") .createOptional val COALESCE_PARTITIONS_INITIAL_PARTITION_NUM = buildConf(\"spark.sql.adaptive.coalescePartitions.initialPartitionNum\") .doc(\"The initial number of shuffle partitions before coalescing. If not set, it equals to \" + s\"${SHUFFLE_PARTITIONS.key}. This configuration only has an effect when \" + s\"'${ADAPTIVE_EXECUTION_ENABLED.key}' and '${COALESCE_PARTITIONS_ENABLED.key}' \" + \"are both true.\") .version(\"3.0.0\") .intConf .checkValue(_ > 0, \"The initial number of partitions must be positive.\") .createOptional org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#queryStageOptimizerRules // A list of physical optimizer rules to be applied to a new stage before its execution. These // optimizations should be stage-independent. @transient private val queryStageOptimizerRules: Seq[Rule[SparkPlan]] = Seq( PlanAdaptiveDynamicPruningFilters(this), ReuseAdaptiveSubquery(context.subqueryCache), OptimizeSkewInRebalancePartitions, ====== rule for coalesce partitions ========== CoalesceShufflePartitions(context.session), ============================================== // `OptimizeShuffleWithLocalRead` needs to make use of 'AQEShuffleReadExec.partitionSpecs' // added by `CoalesceShufflePartitions`, and must be executed after it. OptimizeShuffleWithLocalRead ) org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions /** * A rule to coalesce the shuffle partitions based on the map output statistics, which can * avoid many small reduce tasks that hurt performance. */ case class CoalesceShufflePartitions(session: SparkSession) extends AQEShuffleReadRule { override def apply(plan: SparkPlan): SparkPlan = { if (!conf.coalesceShufflePartitionsEnabled) { return plan } // Ideally, this rule should simply coalesce partitions w.r.t. the target size specified by // ADVISORY_PARTITION_SIZE_IN_BYTES (default 64MB). To avoid perf regression in AQE, this // rule by default tries to maximize the parallelism and set the target size to // `total shuffle size / Spark default parallelism`. In case the `Spark default parallelism` // is too big, this rule also respect the minimum partition size specified by // COALESCE_PARTITIONS_MIN_PARTITION_SIZE (default 1MB). // For history reason, this rule also need to support the config // COALESCE_PARTITIONS_MIN_PARTITION_NUM. We should remove this config in the future. val minNumPartitions = conf.getConf(SQLConf.COALESCE_PARTITIONS_MIN_PARTITION_NUM).getOrElse { if (conf.getConf(SQLConf.COALESCE_PARTITIONS_PARALLELISM_FIRST)) { // We fall back to Spark default parallelism if the minimum number of coalesced partitions // is not set, so to avoid perf regressions compared to no coalescing. session.sparkContext.defaultParallelism } else { // If we don't need to maximize the parallelism, we set `minPartitionNum` to 1, so that // the specified advisory partition size will be respected. 1 } } val advisoryTargetSize = conf.getConf(SQLConf.ADVISORY_PARTITION_SIZE_IN_BYTES) val minPartitionSize = if (Utils.isTesting) { // In the tests, we usually set the target size to a very small value that is even smaller // than the default value of the min partition size. Here we also adjust the min partition // size to be not larger than 20% of the target size, so that the tests don't need to set // both configs all the time to check the coalescing behavior. conf.getConf(SQLConf.COALESCE_PARTITIONS_MIN_PARTITION_SIZE).min(advisoryTargetSize / 5) } else { conf.getConf(SQLConf.COALESCE_PARTITIONS_MIN_PARTITION_SIZE) } // Sub-plans under the Union operator can be coalesced independently, so we can divide them // into independent \"coalesce groups\", and all shuffle stages within each group have to be // coalesced together. val coalesceGroups = collectCoalesceGroups(plan) // Divide minimum task parallelism among coalesce groups according to their data sizes. val minNumPartitionsByGroup = if (coalesceGroups.length == 1) { Seq(math.max(minNumPartitions, 1)) } else { val sizes = coalesceGroups.map(_.flatMap(_.shuffleStage.mapStats.map(_.bytesByPartitionId.sum)).sum) val totalSize = sizes.sum sizes.map { size => val num = if (totalSize > 0) { math.round(minNumPartitions * 1.0 * size / totalSize) } else { minNumPartitions } math.max(num.toInt, 1) } } val specsMap = mutable.HashMap.empty[Int, Seq[ShufflePartitionSpec]] // Coalesce partitions for each coalesce group independently. coalesceGroups.zip(minNumPartitionsByGroup).foreach { case (shuffleStages, minNumPartitions) => val newPartitionSpecs = ShufflePartitionsUtil.coalescePartitions( shuffleStages.map(_.shuffleStage.mapStats), shuffleStages.map(_.partitionSpecs), advisoryTargetSize = advisoryTargetSize, minNumPartitions = minNumPartitions, minPartitionSize = minPartitionSize) if (newPartitionSpecs.nonEmpty) { shuffleStages.zip(newPartitionSpecs).map { case (stageInfo, partSpecs) => specsMap.put(stageInfo.shuffleStage.id, partSpecs) } } } if (specsMap.nonEmpty) { updateShuffleReads(plan, specsMap.toMap) } else { plan } } private def updateShuffleReads( plan: SparkPlan, specsMap: Map[Int, Seq[ShufflePartitionSpec]]): SparkPlan = plan match { // Even for shuffle exchange whose input RDD has 0 partition, we should still update its // `partitionStartIndices`, so that all the leaf shuffles in a stage have the same // number of output partitions. case ShuffleStageInfo(stage, _) => specsMap.get(stage.id).map { specs => AQEShuffleReadExec(stage, specs) }.getOrElse(plan) case other => other.mapChildren(updateShuffleReads(_, specsMap)) } org.apache.spark.sql.execution.adaptive.AQEShuffleReadRule /** * A rule that may create [[AQEShuffleReadExec]] on top of [[ShuffleQueryStageExec]] and change the * plan output partitioning. The AQE framework will skip the rule if it leads to extra shuffles. */ trait AQEShuffleReadRule extends Rule[SparkPlan] { /** * Returns the list of [[ShuffleOrigin]]s supported by this rule. */ protected def supportedShuffleOrigins: Seq[ShuffleOrigin] protected def isSupported(shuffle: ShuffleExchangeLike): Boolean = { supportedShuffleOrigins.contains(shuffle.shuffleOrigin) } } org.apache.spark.sql.execution.adaptive.AQEShuffleReadExec /** * A wrapper of shuffle query stage, which follows the given partition arrangement. * * @param child It is usually `ShuffleQueryStageExec`, but can be the shuffle exchange * node during canonicalization. * @param partitionSpecs The partition specs that defines the arrangement, requires at least one * partition. */ case class AQEShuffleReadExec private( child: SparkPlan, partitionSpecs: Seq[ShufflePartitionSpec]) extends UnaryExecNode { private def shuffleStage = child match { case stage: ShuffleQueryStageExec => Some(stage) case _ => None } private lazy val shuffleRDD: RDD[_] = { shuffleStage match { case Some(stage) => sendDriverMetrics() stage.shuffle.getShuffleRDD(partitionSpecs.toArray) case _ => throw new IllegalStateException(\"operating on canonicalized plan\") } } override protected def doExecute(): RDD[InternalRow] = { shuffleRDD.asInstanceOf[RDD[InternalRow]] } org.apache.spark.sql.execution.exchange.ShuffleExchangeExec#getShuffleRDD override def getShuffleRDD(partitionSpecs: Array[ShufflePartitionSpec]): RDD[InternalRow] = { new ShuffledRowRDD(shuffleDependency, readMetrics, partitionSpecs) }","title":"AQE"},{"location":"SparkSQL/AQE/#adaptive-execution-in-spark","text":"","title":"Adaptive execution in Spark"},{"location":"SparkSQL/AQE/#jira","text":"SPARK-31412 Feature requirement (with subtasks list) Design Doc SPARK-23128 The basic framework for the new Adaptive Query Execution SPARK-28177 Adjust post shuffle partition number in adaptive execution SPARK-29544 Optimize skewed join at runtime with new Adaptive Execution SPARK-31865 Fix complex AQE query stage not reused SPARK-35552 Make query stage materialized more readable SPARK-9850 Adaptive execution in Spark (original idea) Design Doc SPARK-9851 Support submitting map stages individually in DAGScheduler SPARK document Adaptive Query Execution","title":"Jira"},{"location":"SparkSQL/AQE/#queryexecutionpreparations","text":"org.apache.spark.sql.execution.QueryExecution#preparations private[execution] def preparations( sparkSession: SparkSession, adaptiveExecutionRule: Option[InsertAdaptiveSparkPlan] = None, subquery: Boolean): Seq[Rule[SparkPlan]] = { // `AdaptiveSparkPlanExec` is a leaf node. If inserted, all the following rules will be no-op // as the original plan is hidden behind `AdaptiveSparkPlanExec`. adaptiveExecutionRule.toSeq ++","title":"QueryExecution#preparations"},{"location":"SparkSQL/AQE/#insertadaptivesparkplan","text":"org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan /** * This rule wraps the query plan with an [[AdaptiveSparkPlanExec]], which executes the query plan * and re-optimize the plan during execution based on runtime data statistics. * * Note that this rule is stateful and thus should not be reused across query executions. */ case class InsertAdaptiveSparkPlan( adaptiveExecutionContext: AdaptiveExecutionContext) extends Rule[SparkPlan] { override def apply(plan: SparkPlan): SparkPlan = applyInternal(plan, false) private def applyInternal(plan: SparkPlan, isSubquery: Boolean): SparkPlan = plan match { case _ if !conf.adaptiveExecutionEnabled => plan case _: ExecutedCommandExec => plan case _: CommandResultExec => plan case c: DataWritingCommandExec => c.copy(child = apply(c.child)) case c: V2CommandExec => c.withNewChildren(c.children.map(apply)) case _ if shouldApplyAQE(plan, isSubquery) => if (supportAdaptive(plan)) { try { // Plan sub-queries recursively and pass in the shared stage cache for exchange reuse. // Fall back to non-AQE mode if AQE is not supported in any of the sub-queries. val subqueryMap = buildSubqueryMap(plan) val planSubqueriesRule = PlanAdaptiveSubqueries(subqueryMap) val preprocessingRules = Seq( planSubqueriesRule) // Run pre-processing rules. val newPlan = AdaptiveSparkPlanExec.applyPhysicalRules(plan, preprocessingRules) logDebug(s\"Adaptive execution enabled for plan: $plan\") AdaptiveSparkPlanExec(newPlan, adaptiveExecutionContext, preprocessingRules, isSubquery) } catch { case SubqueryAdaptiveNotSupportedException(subquery) => logWarning(s\"${SQLConf.ADAPTIVE_EXECUTION_ENABLED.key} is enabled \" + s\"but is not supported for sub-query: $subquery.\") plan } } else { logDebug(s\"${SQLConf.ADAPTIVE_EXECUTION_ENABLED.key} is enabled \" + s\"but is not supported for query: $plan.\") plan } case _ => plan } // AQE is only useful when the query has exchanges or sub-queries. This method returns true if // one of the following conditions is satisfied: // - The config ADAPTIVE_EXECUTION_FORCE_APPLY is true. // - The input query is from a sub-query. When this happens, it means we've already decided to // apply AQE for the main query and we must continue to do it. // - The query contains exchanges. // - The query may need to add exchanges. It's an overkill to run `EnsureRequirements` here, so // we just check `SparkPlan.requiredChildDistribution` and see if it's possible that the // the query needs to add exchanges later. // - The query contains sub-query. private def shouldApplyAQE(plan: SparkPlan, isSubquery: Boolean): Boolean = { conf.getConf(SQLConf.ADAPTIVE_EXECUTION_FORCE_APPLY) || isSubquery || { plan.exists { case _: Exchange => true case p if !p.requiredChildDistribution.forall(_ == UnspecifiedDistribution) => true case p => p.expressions.exists(_.exists { case _: SubqueryExpression => true case _ => false }) } } }","title":"InsertAdaptiveSparkPlan"},{"location":"SparkSQL/AQE/#adaptivesparkplanexec","text":"org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec /** * A root node to execute the query plan adaptively. It splits the query plan into independent * stages and executes them in order according to their dependencies. The query stage * materializes its output at the end. When one stage completes, the data statistics of the * materialized output will be used to optimize the remainder of the query. * * To create query stages, we traverse the query tree bottom up. When we hit an exchange node, * and if all the child query stages of this exchange node are materialized, we create a new * query stage for this exchange node. The new stage is then materialized asynchronously once it * is created. * * When one query stage finishes materialization, the rest query is re-optimized and planned based * on the latest statistics provided by all materialized stages. Then we traverse the query plan * again and create more stages if possible. After all stages have been materialized, we execute * the rest of the plan. */ case class AdaptiveSparkPlanExec( inputPlan: SparkPlan, @transient context: AdaptiveExecutionContext, @transient preprocessingRules: Seq[Rule[SparkPlan]], @transient isSubquery: Boolean, @transient override val supportsColumnar: Boolean = false) extends LeafExecNode { override def doExecute(): RDD[InternalRow] = { withFinalPlanUpdate(_.execute()) } private def withFinalPlanUpdate[T](fun: SparkPlan => T): T = { val plan = getFinalPhysicalPlan() val result = fun(plan) finalPlanUpdate result } ==================== getFinalPhysicalPlan ========================= private def getFinalPhysicalPlan(): SparkPlan = lock.synchronized { if (isFinalPlan) return currentPhysicalPlan // In case of this adaptive plan being executed out of `withActive` scoped functions, e.g., // `plan.queryExecution.rdd`, we need to set active session here as new plan nodes can be // created in the middle of the execution. context.session.withActive { val executionId = getExecutionId // Use inputPlan logicalLink here in case some top level physical nodes may be removed // during `initialPlan` var currentLogicalPlan = inputPlan.logicalLink.get var result = createQueryStages(currentPhysicalPlan) ===> createQueryStages val events = new LinkedBlockingQueue[StageMaterializationEvent]() val errors = new mutable.ArrayBuffer[Throwable]() var stagesToReplace = Seq.empty[QueryStageExec] while (!result.allChildStagesMaterialized) { ===> wait till all child stages materialized currentPhysicalPlan = result.newPlan if (result.newStages.nonEmpty) { stagesToReplace = result.newStages ++ stagesToReplace executionId.foreach(onUpdatePlan(_, result.newStages.map(_.plan))) // SPARK-33933: we should submit tasks of broadcast stages first, to avoid waiting // for tasks to be scheduled and leading to broadcast timeout. // This partial fix only guarantees the start of materialization for BroadcastQueryStage // is prior to others, but because the submission of collect job for broadcasting is // running in another thread, the issue is not completely resolved. val reorderedNewStages = result.newStages .sortWith { case (_: BroadcastQueryStageExec, _: BroadcastQueryStageExec) => false case (_: BroadcastQueryStageExec, _) => true case _ => false } ==================== stage.materialize() is run as Future async ========================= // Start materialization of all new stages and fail fast if any stages failed eagerly reorderedNewStages.foreach { stage => try { stage.materialize().onComplete { res => ===> materialize() is done async if (res.isSuccess) { events.offer(StageSuccess(stage, res.get)) ===> put into events queue } else { events.offer(StageFailure(stage, res.failed.get)) } }(AdaptiveSparkPlanExec.executionContext) } catch { case e: Throwable => cleanUpAndThrowException(Seq(e), Some(stage.id)) } } ========================================================================================== } ====== Wait on the next completed stage ====== // Wait on the next completed stage, which indicates new stats are available and probably // new stages can be created. There might be other stages that finish at around the same // time, so we process those stages too in order to reduce re-planning. val nextMsg = events.take() ===> block wait on event queue val rem = new util.ArrayList[StageMaterializationEvent]() events.drainTo(rem) (Seq(nextMsg) ++ rem.asScala).foreach { case StageSuccess(stage, res) => stage.resultOption.set(Some(res)) case StageFailure(stage, ex) => errors.append(ex) } // In case of errors, we cancel all running stages and throw exception. if (errors.nonEmpty) { cleanUpAndThrowException(errors.toSeq, None) } ======= re-optimizing and re-planning ====== // Try re-optimizing and re-planning. Adopt the new plan if its cost is equal to or less // than that of the current plan; otherwise keep the current physical plan together with // the current logical plan since the physical plan's logical links point to the logical // plan it has originated from. // Meanwhile, we keep a list of the query stages that have been created since last plan // update, which stands for the \"semantic gap\" between the current logical and physical // plans. And each time before re-planning, we replace the corresponding nodes in the // current logical plan with logical query stages to make it semantically in sync with // the current physical plan. Once a new plan is adopted and both logical and physical // plans are updated, we can clear the query stage list because at this point the two plans // are semantically and physically in sync again. val logicalPlan = replaceWithQueryStagesInLogicalPlan(currentLogicalPlan, stagesToReplace) val afterReOptimize = reOptimize(logicalPlan) if (afterReOptimize.isDefined) { val (newPhysicalPlan, newLogicalPlan) = afterReOptimize.get val origCost = costEvaluator.evaluateCost(currentPhysicalPlan) val newCost = costEvaluator.evaluateCost(newPhysicalPlan) if (newCost < origCost || (newCost == origCost && currentPhysicalPlan != newPhysicalPlan)) { logOnLevel(\"Plan changed:\\n\" + sideBySide(currentPhysicalPlan.treeString, newPhysicalPlan.treeString).mkString(\"\\n\")) cleanUpTempTags(newPhysicalPlan) currentPhysicalPlan = newPhysicalPlan currentLogicalPlan = newLogicalPlan stagesToReplace = Seq.empty[QueryStageExec] } } // Now that some stages have finished, we can try creating new stages. result = createQueryStages(currentPhysicalPlan) } // Run the final plan when there's no more unfinished stages. currentPhysicalPlan = applyPhysicalRules( optimizeQueryStage(result.newPlan, isFinalStage = true), postStageCreationRules(supportsColumnar), Some((planChangeLogger, \"AQE Post Stage Creation\"))) isFinalPlan = true executionId.foreach(onUpdatePlan(_, Seq(currentPhysicalPlan))) currentPhysicalPlan } } /** * This method is called recursively to traverse the plan tree bottom-up and create a new query * stage or try reusing an existing stage if the current node is an [[Exchange]] node and all of * its child stages have been materialized. * * With each call, it returns: * 1) The new plan replaced with [[QueryStageExec]] nodes where new stages are created. * 2) Whether the child query stages (if any) of the current node have all been materialized. * 3) A list of the new query stages that have been created. */ private def createQueryStages(plan: SparkPlan): CreateStageResult = plan match { case e: Exchange => // First have a quick check in the `stageCache` without having to traverse down the node. context.stageCache.get(e.canonicalized) match { case Some(existingStage) if conf.exchangeReuseEnabled => val stage = reuseQueryStage(existingStage, e) val isMaterialized = stage.isMaterialized CreateStageResult( newPlan = stage, allChildStagesMaterialized = isMaterialized, newStages = if (isMaterialized) Seq.empty else Seq(stage)) case _ => val result = createQueryStages(e.child) val newPlan = e.withNewChildren(Seq(result.newPlan)).asInstanceOf[Exchange] // Create a query stage only when all the child query stages are ready. if (result.allChildStagesMaterialized) { var newStage = newQueryStage(newPlan) if (conf.exchangeReuseEnabled) { // Check the `stageCache` again for reuse. If a match is found, ditch the new stage // and reuse the existing stage found in the `stageCache`, otherwise update the // `stageCache` with the new stage. val queryStage = context.stageCache.getOrElseUpdate( newStage.plan.canonicalized, newStage) if (queryStage.ne(newStage)) { newStage = reuseQueryStage(queryStage, e) } } val isMaterialized = newStage.isMaterialized CreateStageResult( newPlan = newStage, allChildStagesMaterialized = isMaterialized, newStages = if (isMaterialized) Seq.empty else Seq(newStage)) } else { CreateStageResult(newPlan = newPlan, allChildStagesMaterialized = false, newStages = result.newStages) } } case q: QueryStageExec => CreateStageResult(newPlan = q, allChildStagesMaterialized = q.isMaterialized, newStages = Seq.empty) case _ => if (plan.children.isEmpty) { CreateStageResult(newPlan = plan, allChildStagesMaterialized = true, newStages = Seq.empty) } else { val results = plan.children.map(createQueryStages) CreateStageResult( newPlan = plan.withNewChildren(results.map(_.newPlan)), allChildStagesMaterialized = results.forall(_.allChildStagesMaterialized), newStages = results.flatMap(_.newStages)) } } private def newQueryStage(e: Exchange): QueryStageExec = { val optimizedPlan = optimizeQueryStage(e.child, isFinalStage = false) val queryStage = e match { case s: ShuffleExchangeLike => val newShuffle = applyPhysicalRules( s.withNewChildren(Seq(optimizedPlan)), postStageCreationRules(outputsColumnar = s.supportsColumnar), Some((planChangeLogger, \"AQE Post Stage Creation\"))) if (!newShuffle.isInstanceOf[ShuffleExchangeLike]) { throw new IllegalStateException( \"Custom columnar rules cannot transform shuffle node to something else.\") } ShuffleQueryStageExec(currentStageId, newShuffle, s.canonicalized) case b: BroadcastExchangeLike => val newBroadcast = applyPhysicalRules( b.withNewChildren(Seq(optimizedPlan)), postStageCreationRules(outputsColumnar = b.supportsColumnar), Some((planChangeLogger, \"AQE Post Stage Creation\"))) if (!newBroadcast.isInstanceOf[BroadcastExchangeLike]) { throw new IllegalStateException( \"Custom columnar rules cannot transform broadcast node to something else.\") } BroadcastQueryStageExec(currentStageId, newBroadcast, b.canonicalized) } currentStageId += 1 setLogicalLinkForNewQueryStage(queryStage, e) queryStage } rules @transient private val costEvaluator = conf.getConf(SQLConf.ADAPTIVE_CUSTOM_COST_EVALUATOR_CLASS) match { case Some(className) => CostEvaluator.instantiate(className, session.sparkContext.getConf) case _ => SimpleCostEvaluator(conf.getConf(SQLConf.ADAPTIVE_FORCE_OPTIMIZE_SKEWED_JOIN)) } // A list of physical plan rules to be applied before creation of query stages. The physical // plan should reach a final status of query stages (i.e., no more addition or removal of // Exchange nodes) after running these rules. @transient private val queryStagePreparationRules: Seq[Rule[SparkPlan]] = { // For cases like `df.repartition(a, b).select(c)`, there is no distribution requirement for // the final plan, but we do need to respect the user-specified repartition. Here we ask // `EnsureRequirements` to not optimize out the user-specified repartition-by-col to work // around this case. val ensureRequirements = EnsureRequirements(requiredDistribution.isDefined, requiredDistribution) Seq( RemoveRedundantProjects, ensureRequirements, ValidateSparkPlan, ReplaceHashWithSortAgg, RemoveRedundantSorts, DisableUnnecessaryBucketedScan, OptimizeSkewedJoin(ensureRequirements) ) ++ context.session.sessionState.adaptiveRulesHolder.queryStagePrepRules } // A list of physical optimizer rules to be applied to a new stage before its execution. These // optimizations should be stage-independent. @transient private val queryStageOptimizerRules: Seq[Rule[SparkPlan]] = Seq( PlanAdaptiveDynamicPruningFilters(this), ReuseAdaptiveSubquery(context.subqueryCache), OptimizeSkewInRebalancePartitions, CoalesceShufflePartitions(context.session), // `OptimizeShuffleWithLocalRead` needs to make use of 'AQEShuffleReadExec.partitionSpecs' // added by `CoalesceShufflePartitions`, and must be executed after it. OptimizeShuffleWithLocalRead ) // This rule is stateful as it maintains the codegen stage ID. We can't create a fresh one every // time and need to keep it in a variable. @transient private val collapseCodegenStagesRule: Rule[SparkPlan] = CollapseCodegenStages() // A list of physical optimizer rules to be applied right after a new stage is created. The input // plan to these rules has exchange as its root node. private def postStageCreationRules(outputsColumnar: Boolean) = Seq( ApplyColumnarRulesAndInsertTransitions( context.session.sessionState.columnarRules, outputsColumnar), collapseCodegenStagesRule ) @transient val initialPlan = context.session.withActive { applyPhysicalRules( inputPlan, queryStagePreparationRules, Some((planChangeLogger, \"AQE Preparations\"))) } @volatile private var currentPhysicalPlan = initialPlan // The logical plan optimizer for re-optimizing the current logical plan. @transient private val optimizer = new AQEOptimizer(conf, session.sessionState.adaptiveRulesHolder.runtimeOptimizerRules) private def optimizeQueryStage(plan: SparkPlan, isFinalStage: Boolean): SparkPlan = { val optimized = queryStageOptimizerRules.foldLeft(plan) { case (latestPlan, rule) => val applied = rule.apply(latestPlan) val result = rule match { case _: AQEShuffleReadRule if !applied.fastEquals(latestPlan) => val distribution = if (isFinalStage) { // If `requiredDistribution` is None, it means `EnsureRequirements` will not optimize // out the user-specified repartition, thus we don't have a distribution requirement // for the final plan. requiredDistribution.getOrElse(UnspecifiedDistribution) } else { UnspecifiedDistribution } if (ValidateRequirements.validate(applied, distribution)) { applied } else { logDebug(s\"Rule ${rule.ruleName} is not applied as it breaks the \" + \"distribution requirement of the query plan.\") latestPlan } case _ => applied } planChangeLogger.logRule(rule.ruleName, latestPlan, result) result } planChangeLogger.logBatch(\"AQE Query Stage Optimization\", plan, optimized) optimized } /** * Re-optimize and run physical planning on the current logical plan based on the latest stats. */ private def reOptimize(logicalPlan: LogicalPlan): Option[(SparkPlan, LogicalPlan)] = { try { logicalPlan.invalidateStatsCache() val optimized = optimizer.execute(logicalPlan) val sparkPlan = context.session.sessionState.planner.plan(ReturnAnswer(optimized)).next() val newPlan = applyPhysicalRules( sparkPlan, preprocessingRules ++ queryStagePreparationRules, Some((planChangeLogger, \"AQE Replanning\"))) // When both enabling AQE and DPP, `PlanAdaptiveDynamicPruningFilters` rule will // add the `BroadcastExchangeExec` node manually in the DPP subquery, // not through `EnsureRequirements` rule. Therefore, when the DPP subquery is complicated // and need to be re-optimized, AQE also need to manually insert the `BroadcastExchangeExec` // node to prevent the loss of the `BroadcastExchangeExec` node in DPP subquery. // Here, we also need to avoid to insert the `BroadcastExchangeExec` node when the newPlan is // already the `BroadcastExchangeExec` plan after apply the `LogicalQueryStageStrategy` rule. val finalPlan = inputPlan match { case b: BroadcastExchangeLike if (!newPlan.isInstanceOf[BroadcastExchangeLike]) => b.withNewChildren(Seq(newPlan)) case _ => newPlan } Some((finalPlan, optimized)) } catch { case e: InvalidAQEPlanException[_] => logOnLevel(s\"Re-optimize - ${e.getMessage()}:\\n${e.plan}\") None } }","title":"AdaptiveSparkPlanExec"},{"location":"SparkSQL/AQE/#querystageexec","text":"org.apache.spark.sql.execution.adaptive.QueryStageExec /** * A query stage is an independent subgraph of the query plan. Query stage materializes its output * before proceeding with further operators of the query plan. The data statistics of the * materialized output can be used to optimize subsequent query stages. * * There are 2 kinds of query stages: * 1. Shuffle query stage. This stage materializes its output to shuffle files, and Spark launches * another job to execute the further operators. * 2. Broadcast query stage. This stage materializes its output to an array in driver JVM. Spark * broadcasts the array before executing the further operators. */ abstract class QueryStageExec extends LeafExecNode { @transient @volatile protected var _resultOption = new AtomicReference[Option[Any]](None) private[adaptive] def resultOption: AtomicReference[Option[Any]] = _resultOption def isMaterialized: Boolean = resultOption.get().isDefined /** * Compute the statistics of the query stage if executed, otherwise None. */ def computeStats(): Option[Statistics] = if (isMaterialized) { val runtimeStats = getRuntimeStatistics val dataSize = runtimeStats.sizeInBytes.max(0) val numOutputRows = runtimeStats.rowCount.map(_.max(0)) Some(Statistics(dataSize, numOutputRows, isRuntime = true)) } else { None } /** * Materialize this query stage, to prepare for the execution, like submitting map stages, * broadcasting data, etc. The caller side can use the returned [[Future]] to wait until this * stage is ready. */ def doMaterialize(): Future[Any] ====== materialize() is called by org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#getFinalPhysicalPlan === /** * Materialize this query stage, to prepare for the execution, like submitting map stages, * broadcasting data, etc. The caller side can use the returned [[Future]] to wait until this * stage is ready. */ final def materialize(): Future[Any] = { logDebug(s\"Materialize query stage ${this.getClass.getSimpleName}: $id\") doMaterialize() } =========================================================================================================","title":"QueryStageExec"},{"location":"SparkSQL/AQE/#shufflequerystageexec","text":"org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#getFinalPhysicalPlan => org.apache.spark.sql.execution.adaptive.QueryStageExec#materialize => org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec#doMaterialize=> org.apache.spark.sql.execution.exchange.ShuffleExchangeLike#submitShuffleJob=> org.apache.spark.sql.execution.SparkPlan#executeQuery=> org.apache.spark.sql.execution.exchange.ShuffleExchangeExec#mapOutputStatisticsFuture=> org.apache.spark.SparkContext#submitMapStage=> org.apache.spark.scheduler.DAGScheduler#submitMapStage=> org.apache.spark.scheduler.JobWaiter run test test(\"SPARK-37063: OptimizeSkewInRebalancePartitions support optimize non-root node\") /** * A shuffle query stage whose child is a [[ShuffleExchangeLike]] or [[ReusedExchangeExec]]. * * @param id the query stage id. * @param plan the underlying plan. * @param _canonicalized the canonicalized plan before applying query stage optimizer rules. */ case class ShuffleQueryStageExec( override val id: Int, override val plan: SparkPlan, override val _canonicalized: SparkPlan) extends QueryStageExec { @transient val shuffle = plan match { case s: ShuffleExchangeLike => s case ReusedExchangeExec(_, s: ShuffleExchangeLike) => s case _ => throw new IllegalStateException(s\"wrong plan for shuffle stage:\\n ${plan.treeString}\") } @transient private lazy val shuffleFuture = shuffle.submitShuffleJob ===> org.apache.spark.sql.execution.exchange.ShuffleExchangeLike#submitShuffleJob override def doMaterialize(): Future[Any] = shuffleFuture override def getRuntimeStatistics: Statistics = shuffle.runtimeStatistics org.apache.spark.sql.execution.exchange.ShuffleExchangeLike#submitShuffleJob /** * The asynchronous job that materializes the shuffle. It also does the preparations work, * such as waiting for the subqueries. */ final def submitShuffleJob: Future[MapOutputStatistics] = executeQuery { mapOutputStatisticsFuture } ===> org.apache.spark.sql.execution.exchange.ShuffleExchangeExec#mapOutputStatisticsFuture // 'mapOutputStatisticsFuture' is only needed when enable AQE. @transient override lazy val mapOutputStatisticsFuture: Future[MapOutputStatistics] = { if (inputRDD.getNumPartitions == 0) { Future.successful(null) } else { sparkContext.submitMapStage(shuffleDependency) } } ===> org.apache.spark.sql.execution.SparkPlan#executeQuery /** * Executes a query after preparing the query and adding query plan information to created RDDs * for visualization. */ protected final def executeQuery[T](query: => T): T = { RDDOperationScope.withScope(sparkContext, nodeName, false, true) { prepare() waitForSubqueries() query } } ===> org.apache.spark.SparkContext#submitMapStage /** * Submit a map stage for execution. This is currently an internal API only, but might be * promoted to DeveloperApi in the future. */ private[spark] def submitMapStage[K, V, C](dependency: ShuffleDependency[K, V, C]) : SimpleFutureAction[MapOutputStatistics] = { assertNotStopped() val callSite = getCallSite() var result: MapOutputStatistics = null val waiter = dagScheduler.submitMapStage( dependency, (r: MapOutputStatistics) => { result = r }, callSite, localProperties.get) new SimpleFutureAction[MapOutputStatistics](waiter, result) } ===> org.apache.spark.scheduler.DAGScheduler#submitMapStage /** * Submit a shuffle map stage to run independently and get a JobWaiter object back. The waiter * can be used to block until the job finishes executing or can be used to cancel the job. * This method is used for adaptive query planning, to run map stages and look at statistics * about their outputs before submitting downstream stages. * * @param dependency the ShuffleDependency to run a map stage for * @param callback function called with the result of the job, which in this case will be a * single MapOutputStatistics object showing how much data was produced for each partition * @param callSite where in the user program this job was submitted * @param properties scheduler properties to attach to this job, e.g. fair scheduler pool name */ def submitMapStage[K, V, C]( dependency: ShuffleDependency[K, V, C], callback: MapOutputStatistics => Unit, callSite: CallSite, properties: Properties): JobWaiter[MapOutputStatistics] = { val rdd = dependency.rdd val jobId = nextJobId.getAndIncrement() if (rdd.partitions.length == 0) { throw SparkCoreErrors.cannotRunSubmitMapStageOnZeroPartitionRDDError() } // SPARK-23626: `RDD.getPartitions()` can be slow, so we eagerly compute // `.partitions` on every RDD in the DAG to ensure that `getPartitions()` // is evaluated outside of the DAGScheduler's single-threaded event loop: eagerlyComputePartitionsForRddAndAncestors(rdd) // We create a JobWaiter with only one \"task\", which will be marked as complete when the whole // map stage has completed, and will be passed the MapOutputStatistics for that stage. // This makes it easier to avoid race conditions between the user code and the map output // tracker that might result if we told the user the stage had finished, but then they queries // the map output tracker and some node failures had caused the output statistics to be lost. val waiter = new JobWaiter[MapOutputStatistics]( this, jobId, 1, (_: Int, r: MapOutputStatistics) => callback(r)) eventProcessLoop.post(MapStageSubmitted( jobId, dependency, callSite, waiter, Utils.cloneProperties(properties))) waiter } ===> org.apache.spark.scheduler.JobWaiter /** * An object that waits for a DAGScheduler job to complete. As tasks finish, it passes their * results to the given handler function. */ private[spark] class JobWaiter[T]( dagScheduler: DAGScheduler, val jobId: Int, totalTasks: Int, resultHandler: (Int, T) => Unit) extends JobListener with Logging { stack in test(\"SPARK-37063: OptimizeSkewInRebalancePartitions support optimize non-root node\") at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:291) at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:244) at org.apache.spark.sql.execution.SparkPlan$$Lambda$2339.1114370802.apply(Unknown Source:-1) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243) at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:73) at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:72) at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:120) at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:185) - locked <0x362c> (a org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec) at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:185) at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:187) at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:286) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:284) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2336.1706125628.apply(Unknown Source:-1) at scala.collection.Iterator.foreach(Iterator.scala:943) at scala.collection.Iterator.foreach$(Iterator.scala:943) at scala.collection.AbstractIterator.foreach(Iterator.scala:1431) at scala.collection.IterableLike.foreach(IterableLike.scala:74) at scala.collection.IterableLike.foreach$(IterableLike.scala:73) at scala.collection.AbstractIterable.foreach(Iterable.scala:56) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:284) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2295.312068212.apply(Unknown Source:-1) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256) - locked <0x389c> (a java.lang.Object) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374) at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4384) at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3625) at org.apache.spark.sql.Dataset$$Lambda$2283.289194317.apply(Unknown Source:-1) at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4374) at org.apache.spark.sql.Dataset$$Lambda$2290.1482151715.apply(Unknown Source:-1) at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:529) at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4372) at org.apache.spark.sql.Dataset$$Lambda$2284.280804333.apply(Unknown Source:-1) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118) at org.apache.spark.sql.execution.SQLExecution$$$Lambda$1713.806842585.apply(Unknown Source:-1) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103) at org.apache.spark.sql.execution.SQLExecution$$$Lambda$1703.1020198427.apply(Unknown Source:-1) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4372) at org.apache.spark.sql.Dataset.collect(Dataset.scala:3625) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.runAdaptiveAndVerifyResult(AdaptiveQueryExecSuite.scala:83) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.checkRebalance$1(AdaptiveQueryExecSuite.scala:2417) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.$anonfun$new$273(AdaptiveQueryExecSuite.scala:2430) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite$$Lambda$2074.1374673778.apply$mcV$sp(Unknown Source:-1) at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf(SQLHelper.scala:54) at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf$(SQLHelper.scala:38) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.org$apache$spark$sql$test$SQLTestUtilsBase$$super$withSQLConf(AdaptiveQueryExecSuite.scala:51) at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf(SQLTestUtils.scala:247) at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf$(SQLTestUtils.scala:245) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.withSQLConf(AdaptiveQueryExecSuite.scala:51) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.$anonfun$new$270(AdaptiveQueryExecSuite.scala:2429) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite$$Lambda$2046.860717660.apply$mcV$sp(Unknown Source:-1) at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf(SQLHelper.scala:54) at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf$(SQLHelper.scala:38) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.org$apache$spark$sql$test$SQLTestUtilsBase$$super$withSQLConf(AdaptiveQueryExecSuite.scala:51) at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf(SQLTestUtils.scala:247) at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf$(SQLTestUtils.scala:245) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.withSQLConf(AdaptiveQueryExecSuite.scala:51) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.$anonfun$new$269(AdaptiveQueryExecSuite.scala:2411) at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite$$Lambda$2044.1413677222.apply$mcV$sp(Unknown Source:-1)","title":"ShuffleQueryStageExec"},{"location":"SparkSQL/AQE/#broadcastquerystageexec","text":"/** * A broadcast query stage whose child is a [[BroadcastExchangeLike]] or [[ReusedExchangeExec]]. * * @param id the query stage id. * @param plan the underlying plan. * @param _canonicalized the canonicalized plan before applying query stage optimizer rules. */ case class BroadcastQueryStageExec( override val id: Int, override val plan: SparkPlan, override val _canonicalized: SparkPlan) extends QueryStageExec { @transient val broadcast = plan match { case b: BroadcastExchangeLike => b case ReusedExchangeExec(_, b: BroadcastExchangeLike) => b case _ => throw new IllegalStateException(s\"wrong plan for broadcast stage:\\n ${plan.treeString}\") } override def doMaterialize(): Future[Any] = { broadcast.submitBroadcastJob } override def getRuntimeStatistics: Statistics = broadcast.runtimeStatistics","title":"BroadcastQueryStageExec"},{"location":"SparkSQL/AQE/#reusequerystage","text":"org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#createQueryStages => reuseQueryStage org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#reuseQueryStage private def reuseQueryStage(existing: QueryStageExec, exchange: Exchange): QueryStageExec = { val queryStage = existing.newReuseInstance(currentStageId, exchange.output) currentStageId += 1 setLogicalLinkForNewQueryStage(queryStage, exchange) queryStage } org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec#newReuseInstance override def newReuseInstance(newStageId: Int, newOutput: Seq[Attribute]): QueryStageExec = { val reuse = BroadcastQueryStageExec( newStageId, ReusedExchangeExec(newOutput, broadcast), _canonicalized) reuse._resultOption = this._resultOption reuse } org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec#newReuseInstance override def newReuseInstance(newStageId: Int, newOutput: Seq[Attribute]): QueryStageExec = { val reuse = ShuffleQueryStageExec( newStageId, ReusedExchangeExec(newOutput, shuffle), _canonicalized) reuse._resultOption = this._resultOption reuse }","title":"reuseQueryStage"},{"location":"SparkSQL/AQE/#adaptive-coalesce-partitions","text":"SQLConf val COALESCE_PARTITIONS_ENABLED = buildConf(\"spark.sql.adaptive.coalescePartitions.enabled\") .doc(s\"When true and '${ADAPTIVE_EXECUTION_ENABLED.key}' is true, Spark will coalesce \" + \"contiguous shuffle partitions according to the target size (specified by \" + s\"'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}'), to avoid too many small tasks.\") .version(\"3.0.0\") .booleanConf .createWithDefault(true) val COALESCE_PARTITIONS_PARALLELISM_FIRST = buildConf(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\") .doc(\"When true, Spark does not respect the target size specified by \" + s\"'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}' (default 64MB) when coalescing contiguous \" + \"shuffle partitions, but adaptively calculate the target size according to the default \" + \"parallelism of the Spark cluster. The calculated size is usually smaller than the \" + \"configured target size. This is to maximize the parallelism and avoid performance \" + \"regression when enabling adaptive query execution. It's recommended to set this config \" + \"to false and respect the configured target size.\") .version(\"3.2.0\") .booleanConf .createWithDefault(true) val COALESCE_PARTITIONS_MIN_PARTITION_SIZE = buildConf(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\") .doc(\"The minimum size of shuffle partitions after coalescing. This is useful when the \" + \"adaptively calculated target size is too small during partition coalescing.\") .version(\"3.2.0\") .bytesConf(ByteUnit.BYTE) .checkValue(_ > 0, \"minPartitionSize must be positive\") .createWithDefaultString(\"1MB\") val COALESCE_PARTITIONS_MIN_PARTITION_NUM = buildConf(\"spark.sql.adaptive.coalescePartitions.minPartitionNum\") .internal() .doc(\"(deprecated) The suggested (not guaranteed) minimum number of shuffle partitions \" + \"after coalescing. If not set, the default value is the default parallelism of the \" + \"Spark cluster. This configuration only has an effect when \" + s\"'${ADAPTIVE_EXECUTION_ENABLED.key}' and \" + s\"'${COALESCE_PARTITIONS_ENABLED.key}' are both true.\") .version(\"3.0.0\") .intConf .checkValue(_ > 0, \"The minimum number of partitions must be positive.\") .createOptional val COALESCE_PARTITIONS_INITIAL_PARTITION_NUM = buildConf(\"spark.sql.adaptive.coalescePartitions.initialPartitionNum\") .doc(\"The initial number of shuffle partitions before coalescing. If not set, it equals to \" + s\"${SHUFFLE_PARTITIONS.key}. This configuration only has an effect when \" + s\"'${ADAPTIVE_EXECUTION_ENABLED.key}' and '${COALESCE_PARTITIONS_ENABLED.key}' \" + \"are both true.\") .version(\"3.0.0\") .intConf .checkValue(_ > 0, \"The initial number of partitions must be positive.\") .createOptional org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#queryStageOptimizerRules // A list of physical optimizer rules to be applied to a new stage before its execution. These // optimizations should be stage-independent. @transient private val queryStageOptimizerRules: Seq[Rule[SparkPlan]] = Seq( PlanAdaptiveDynamicPruningFilters(this), ReuseAdaptiveSubquery(context.subqueryCache), OptimizeSkewInRebalancePartitions, ====== rule for coalesce partitions ========== CoalesceShufflePartitions(context.session), ============================================== // `OptimizeShuffleWithLocalRead` needs to make use of 'AQEShuffleReadExec.partitionSpecs' // added by `CoalesceShufflePartitions`, and must be executed after it. OptimizeShuffleWithLocalRead ) org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions /** * A rule to coalesce the shuffle partitions based on the map output statistics, which can * avoid many small reduce tasks that hurt performance. */ case class CoalesceShufflePartitions(session: SparkSession) extends AQEShuffleReadRule { override def apply(plan: SparkPlan): SparkPlan = { if (!conf.coalesceShufflePartitionsEnabled) { return plan } // Ideally, this rule should simply coalesce partitions w.r.t. the target size specified by // ADVISORY_PARTITION_SIZE_IN_BYTES (default 64MB). To avoid perf regression in AQE, this // rule by default tries to maximize the parallelism and set the target size to // `total shuffle size / Spark default parallelism`. In case the `Spark default parallelism` // is too big, this rule also respect the minimum partition size specified by // COALESCE_PARTITIONS_MIN_PARTITION_SIZE (default 1MB). // For history reason, this rule also need to support the config // COALESCE_PARTITIONS_MIN_PARTITION_NUM. We should remove this config in the future. val minNumPartitions = conf.getConf(SQLConf.COALESCE_PARTITIONS_MIN_PARTITION_NUM).getOrElse { if (conf.getConf(SQLConf.COALESCE_PARTITIONS_PARALLELISM_FIRST)) { // We fall back to Spark default parallelism if the minimum number of coalesced partitions // is not set, so to avoid perf regressions compared to no coalescing. session.sparkContext.defaultParallelism } else { // If we don't need to maximize the parallelism, we set `minPartitionNum` to 1, so that // the specified advisory partition size will be respected. 1 } } val advisoryTargetSize = conf.getConf(SQLConf.ADVISORY_PARTITION_SIZE_IN_BYTES) val minPartitionSize = if (Utils.isTesting) { // In the tests, we usually set the target size to a very small value that is even smaller // than the default value of the min partition size. Here we also adjust the min partition // size to be not larger than 20% of the target size, so that the tests don't need to set // both configs all the time to check the coalescing behavior. conf.getConf(SQLConf.COALESCE_PARTITIONS_MIN_PARTITION_SIZE).min(advisoryTargetSize / 5) } else { conf.getConf(SQLConf.COALESCE_PARTITIONS_MIN_PARTITION_SIZE) } // Sub-plans under the Union operator can be coalesced independently, so we can divide them // into independent \"coalesce groups\", and all shuffle stages within each group have to be // coalesced together. val coalesceGroups = collectCoalesceGroups(plan) // Divide minimum task parallelism among coalesce groups according to their data sizes. val minNumPartitionsByGroup = if (coalesceGroups.length == 1) { Seq(math.max(minNumPartitions, 1)) } else { val sizes = coalesceGroups.map(_.flatMap(_.shuffleStage.mapStats.map(_.bytesByPartitionId.sum)).sum) val totalSize = sizes.sum sizes.map { size => val num = if (totalSize > 0) { math.round(minNumPartitions * 1.0 * size / totalSize) } else { minNumPartitions } math.max(num.toInt, 1) } } val specsMap = mutable.HashMap.empty[Int, Seq[ShufflePartitionSpec]] // Coalesce partitions for each coalesce group independently. coalesceGroups.zip(minNumPartitionsByGroup).foreach { case (shuffleStages, minNumPartitions) => val newPartitionSpecs = ShufflePartitionsUtil.coalescePartitions( shuffleStages.map(_.shuffleStage.mapStats), shuffleStages.map(_.partitionSpecs), advisoryTargetSize = advisoryTargetSize, minNumPartitions = minNumPartitions, minPartitionSize = minPartitionSize) if (newPartitionSpecs.nonEmpty) { shuffleStages.zip(newPartitionSpecs).map { case (stageInfo, partSpecs) => specsMap.put(stageInfo.shuffleStage.id, partSpecs) } } } if (specsMap.nonEmpty) { updateShuffleReads(plan, specsMap.toMap) } else { plan } } private def updateShuffleReads( plan: SparkPlan, specsMap: Map[Int, Seq[ShufflePartitionSpec]]): SparkPlan = plan match { // Even for shuffle exchange whose input RDD has 0 partition, we should still update its // `partitionStartIndices`, so that all the leaf shuffles in a stage have the same // number of output partitions. case ShuffleStageInfo(stage, _) => specsMap.get(stage.id).map { specs => AQEShuffleReadExec(stage, specs) }.getOrElse(plan) case other => other.mapChildren(updateShuffleReads(_, specsMap)) } org.apache.spark.sql.execution.adaptive.AQEShuffleReadRule /** * A rule that may create [[AQEShuffleReadExec]] on top of [[ShuffleQueryStageExec]] and change the * plan output partitioning. The AQE framework will skip the rule if it leads to extra shuffles. */ trait AQEShuffleReadRule extends Rule[SparkPlan] { /** * Returns the list of [[ShuffleOrigin]]s supported by this rule. */ protected def supportedShuffleOrigins: Seq[ShuffleOrigin] protected def isSupported(shuffle: ShuffleExchangeLike): Boolean = { supportedShuffleOrigins.contains(shuffle.shuffleOrigin) } } org.apache.spark.sql.execution.adaptive.AQEShuffleReadExec /** * A wrapper of shuffle query stage, which follows the given partition arrangement. * * @param child It is usually `ShuffleQueryStageExec`, but can be the shuffle exchange * node during canonicalization. * @param partitionSpecs The partition specs that defines the arrangement, requires at least one * partition. */ case class AQEShuffleReadExec private( child: SparkPlan, partitionSpecs: Seq[ShufflePartitionSpec]) extends UnaryExecNode { private def shuffleStage = child match { case stage: ShuffleQueryStageExec => Some(stage) case _ => None } private lazy val shuffleRDD: RDD[_] = { shuffleStage match { case Some(stage) => sendDriverMetrics() stage.shuffle.getShuffleRDD(partitionSpecs.toArray) case _ => throw new IllegalStateException(\"operating on canonicalized plan\") } } override protected def doExecute(): RDD[InternalRow] = { shuffleRDD.asInstanceOf[RDD[InternalRow]] } org.apache.spark.sql.execution.exchange.ShuffleExchangeExec#getShuffleRDD override def getShuffleRDD(partitionSpecs: Array[ShufflePartitionSpec]): RDD[InternalRow] = { new ShuffledRowRDD(shuffleDependency, readMetrics, partitionSpecs) }","title":"Adaptive coalesce partitions"},{"location":"SparkSQL/Aggregation/","text":"Aggregation code test case test(\u201cgroupBy\u201d) HashAggregateExec BufferedRowIterator GeneratedIteratorForCodegenStage1 inputRDDs Aggregation \u00b6 code \u00b6 test case test(\u201cgroupBy\u201d) \u00b6 class DataFrameAggregateSuite extends QueryTest with SharedSparkSession with AdaptiveSparkPlanHelper { import testImplicits._ val absTol = 1e-8 test(\"groupBy1\") { checkAnswer( testData2.groupBy(\"a\").agg(sum($\"b\")), Seq(Row(1, 3), Row(2, 3), Row(3, 3)) ) } test(\"count withSQLConf\") { for ((wholeStage, useObjectHashAgg) <- Seq( // (true, true), // (true, false), (false, true), // (false, false) )) { withSQLConf( (SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, wholeStage.toString), (SQLConf.USE_OBJECT_HASH_AGG.key, useObjectHashAgg.toString)) { checkAnswer( testData2.groupBy(\"a\").agg(count(\"*\")), Row(1, 2) :: Row(2, 2) :: Row(3, 2) :: Nil ) } } } HashAggregateExec \u00b6 org.apache.spark.sql.execution.aggregate.HashAggregateExec#doExecute is not used if whole code gen enabled org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport#doProduce protected override def doProduce(ctx: CodegenContext): String = { if (groupingExpressions.isEmpty) { doProduceWithoutKeys(ctx) } else { doProduceWithKeys(ctx) } } => org.apache.spark.sql.execution.aggregate.HashAggregateExec#doProduceWithKeys org.apache.spark.sql.execution.InputAdapter ====> Scan[obj#2] => org.apache.spark.sql.execution.InputRDDCodegen#doProduce => org.apache.spark.sql.execution.InputAdapter (call InputAdapter\u2019s consume which is in its parent CodegenSupport) => org.apache.spark.sql.execution.CodegenSupport#consume => org.apache.spark.sql.execution.CodegenSupport#constructDoConsumeFunction => org.apache.spark.sql.execution.SerializeFromObjectExec#doConsume => org.apache.spark.sql.execution.aggregate.HashAggregateExec (call HashAggregateExec\u2019s doConsume which is in its parent AggregateCodegenSupport) => org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport#doConsume => org.apache.spark.sql.execution.aggregate.HashAggregateExec#doConsumeWithKeys BufferedRowIterator \u00b6 /** * An iterator interface used to pull the output from generated function for multiple operators * (whole stage codegen). */ public abstract class BufferedRowIterator { protected LinkedList<InternalRow> currentRows = new LinkedList<>(); // used when there is no column in output protected UnsafeRow unsafeRow = new UnsafeRow(0); private long startTimeNs = System.nanoTime(); protected int partitionIndex = -1; public boolean hasNext() throws IOException { if (currentRows.isEmpty()) { processNext(); } return !currentRows.isEmpty(); } public void append(InternalRow row) { ===> called by GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeysOutput_0 currentRows.add(row); } /** * Returns whether `processNext()` should stop processing next row from `input` or not. * * If it returns true, the caller should exit the loop (return from processNext()). */ public boolean shouldStop() { return !currentRows.isEmpty(); } GeneratedIteratorForCodegenStage1 \u00b6 org.apache.spark.sql.execution.WholeStageCodegenExec#doCodeGen => \u2018cleanedSource\u2019 final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator { => processNext() public Object generate(Object[] references) { return new GeneratedIteratorForCodegenStage1(references); } /*wsc_codegenStageId*/ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator { private Object[] references; private scala.collection.Iterator[] inputs; private boolean hashAgg_initAgg_0; private boolean hashAgg_bufIsNull_0; private long hashAgg_bufValue_0; private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0; private org.apache.spark.unsafe.KVIterator < UnsafeRow, UnsafeRow > hashAgg_fastHashMapIter_0; private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0; private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0; private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0; private scala.collection.Iterator inputadapter_input_0; private boolean hashAgg_hashAgg_isNull_6_0; private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] serializefromobject_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[4]; public GeneratedIteratorForCodegenStage1(Object[] references) { this.references = references; } public void init(int index, scala.collection.Iterator[] inputs) { partitionIndex = index; this.inputs = inputs; inputadapter_input_0 = inputs[0]; serializefromobject_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0); serializefromobject_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0); serializefromobject_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0); serializefromobject_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0); } public class hashAgg_FastHashMap_0 { private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch; private int[] buckets; private int capacity = 1 << 16; private double loadFactor = 0.5; private int numBuckets = (int)(capacity / loadFactor); private int maxSteps = 2; private int numRows = 0; private Object emptyVBase; private long emptyVOff; private int emptyVLen; private boolean isBatchFull = false; private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter; public hashAgg_FastHashMap_0( org.apache.spark.memory.TaskMemoryManager taskMemoryManager, InternalRow emptyAggregationBuffer) { batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */ ), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */ ), taskMemoryManager, capacity); final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */ )); final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes(); emptyVBase = emptyBuffer; emptyVOff = Platform.BYTE_ARRAY_OFFSET; emptyVLen = emptyBuffer.length; agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter( 1, 0); buckets = new int[numBuckets]; java.util.Arrays.fill(buckets, -1); } public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(int hashAgg_key_0) { long h = hash(hashAgg_key_0); int step = 0; int idx = (int) h & (numBuckets - 1); while (step < maxSteps) { // Return bucket index if it's either an empty slot or already contains the key if (buckets[idx] == -1) { if (numRows < capacity && !isBatchFull) { agg_rowWriter.reset(); agg_rowWriter.write(0, hashAgg_key_0); org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result = agg_rowWriter.getRow(); Object kbase = agg_result.getBaseObject(); long koff = agg_result.getBaseOffset(); int klen = agg_result.getSizeInBytes(); UnsafeRow vRow = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen); if (vRow == null) { isBatchFull = true; } else { buckets[idx] = numRows++; } return vRow; } else { // No more space return null; } } else if (equals(idx, hashAgg_key_0)) { return batch.getValueRow(buckets[idx]); } idx = (idx + 1) & (numBuckets - 1); step++; } // Didn't find it return null; } private boolean equals(int idx, int hashAgg_key_0) { UnsafeRow row = batch.getKeyRow(buckets[idx]); return (row.getInt(0) == hashAgg_key_0); } private long hash(int hashAgg_key_0) { long hashAgg_hash_0 = 0; int hashAgg_result_0 = hashAgg_key_0; hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2); return hashAgg_hash_0; } public org.apache.spark.unsafe.KVIterator < UnsafeRow, UnsafeRow > rowIterator() { return batch.rowIterator(); } public void close() { batch.close(); } } private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException { while (inputadapter_input_0.hasNext()) { InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next(); boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0); org.apache.spark.sql.test.SQLTestData$TestData2 inputadapter_value_0 = inputadapter_isNull_0 ? null : ((org.apache.spark.sql.test.SQLTestData$TestData2) inputadapter_row_0.get(0, null)); serializefromobject_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0); // shouldStop check is eliminated } hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator(); hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */ ).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */ ), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */ ), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */ ), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */ )); } private void hashAgg_doConsume_0(int hashAgg_expr_0_0, int hashAgg_expr_1_0) throws java.io.IOException { UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null; UnsafeRow hashAgg_fastAggBuffer_0 = null; if (!false) { hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert( hashAgg_expr_0_0); } // Cannot find the key in fast hash map, try regular hash map. if (hashAgg_fastAggBuffer_0 == null) { // generate grouping key serializefromobject_mutableStateArray_0[2].reset(); serializefromobject_mutableStateArray_0[2].write(0, hashAgg_expr_0_0); int hashAgg_unsafeRowKeyHash_0 = (serializefromobject_mutableStateArray_0[2].getRow()).hashCode(); if (true) { // try to get the buffer from hash map hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((serializefromobject_mutableStateArray_0[2].getRow()), hashAgg_unsafeRowKeyHash_0); } // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based // aggregation after processing all input rows. if (hashAgg_unsafeRowAggBuffer_0 == null) { if (hashAgg_sorter_0 == null) { hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter(); } else { hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter()); } // the hash map had be spilled, it should have enough memory now, // try to allocate buffer again. hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow( (serializefromobject_mutableStateArray_0[2].getRow()), hashAgg_unsafeRowKeyHash_0); if (hashAgg_unsafeRowAggBuffer_0 == null) { // failed to allocate the first page throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\"); } } } // Updates the proper row buffer if (hashAgg_fastAggBuffer_0 != null) { hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0; } // common sub-expressions // evaluate aggregate functions and update aggregation buffers hashAgg_hashAgg_isNull_6_0 = true; long hashAgg_value_7 = -1 L; do { boolean hashAgg_isNull_7 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0); long hashAgg_value_8 = hashAgg_isNull_7 ? -1 L : (hashAgg_unsafeRowAggBuffer_0.getLong(0)); if (!hashAgg_isNull_7) { hashAgg_hashAgg_isNull_6_0 = false; hashAgg_value_7 = hashAgg_value_8; continue; } if (!false) { hashAgg_hashAgg_isNull_6_0 = false; hashAgg_value_7 = 0 L; continue; } } while (false); boolean hashAgg_isNull_9 = false; long hashAgg_value_10 = -1 L; if (!false) { hashAgg_value_10 = (long) hashAgg_expr_1_0; } long hashAgg_value_6 = -1 L; hashAgg_value_6 = hashAgg_value_7 + hashAgg_value_10; hashAgg_unsafeRowAggBuffer_0.setLong(0, hashAgg_value_6); } private void serializefromobject_doConsume_0(InternalRow inputadapter_row_0, org.apache.spark.sql.test.SQLTestData$TestData2 serializefromobject_expr_0_0, boolean serializefromobject_exprIsNull_0_0) throws java.io.IOException { if (serializefromobject_exprIsNull_0_0) { throw new NullPointerException(((java.lang.String) references[7] /* errMsg */ )); } boolean serializefromobject_isNull_0 = true; int serializefromobject_value_0 = -1; serializefromobject_isNull_0 = false; if (!serializefromobject_isNull_0) { serializefromobject_value_0 = serializefromobject_expr_0_0.a(); } if (serializefromobject_exprIsNull_0_0) { throw new NullPointerException(((java.lang.String) references[8] /* errMsg */ )); } boolean serializefromobject_isNull_4 = true; int serializefromobject_value_4 = -1; serializefromobject_isNull_4 = false; if (!serializefromobject_isNull_4) { serializefromobject_value_4 = serializefromobject_expr_0_0.b(); } hashAgg_doConsume_0(serializefromobject_value_0, serializefromobject_value_4); } private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0) throws java.io.IOException { ((org.apache.spark.sql.execution.metric.SQLMetric) references[9] /* numOutputRows */ ).add(1); int hashAgg_value_12 = hashAgg_keyTerm_0.getInt(0); boolean hashAgg_isNull_12 = hashAgg_bufferTerm_0.isNullAt(0); long hashAgg_value_13 = hashAgg_isNull_12 ? -1 L : (hashAgg_bufferTerm_0.getLong(0)); serializefromobject_mutableStateArray_0[3].reset(); serializefromobject_mutableStateArray_0[3].zeroOutNullBytes(); serializefromobject_mutableStateArray_0[3].write(0, hashAgg_value_12); if (hashAgg_isNull_12) { serializefromobject_mutableStateArray_0[3].setNullAt(1); } else { serializefromobject_mutableStateArray_0[3].write(1, hashAgg_value_13); } append((serializefromobject_mutableStateArray_0[3].getRow())); ===> append() is in 'BufferedRowIterator' } protected void processNext() throws java.io.IOException { if (!hashAgg_initAgg_0) { hashAgg_initAgg_0 = true; hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */ ).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */ ).getEmptyAggregationBuffer()); ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */ ).getTaskContext().addTaskCompletionListener( new org.apache.spark.util.TaskCompletionListener() { @Override public void onTaskCompletion(org.apache.spark.TaskContext context) { hashAgg_fastHashMap_0.close(); } }); hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */ ).createHashMap(); long wholestagecodegen_beforeAgg_0 = System.nanoTime(); hashAgg_doAggregateWithKeys_0(); ((org.apache.spark.sql.execution.metric.SQLMetric) references[10] /* aggTime */ ).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000); } // output the result while (hashAgg_fastHashMapIter_0.next()) { UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey(); UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue(); hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0); if (shouldStop()) return; } hashAgg_fastHashMap_0.close(); while (hashAgg_mapIter_0.next()) { UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey(); UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue(); hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0); if (shouldStop()) return; } hashAgg_mapIter_0.close(); if (hashAgg_sorter_0 == null) { hashAgg_hashMap_0.free(); } } } inputRDDs \u00b6 plan *(1) HashAggregate(keys=[a#3], functions=[partial_sum(b#4)], output=[a#3, sum#17L]) +- *(1) SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData2, true])).a AS a#3, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData2, true])).b AS b#4] +- Scan[obj#2] code org.apache.spark.sql.execution.WholeStageCodegenExec#doExecute override def doExecute(): RDD[InternalRow] = { val (ctx, cleanedSource) = doCodeGen() // try to compile and fallback if it failed val (_, compiledCodeStats) = try { CodeGenerator.compile(cleanedSource) } catch { case NonFatal(_) if !Utils.isTesting && conf.codegenFallback => // We should already saw the error message logWarning(s\"Whole-stage codegen disabled for plan (id=$codegenStageId):\\n $treeString\") return child.execute() } // Check if compiled code has a too large function if (compiledCodeStats.maxMethodCodeSize > conf.hugeMethodLimit) { logInfo(s\"Found too long generated codes and JIT optimization might not work: \" + s\"the bytecode size (${compiledCodeStats.maxMethodCodeSize}) is above the limit \" + s\"${conf.hugeMethodLimit}, and the whole-stage codegen was disabled \" + s\"for this plan (id=$codegenStageId). To avoid this, you can raise the limit \" + s\"`${SQLConf.WHOLESTAGE_HUGE_METHOD_LIMIT.key}`:\\n$treeString\") return child.execute() } val references = ctx.references.toArray val durationMs = longMetric(\"pipelineTime\") // Even though rdds is an RDD[InternalRow] it may actually be an RDD[ColumnarBatch] with // type erasure hiding that. This allows for the input to a code gen stage to be columnar, // but the output must be rows. val rdds = child.asInstanceOf[CodegenSupport].inputRDDs() assert(rdds.size <= 2, \"Up to two input RDDs can be supported\") val evaluatorFactory = new WholeStageCodegenEvaluatorFactory( cleanedSource, durationMs, references) if (rdds.length == 1) { if (conf.usePartitionEvaluator) { rdds.head.mapPartitionsWithEvaluator(evaluatorFactory) ===> Return a new RDD by applying an evaluator to each partition of this RDD. The given evaluator factory will be serialized and sent to executors, and each task will create an evaluator with the factory, and use the evaluator to transform the data of the input partition. } else { rdds.head.mapPartitionsWithIndex { (index, iter) => val evaluator = evaluatorFactory.createEvaluator() evaluator.eval(index, iter) ===> run the generated code } } } else { // Right now, we support up to two input RDDs. if (conf.usePartitionEvaluator) { rdds.head.zipPartitionsWithEvaluator(rdds(1), evaluatorFactory) } else { rdds.head.zipPartitions(rdds(1)) { (leftIter, rightIter) => Iterator((leftIter, rightIter)) // a small hack to obtain the correct partition index }.mapPartitionsWithIndex { (index, zippedIter) => val (leftIter, rightIter) = zippedIter.next() val evaluator = evaluatorFactory.createEvaluator() evaluator.eval(index, leftIter, rightIter) } } } } => org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport#inputRDDs, org.apache.spark.sql.execution.SerializeFromObjectExec#inputRDDs override def inputRDDs(): Seq[RDD[InternalRow]] = { child.asInstanceOf[CodegenSupport].inputRDDs() } => org.apache.spark.sql.execution.InputAdapter /** * InputAdapter is used to hide a SparkPlan from a subtree that supports codegen. * * This is the leaf node of a tree with WholeStageCodegen that is used to generate code * that consumes an RDD iterator of InternalRow. */ case class InputAdapter(child: SparkPlan) extends UnaryExecNode with InputRDDCodegen { // `InputAdapter` can only generate code to process the rows from its child. If the child produces // columnar batches, there must be a `ColumnarToRowExec` above `InputAdapter` to handle it by // overriding `inputRDDs` and calling `InputAdapter#executeColumnar` directly. override def inputRDD: RDD[InternalRow] = child.execute() => org.apache.spark.sql.execution.InputRDDCodegen /** * Leaf codegen node reading from a single RDD. */ trait InputRDDCodegen extends CodegenSupport { def inputRDD: RDD[InternalRow] ... override def inputRDDs(): Seq[RDD[InternalRow]] = { inputRDD :: Nil } => inputRDD is in InputAdapter (override def inputRDD: RDD[InternalRow] = child.execute()) whose \u2018child\u2019 is ExternalRDDScanExec org.apache.spark.sql.execution.ExternalRDDScanExec /** Physical plan node for scanning data from an RDD. */ case class ExternalRDDScanExec[T]( outputObjAttr: Attribute, rdd: RDD[T]) extends LeafExecNode with ObjectProducerExec { ExternalRDDScanExec is initialized at object BasicOperators extends Strategy { case ExternalRDD(outputObjAttr, rdd) => ExternalRDDScanExec(outputObjAttr, rdd) :: Nil","title":"Aggregation"},{"location":"SparkSQL/Aggregation/#aggregation","text":"","title":"Aggregation"},{"location":"SparkSQL/Aggregation/#code","text":"","title":"code"},{"location":"SparkSQL/Aggregation/#test-case-testgroupby","text":"class DataFrameAggregateSuite extends QueryTest with SharedSparkSession with AdaptiveSparkPlanHelper { import testImplicits._ val absTol = 1e-8 test(\"groupBy1\") { checkAnswer( testData2.groupBy(\"a\").agg(sum($\"b\")), Seq(Row(1, 3), Row(2, 3), Row(3, 3)) ) } test(\"count withSQLConf\") { for ((wholeStage, useObjectHashAgg) <- Seq( // (true, true), // (true, false), (false, true), // (false, false) )) { withSQLConf( (SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, wholeStage.toString), (SQLConf.USE_OBJECT_HASH_AGG.key, useObjectHashAgg.toString)) { checkAnswer( testData2.groupBy(\"a\").agg(count(\"*\")), Row(1, 2) :: Row(2, 2) :: Row(3, 2) :: Nil ) } } }","title":"test case test(\"groupBy\")"},{"location":"SparkSQL/Aggregation/#hashaggregateexec","text":"org.apache.spark.sql.execution.aggregate.HashAggregateExec#doExecute is not used if whole code gen enabled org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport#doProduce protected override def doProduce(ctx: CodegenContext): String = { if (groupingExpressions.isEmpty) { doProduceWithoutKeys(ctx) } else { doProduceWithKeys(ctx) } } => org.apache.spark.sql.execution.aggregate.HashAggregateExec#doProduceWithKeys org.apache.spark.sql.execution.InputAdapter ====> Scan[obj#2] => org.apache.spark.sql.execution.InputRDDCodegen#doProduce => org.apache.spark.sql.execution.InputAdapter (call InputAdapter\u2019s consume which is in its parent CodegenSupport) => org.apache.spark.sql.execution.CodegenSupport#consume => org.apache.spark.sql.execution.CodegenSupport#constructDoConsumeFunction => org.apache.spark.sql.execution.SerializeFromObjectExec#doConsume => org.apache.spark.sql.execution.aggregate.HashAggregateExec (call HashAggregateExec\u2019s doConsume which is in its parent AggregateCodegenSupport) => org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport#doConsume => org.apache.spark.sql.execution.aggregate.HashAggregateExec#doConsumeWithKeys","title":"HashAggregateExec"},{"location":"SparkSQL/Aggregation/#bufferedrowiterator","text":"/** * An iterator interface used to pull the output from generated function for multiple operators * (whole stage codegen). */ public abstract class BufferedRowIterator { protected LinkedList<InternalRow> currentRows = new LinkedList<>(); // used when there is no column in output protected UnsafeRow unsafeRow = new UnsafeRow(0); private long startTimeNs = System.nanoTime(); protected int partitionIndex = -1; public boolean hasNext() throws IOException { if (currentRows.isEmpty()) { processNext(); } return !currentRows.isEmpty(); } public void append(InternalRow row) { ===> called by GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeysOutput_0 currentRows.add(row); } /** * Returns whether `processNext()` should stop processing next row from `input` or not. * * If it returns true, the caller should exit the loop (return from processNext()). */ public boolean shouldStop() { return !currentRows.isEmpty(); }","title":"BufferedRowIterator"},{"location":"SparkSQL/Aggregation/#generatediteratorforcodegenstage1","text":"org.apache.spark.sql.execution.WholeStageCodegenExec#doCodeGen => \u2018cleanedSource\u2019 final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator { => processNext() public Object generate(Object[] references) { return new GeneratedIteratorForCodegenStage1(references); } /*wsc_codegenStageId*/ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator { private Object[] references; private scala.collection.Iterator[] inputs; private boolean hashAgg_initAgg_0; private boolean hashAgg_bufIsNull_0; private long hashAgg_bufValue_0; private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0; private org.apache.spark.unsafe.KVIterator < UnsafeRow, UnsafeRow > hashAgg_fastHashMapIter_0; private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0; private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0; private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0; private scala.collection.Iterator inputadapter_input_0; private boolean hashAgg_hashAgg_isNull_6_0; private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] serializefromobject_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[4]; public GeneratedIteratorForCodegenStage1(Object[] references) { this.references = references; } public void init(int index, scala.collection.Iterator[] inputs) { partitionIndex = index; this.inputs = inputs; inputadapter_input_0 = inputs[0]; serializefromobject_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0); serializefromobject_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0); serializefromobject_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0); serializefromobject_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0); } public class hashAgg_FastHashMap_0 { private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch; private int[] buckets; private int capacity = 1 << 16; private double loadFactor = 0.5; private int numBuckets = (int)(capacity / loadFactor); private int maxSteps = 2; private int numRows = 0; private Object emptyVBase; private long emptyVOff; private int emptyVLen; private boolean isBatchFull = false; private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter; public hashAgg_FastHashMap_0( org.apache.spark.memory.TaskMemoryManager taskMemoryManager, InternalRow emptyAggregationBuffer) { batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */ ), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */ ), taskMemoryManager, capacity); final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */ )); final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes(); emptyVBase = emptyBuffer; emptyVOff = Platform.BYTE_ARRAY_OFFSET; emptyVLen = emptyBuffer.length; agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter( 1, 0); buckets = new int[numBuckets]; java.util.Arrays.fill(buckets, -1); } public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(int hashAgg_key_0) { long h = hash(hashAgg_key_0); int step = 0; int idx = (int) h & (numBuckets - 1); while (step < maxSteps) { // Return bucket index if it's either an empty slot or already contains the key if (buckets[idx] == -1) { if (numRows < capacity && !isBatchFull) { agg_rowWriter.reset(); agg_rowWriter.write(0, hashAgg_key_0); org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result = agg_rowWriter.getRow(); Object kbase = agg_result.getBaseObject(); long koff = agg_result.getBaseOffset(); int klen = agg_result.getSizeInBytes(); UnsafeRow vRow = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen); if (vRow == null) { isBatchFull = true; } else { buckets[idx] = numRows++; } return vRow; } else { // No more space return null; } } else if (equals(idx, hashAgg_key_0)) { return batch.getValueRow(buckets[idx]); } idx = (idx + 1) & (numBuckets - 1); step++; } // Didn't find it return null; } private boolean equals(int idx, int hashAgg_key_0) { UnsafeRow row = batch.getKeyRow(buckets[idx]); return (row.getInt(0) == hashAgg_key_0); } private long hash(int hashAgg_key_0) { long hashAgg_hash_0 = 0; int hashAgg_result_0 = hashAgg_key_0; hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2); return hashAgg_hash_0; } public org.apache.spark.unsafe.KVIterator < UnsafeRow, UnsafeRow > rowIterator() { return batch.rowIterator(); } public void close() { batch.close(); } } private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException { while (inputadapter_input_0.hasNext()) { InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next(); boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0); org.apache.spark.sql.test.SQLTestData$TestData2 inputadapter_value_0 = inputadapter_isNull_0 ? null : ((org.apache.spark.sql.test.SQLTestData$TestData2) inputadapter_row_0.get(0, null)); serializefromobject_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0); // shouldStop check is eliminated } hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator(); hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */ ).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */ ), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */ ), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */ ), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */ )); } private void hashAgg_doConsume_0(int hashAgg_expr_0_0, int hashAgg_expr_1_0) throws java.io.IOException { UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null; UnsafeRow hashAgg_fastAggBuffer_0 = null; if (!false) { hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert( hashAgg_expr_0_0); } // Cannot find the key in fast hash map, try regular hash map. if (hashAgg_fastAggBuffer_0 == null) { // generate grouping key serializefromobject_mutableStateArray_0[2].reset(); serializefromobject_mutableStateArray_0[2].write(0, hashAgg_expr_0_0); int hashAgg_unsafeRowKeyHash_0 = (serializefromobject_mutableStateArray_0[2].getRow()).hashCode(); if (true) { // try to get the buffer from hash map hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((serializefromobject_mutableStateArray_0[2].getRow()), hashAgg_unsafeRowKeyHash_0); } // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based // aggregation after processing all input rows. if (hashAgg_unsafeRowAggBuffer_0 == null) { if (hashAgg_sorter_0 == null) { hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter(); } else { hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter()); } // the hash map had be spilled, it should have enough memory now, // try to allocate buffer again. hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow( (serializefromobject_mutableStateArray_0[2].getRow()), hashAgg_unsafeRowKeyHash_0); if (hashAgg_unsafeRowAggBuffer_0 == null) { // failed to allocate the first page throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\"); } } } // Updates the proper row buffer if (hashAgg_fastAggBuffer_0 != null) { hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0; } // common sub-expressions // evaluate aggregate functions and update aggregation buffers hashAgg_hashAgg_isNull_6_0 = true; long hashAgg_value_7 = -1 L; do { boolean hashAgg_isNull_7 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0); long hashAgg_value_8 = hashAgg_isNull_7 ? -1 L : (hashAgg_unsafeRowAggBuffer_0.getLong(0)); if (!hashAgg_isNull_7) { hashAgg_hashAgg_isNull_6_0 = false; hashAgg_value_7 = hashAgg_value_8; continue; } if (!false) { hashAgg_hashAgg_isNull_6_0 = false; hashAgg_value_7 = 0 L; continue; } } while (false); boolean hashAgg_isNull_9 = false; long hashAgg_value_10 = -1 L; if (!false) { hashAgg_value_10 = (long) hashAgg_expr_1_0; } long hashAgg_value_6 = -1 L; hashAgg_value_6 = hashAgg_value_7 + hashAgg_value_10; hashAgg_unsafeRowAggBuffer_0.setLong(0, hashAgg_value_6); } private void serializefromobject_doConsume_0(InternalRow inputadapter_row_0, org.apache.spark.sql.test.SQLTestData$TestData2 serializefromobject_expr_0_0, boolean serializefromobject_exprIsNull_0_0) throws java.io.IOException { if (serializefromobject_exprIsNull_0_0) { throw new NullPointerException(((java.lang.String) references[7] /* errMsg */ )); } boolean serializefromobject_isNull_0 = true; int serializefromobject_value_0 = -1; serializefromobject_isNull_0 = false; if (!serializefromobject_isNull_0) { serializefromobject_value_0 = serializefromobject_expr_0_0.a(); } if (serializefromobject_exprIsNull_0_0) { throw new NullPointerException(((java.lang.String) references[8] /* errMsg */ )); } boolean serializefromobject_isNull_4 = true; int serializefromobject_value_4 = -1; serializefromobject_isNull_4 = false; if (!serializefromobject_isNull_4) { serializefromobject_value_4 = serializefromobject_expr_0_0.b(); } hashAgg_doConsume_0(serializefromobject_value_0, serializefromobject_value_4); } private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0) throws java.io.IOException { ((org.apache.spark.sql.execution.metric.SQLMetric) references[9] /* numOutputRows */ ).add(1); int hashAgg_value_12 = hashAgg_keyTerm_0.getInt(0); boolean hashAgg_isNull_12 = hashAgg_bufferTerm_0.isNullAt(0); long hashAgg_value_13 = hashAgg_isNull_12 ? -1 L : (hashAgg_bufferTerm_0.getLong(0)); serializefromobject_mutableStateArray_0[3].reset(); serializefromobject_mutableStateArray_0[3].zeroOutNullBytes(); serializefromobject_mutableStateArray_0[3].write(0, hashAgg_value_12); if (hashAgg_isNull_12) { serializefromobject_mutableStateArray_0[3].setNullAt(1); } else { serializefromobject_mutableStateArray_0[3].write(1, hashAgg_value_13); } append((serializefromobject_mutableStateArray_0[3].getRow())); ===> append() is in 'BufferedRowIterator' } protected void processNext() throws java.io.IOException { if (!hashAgg_initAgg_0) { hashAgg_initAgg_0 = true; hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */ ).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */ ).getEmptyAggregationBuffer()); ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */ ).getTaskContext().addTaskCompletionListener( new org.apache.spark.util.TaskCompletionListener() { @Override public void onTaskCompletion(org.apache.spark.TaskContext context) { hashAgg_fastHashMap_0.close(); } }); hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */ ).createHashMap(); long wholestagecodegen_beforeAgg_0 = System.nanoTime(); hashAgg_doAggregateWithKeys_0(); ((org.apache.spark.sql.execution.metric.SQLMetric) references[10] /* aggTime */ ).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000); } // output the result while (hashAgg_fastHashMapIter_0.next()) { UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey(); UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue(); hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0); if (shouldStop()) return; } hashAgg_fastHashMap_0.close(); while (hashAgg_mapIter_0.next()) { UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey(); UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue(); hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0); if (shouldStop()) return; } hashAgg_mapIter_0.close(); if (hashAgg_sorter_0 == null) { hashAgg_hashMap_0.free(); } } }","title":"GeneratedIteratorForCodegenStage1"},{"location":"SparkSQL/Aggregation/#inputrdds","text":"plan *(1) HashAggregate(keys=[a#3], functions=[partial_sum(b#4)], output=[a#3, sum#17L]) +- *(1) SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData2, true])).a AS a#3, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData2, true])).b AS b#4] +- Scan[obj#2] code org.apache.spark.sql.execution.WholeStageCodegenExec#doExecute override def doExecute(): RDD[InternalRow] = { val (ctx, cleanedSource) = doCodeGen() // try to compile and fallback if it failed val (_, compiledCodeStats) = try { CodeGenerator.compile(cleanedSource) } catch { case NonFatal(_) if !Utils.isTesting && conf.codegenFallback => // We should already saw the error message logWarning(s\"Whole-stage codegen disabled for plan (id=$codegenStageId):\\n $treeString\") return child.execute() } // Check if compiled code has a too large function if (compiledCodeStats.maxMethodCodeSize > conf.hugeMethodLimit) { logInfo(s\"Found too long generated codes and JIT optimization might not work: \" + s\"the bytecode size (${compiledCodeStats.maxMethodCodeSize}) is above the limit \" + s\"${conf.hugeMethodLimit}, and the whole-stage codegen was disabled \" + s\"for this plan (id=$codegenStageId). To avoid this, you can raise the limit \" + s\"`${SQLConf.WHOLESTAGE_HUGE_METHOD_LIMIT.key}`:\\n$treeString\") return child.execute() } val references = ctx.references.toArray val durationMs = longMetric(\"pipelineTime\") // Even though rdds is an RDD[InternalRow] it may actually be an RDD[ColumnarBatch] with // type erasure hiding that. This allows for the input to a code gen stage to be columnar, // but the output must be rows. val rdds = child.asInstanceOf[CodegenSupport].inputRDDs() assert(rdds.size <= 2, \"Up to two input RDDs can be supported\") val evaluatorFactory = new WholeStageCodegenEvaluatorFactory( cleanedSource, durationMs, references) if (rdds.length == 1) { if (conf.usePartitionEvaluator) { rdds.head.mapPartitionsWithEvaluator(evaluatorFactory) ===> Return a new RDD by applying an evaluator to each partition of this RDD. The given evaluator factory will be serialized and sent to executors, and each task will create an evaluator with the factory, and use the evaluator to transform the data of the input partition. } else { rdds.head.mapPartitionsWithIndex { (index, iter) => val evaluator = evaluatorFactory.createEvaluator() evaluator.eval(index, iter) ===> run the generated code } } } else { // Right now, we support up to two input RDDs. if (conf.usePartitionEvaluator) { rdds.head.zipPartitionsWithEvaluator(rdds(1), evaluatorFactory) } else { rdds.head.zipPartitions(rdds(1)) { (leftIter, rightIter) => Iterator((leftIter, rightIter)) // a small hack to obtain the correct partition index }.mapPartitionsWithIndex { (index, zippedIter) => val (leftIter, rightIter) = zippedIter.next() val evaluator = evaluatorFactory.createEvaluator() evaluator.eval(index, leftIter, rightIter) } } } } => org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport#inputRDDs, org.apache.spark.sql.execution.SerializeFromObjectExec#inputRDDs override def inputRDDs(): Seq[RDD[InternalRow]] = { child.asInstanceOf[CodegenSupport].inputRDDs() } => org.apache.spark.sql.execution.InputAdapter /** * InputAdapter is used to hide a SparkPlan from a subtree that supports codegen. * * This is the leaf node of a tree with WholeStageCodegen that is used to generate code * that consumes an RDD iterator of InternalRow. */ case class InputAdapter(child: SparkPlan) extends UnaryExecNode with InputRDDCodegen { // `InputAdapter` can only generate code to process the rows from its child. If the child produces // columnar batches, there must be a `ColumnarToRowExec` above `InputAdapter` to handle it by // overriding `inputRDDs` and calling `InputAdapter#executeColumnar` directly. override def inputRDD: RDD[InternalRow] = child.execute() => org.apache.spark.sql.execution.InputRDDCodegen /** * Leaf codegen node reading from a single RDD. */ trait InputRDDCodegen extends CodegenSupport { def inputRDD: RDD[InternalRow] ... override def inputRDDs(): Seq[RDD[InternalRow]] = { inputRDD :: Nil } => inputRDD is in InputAdapter (override def inputRDD: RDD[InternalRow] = child.execute()) whose \u2018child\u2019 is ExternalRDDScanExec org.apache.spark.sql.execution.ExternalRDDScanExec /** Physical plan node for scanning data from an RDD. */ case class ExternalRDDScanExec[T]( outputObjAttr: Attribute, rdd: RDD[T]) extends LeafExecNode with ObjectProducerExec { ExternalRDDScanExec is initialized at object BasicOperators extends Strategy { case ExternalRDD(outputObjAttr, rdd) => ExternalRDDScanExec(outputObjAttr, rdd) :: Nil","title":"inputRDDs"},{"location":"SparkSQL/Analyzer/","text":"Analyzer Logical query plan analyzer ResolveSessionCatalog analyzeTable Analyzer \u00b6 Logical query plan analyzer \u00b6 abstract class BaseSessionStateBuilder( val session: SparkSession, val parentState: Option[SessionState]) { /** * Logical query plan analyzer for resolving unresolved attributes and relations. * * Note: this depends on the `conf` and `catalog` fields. */ protected def analyzer: Analyzer = new Analyzer(catalogManager) { override val extendedResolutionRules: Seq[Rule[LogicalPlan]] = new FindDataSourceTable(session) +: new ResolveSQLOnFile(session) +: new FallBackFileSourceV2(session) +: ResolveEncodersInScalaAgg +: new ResolveSessionCatalog(catalogManager) +: =====> ResolveSessionCatalog Rule ResolveWriteToStream +: new EvalSubqueriesForTimeTravel +: customResolutionRules ResolveSessionCatalog \u00b6 /** * Resolves catalogs from the multi-part identifiers in SQL statements, and convert the statements * to the corresponding v1 or v2 commands if the resolved catalog is the session catalog. * * We can remove this rule once we implement all the catalog functionality in `V2SessionCatalog`. */ class ResolveSessionCatalog(val catalogManager: CatalogManager) extends Rule[LogicalPlan] with LookupCatalog { override def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUp { case AnalyzeTables(DatabaseInSessionCatalog(db), noScan) => AnalyzeTablesCommand(Some(db), noScan) /** * Analyzes all tables in the given database to generate statistics. */ case class AnalyzeTablesCommand( databaseName: Option[String], noScan: Boolean) extends LeafRunnableCommand { override def run(sparkSession: SparkSession): Seq[Row] = { val catalog = sparkSession.sessionState.catalog val db = databaseName.getOrElse(catalog.getCurrentDatabase) catalog.listTables(db).foreach { tbl => try { CommandUtils.analyzeTable(sparkSession, tbl, noScan) } catch { case NonFatal(e) => logWarning(s\"Failed to analyze table ${tbl.table} in the \" + s\"database $db because of ${e.toString}\", e) } } Seq.empty[Row] } } analyzeTable \u00b6 org.apache.spark.sql.execution.command.CommandUtils#analyzeTable def analyzeTable( sparkSession: SparkSession, tableIdent: TableIdentifier, noScan: Boolean): Unit = { val sessionState = sparkSession.sessionState val db = tableIdent.database.getOrElse(sessionState.catalog.getCurrentDatabase) val tableIdentWithDB = TableIdentifier(tableIdent.table, Some(db)) val tableMeta = sessionState.catalog.getTableMetadata(tableIdentWithDB) if (tableMeta.tableType == CatalogTableType.VIEW) { // Analyzes a catalog view if the view is cached val table = sparkSession.table(tableIdent.quotedString) val cacheManager = sparkSession.sharedState.cacheManager if (cacheManager.lookupCachedData(table.logicalPlan).isDefined) { if (!noScan) { // To collect table stats, materializes an underlying columnar RDD table.count() } } else { throw QueryCompilationErrors.analyzeTableNotSupportedOnViewsError() } } else { // Compute stats for the whole table val (newTotalSize, _) = CommandUtils.calculateTotalSize(sparkSession, tableMeta) =====> calculateTotalSize of the table val newRowCount = if (noScan) None else Some(BigInt(sparkSession.table(tableIdentWithDB).count())) // Update the metastore if the above statistics of the table are different from those // recorded in the metastore. val newStats = CommandUtils.compareAndGetNewStats(tableMeta.stats, newTotalSize, newRowCount) if (newStats.isDefined) { sessionState.catalog.alterTableStats(tableIdentWithDB, newStats) } } } def calculateTotalSize( spark: SparkSession, catalogTable: CatalogTable): (BigInt, Seq[CatalogTablePartition]) = { val sessionState = spark.sessionState val startTime = System.nanoTime() val (totalSize, newPartitions) = if (catalogTable.partitionColumnNames.isEmpty) { (calculateSingleLocationSize(sessionState, catalogTable.identifier, =====> calculateSingleLocationSize catalogTable.storage.locationUri), Seq()) } else { // Calculate table size as a sum of the visible partitions. See SPARK-21079 val partitions = sessionState.catalog.listPartitions(catalogTable.identifier) =====> listPartitions logInfo(s\"Starting to calculate sizes for ${partitions.length} partitions.\") val paths = partitions.map(_.storage.locationUri) val sizes = calculateMultipleLocationSizes(spark, catalogTable.identifier, paths) =====> calculateMultipleLocationSizes val newPartitions = partitions.zipWithIndex.flatMap { case (p, idx) => val newStats = CommandUtils.compareAndGetNewStats(p.stats, sizes(idx), None) newStats.map(_ => p.copy(stats = newStats)) } (sizes.sum, newPartitions) } logInfo(s\"It took ${(System.nanoTime() - startTime) / (1000 * 1000)} ms to calculate\" + s\" the total size for table ${catalogTable.identifier}.\") (totalSize, newPartitions) } def calculateMultipleLocationSizes( sparkSession: SparkSession, tid: TableIdentifier, paths: Seq[Option[URI]]): Seq[Long] = { if (sparkSession.sessionState.conf.parallelFileListingInStatsComputation) { calculateMultipleLocationSizesInParallel(sparkSession, paths.map(_.map(new Path(_)))) } else { paths.map(p => calculateSingleLocationSize(sparkSession.sessionState, tid, p)) } } def calculateSingleLocationSize( sessionState: SessionState, identifier: TableIdentifier, locationUri: Option[URI]): Long = { // This method is mainly based on // org.apache.hadoop.hive.ql.stats.StatsUtils.getFileSizeForTable(HiveConf, Table) // in Hive 0.13 (except that we do not use fs.getContentSummary). // TODO: Generalize statistics collection. // TODO: Why fs.getContentSummary returns wrong size on Jenkins? // Can we use fs.getContentSummary in future? // Seems fs.getContentSummary returns wrong table size on Jenkins. So we use // countFileSize to count the table size. val stagingDir = sessionState.conf.getConfString(\"hive.exec.stagingdir\", \".hive-staging\") def getPathSize(fs: FileSystem, path: Path): Long = { val fileStatus = fs.getFileStatus(path) val size = if (fileStatus.isDirectory) { fs.listStatus(path) .map { status => if (isDataPath(status.getPath, stagingDir)) { getPathSize(fs, status.getPath) } else { 0L } }.sum } else { fileStatus.getLen } size } val startTime = System.nanoTime() val size = locationUri.map { p => val path = new Path(p) try { val fs = path.getFileSystem(sessionState.newHadoopConf()) getPathSize(fs, path) } catch { case NonFatal(e) => logWarning( s\"Failed to get the size of table ${identifier.table} in the \" + s\"database ${identifier.database} because of ${e.toString}\", e) 0L } }.getOrElse(0L) val durationInMs = (System.nanoTime() - startTime) / (1000 * 1000) logDebug(s\"It took $durationInMs ms to calculate the total file size under path $locationUri.\") size } private[spark] object HadoopFSUtils extends Logging { /** * Lists a collection of paths recursively. Picks the listing strategy adaptively depending * on the number of paths to list. * * This may only be called on the driver. * * @param sc Spark context used to run parallel listing. * @param paths Input paths to list * @param hadoopConf Hadoop configuration * @param filter Path filter used to exclude leaf files from result * @param ignoreMissingFiles Ignore missing files that happen during recursive listing * (e.g., due to race conditions) * @param ignoreLocality Whether to fetch data locality info when listing leaf files. If false, * this will return `FileStatus` without `BlockLocation` info. * @param parallelismThreshold The threshold to enable parallelism. If the number of input paths * is smaller than this value, this will fallback to use * sequential listing. * @param parallelismMax The maximum parallelism for listing. If the number of input paths is * larger than this value, parallelism will be throttled to this value * to avoid generating too many tasks. * @return for each input path, the set of discovered files for the path */ def parallelListLeafFiles( sc: SparkContext, paths: Seq[Path], hadoopConf: Configuration, filter: PathFilter, ignoreMissingFiles: Boolean, ignoreLocality: Boolean, parallelismThreshold: Int, parallelismMax: Int): Seq[(Path, Seq[FileStatus])] = { parallelListLeafFilesInternal(sc, paths, hadoopConf, filter, isRootLevel = true, ignoreMissingFiles, ignoreLocality, parallelismThreshold, parallelismMax) }","title":"Analyzer"},{"location":"SparkSQL/Analyzer/#analyzer","text":"","title":"Analyzer"},{"location":"SparkSQL/Analyzer/#logical-query-plan-analyzer","text":"abstract class BaseSessionStateBuilder( val session: SparkSession, val parentState: Option[SessionState]) { /** * Logical query plan analyzer for resolving unresolved attributes and relations. * * Note: this depends on the `conf` and `catalog` fields. */ protected def analyzer: Analyzer = new Analyzer(catalogManager) { override val extendedResolutionRules: Seq[Rule[LogicalPlan]] = new FindDataSourceTable(session) +: new ResolveSQLOnFile(session) +: new FallBackFileSourceV2(session) +: ResolveEncodersInScalaAgg +: new ResolveSessionCatalog(catalogManager) +: =====> ResolveSessionCatalog Rule ResolveWriteToStream +: new EvalSubqueriesForTimeTravel +: customResolutionRules","title":"Logical query plan analyzer"},{"location":"SparkSQL/Analyzer/#resolvesessioncatalog","text":"/** * Resolves catalogs from the multi-part identifiers in SQL statements, and convert the statements * to the corresponding v1 or v2 commands if the resolved catalog is the session catalog. * * We can remove this rule once we implement all the catalog functionality in `V2SessionCatalog`. */ class ResolveSessionCatalog(val catalogManager: CatalogManager) extends Rule[LogicalPlan] with LookupCatalog { override def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUp { case AnalyzeTables(DatabaseInSessionCatalog(db), noScan) => AnalyzeTablesCommand(Some(db), noScan) /** * Analyzes all tables in the given database to generate statistics. */ case class AnalyzeTablesCommand( databaseName: Option[String], noScan: Boolean) extends LeafRunnableCommand { override def run(sparkSession: SparkSession): Seq[Row] = { val catalog = sparkSession.sessionState.catalog val db = databaseName.getOrElse(catalog.getCurrentDatabase) catalog.listTables(db).foreach { tbl => try { CommandUtils.analyzeTable(sparkSession, tbl, noScan) } catch { case NonFatal(e) => logWarning(s\"Failed to analyze table ${tbl.table} in the \" + s\"database $db because of ${e.toString}\", e) } } Seq.empty[Row] } }","title":"ResolveSessionCatalog"},{"location":"SparkSQL/Analyzer/#analyzetable","text":"org.apache.spark.sql.execution.command.CommandUtils#analyzeTable def analyzeTable( sparkSession: SparkSession, tableIdent: TableIdentifier, noScan: Boolean): Unit = { val sessionState = sparkSession.sessionState val db = tableIdent.database.getOrElse(sessionState.catalog.getCurrentDatabase) val tableIdentWithDB = TableIdentifier(tableIdent.table, Some(db)) val tableMeta = sessionState.catalog.getTableMetadata(tableIdentWithDB) if (tableMeta.tableType == CatalogTableType.VIEW) { // Analyzes a catalog view if the view is cached val table = sparkSession.table(tableIdent.quotedString) val cacheManager = sparkSession.sharedState.cacheManager if (cacheManager.lookupCachedData(table.logicalPlan).isDefined) { if (!noScan) { // To collect table stats, materializes an underlying columnar RDD table.count() } } else { throw QueryCompilationErrors.analyzeTableNotSupportedOnViewsError() } } else { // Compute stats for the whole table val (newTotalSize, _) = CommandUtils.calculateTotalSize(sparkSession, tableMeta) =====> calculateTotalSize of the table val newRowCount = if (noScan) None else Some(BigInt(sparkSession.table(tableIdentWithDB).count())) // Update the metastore if the above statistics of the table are different from those // recorded in the metastore. val newStats = CommandUtils.compareAndGetNewStats(tableMeta.stats, newTotalSize, newRowCount) if (newStats.isDefined) { sessionState.catalog.alterTableStats(tableIdentWithDB, newStats) } } } def calculateTotalSize( spark: SparkSession, catalogTable: CatalogTable): (BigInt, Seq[CatalogTablePartition]) = { val sessionState = spark.sessionState val startTime = System.nanoTime() val (totalSize, newPartitions) = if (catalogTable.partitionColumnNames.isEmpty) { (calculateSingleLocationSize(sessionState, catalogTable.identifier, =====> calculateSingleLocationSize catalogTable.storage.locationUri), Seq()) } else { // Calculate table size as a sum of the visible partitions. See SPARK-21079 val partitions = sessionState.catalog.listPartitions(catalogTable.identifier) =====> listPartitions logInfo(s\"Starting to calculate sizes for ${partitions.length} partitions.\") val paths = partitions.map(_.storage.locationUri) val sizes = calculateMultipleLocationSizes(spark, catalogTable.identifier, paths) =====> calculateMultipleLocationSizes val newPartitions = partitions.zipWithIndex.flatMap { case (p, idx) => val newStats = CommandUtils.compareAndGetNewStats(p.stats, sizes(idx), None) newStats.map(_ => p.copy(stats = newStats)) } (sizes.sum, newPartitions) } logInfo(s\"It took ${(System.nanoTime() - startTime) / (1000 * 1000)} ms to calculate\" + s\" the total size for table ${catalogTable.identifier}.\") (totalSize, newPartitions) } def calculateMultipleLocationSizes( sparkSession: SparkSession, tid: TableIdentifier, paths: Seq[Option[URI]]): Seq[Long] = { if (sparkSession.sessionState.conf.parallelFileListingInStatsComputation) { calculateMultipleLocationSizesInParallel(sparkSession, paths.map(_.map(new Path(_)))) } else { paths.map(p => calculateSingleLocationSize(sparkSession.sessionState, tid, p)) } } def calculateSingleLocationSize( sessionState: SessionState, identifier: TableIdentifier, locationUri: Option[URI]): Long = { // This method is mainly based on // org.apache.hadoop.hive.ql.stats.StatsUtils.getFileSizeForTable(HiveConf, Table) // in Hive 0.13 (except that we do not use fs.getContentSummary). // TODO: Generalize statistics collection. // TODO: Why fs.getContentSummary returns wrong size on Jenkins? // Can we use fs.getContentSummary in future? // Seems fs.getContentSummary returns wrong table size on Jenkins. So we use // countFileSize to count the table size. val stagingDir = sessionState.conf.getConfString(\"hive.exec.stagingdir\", \".hive-staging\") def getPathSize(fs: FileSystem, path: Path): Long = { val fileStatus = fs.getFileStatus(path) val size = if (fileStatus.isDirectory) { fs.listStatus(path) .map { status => if (isDataPath(status.getPath, stagingDir)) { getPathSize(fs, status.getPath) } else { 0L } }.sum } else { fileStatus.getLen } size } val startTime = System.nanoTime() val size = locationUri.map { p => val path = new Path(p) try { val fs = path.getFileSystem(sessionState.newHadoopConf()) getPathSize(fs, path) } catch { case NonFatal(e) => logWarning( s\"Failed to get the size of table ${identifier.table} in the \" + s\"database ${identifier.database} because of ${e.toString}\", e) 0L } }.getOrElse(0L) val durationInMs = (System.nanoTime() - startTime) / (1000 * 1000) logDebug(s\"It took $durationInMs ms to calculate the total file size under path $locationUri.\") size } private[spark] object HadoopFSUtils extends Logging { /** * Lists a collection of paths recursively. Picks the listing strategy adaptively depending * on the number of paths to list. * * This may only be called on the driver. * * @param sc Spark context used to run parallel listing. * @param paths Input paths to list * @param hadoopConf Hadoop configuration * @param filter Path filter used to exclude leaf files from result * @param ignoreMissingFiles Ignore missing files that happen during recursive listing * (e.g., due to race conditions) * @param ignoreLocality Whether to fetch data locality info when listing leaf files. If false, * this will return `FileStatus` without `BlockLocation` info. * @param parallelismThreshold The threshold to enable parallelism. If the number of input paths * is smaller than this value, this will fallback to use * sequential listing. * @param parallelismMax The maximum parallelism for listing. If the number of input paths is * larger than this value, parallelism will be throttled to this value * to avoid generating too many tasks. * @return for each input path, the set of discovered files for the path */ def parallelListLeafFiles( sc: SparkContext, paths: Seq[Path], hadoopConf: Configuration, filter: PathFilter, ignoreMissingFiles: Boolean, ignoreLocality: Boolean, parallelismThreshold: Int, parallelismMax: Int): Seq[(Path, Seq[FileStatus])] = { parallelListLeafFilesInternal(sc, paths, hadoopConf, filter, isRootLevel = true, ignoreMissingFiles, ignoreLocality, parallelismThreshold, parallelismMax) }","title":"analyzeTable"},{"location":"SparkSQL/Hive/","text":"Hive Hive \u00b6","title":"Hive"},{"location":"SparkSQL/Hive/#hive","text":"","title":"Hive"},{"location":"SparkSQL/Join/","text":"Join Optimizer: re-order Join JIRA Code Join \u00b6 Optimizer: re-order Join \u00b6 JIRA \u00b6 SPARK-12032 Filter can\u2019t be pushed down to correct Join because of bad order of Join code: PR-10073 ref code: PR-10258 For this query: select d.d_year, count(*) cnt FROM store_sales, date_dim d, customer c WHERE ss_customer_sk = c.c_customer_sk AND c.c_first_shipto_date_sk = d.d_date_sk group by d.d_year Current optimized plan is == Optimized Logical Plan == Aggregate [d_year#147], [d_year#147,(count(1),mode=Complete,isDistinct=false) AS cnt#425L] Project [d_year#147] Join Inner, Some(((ss_customer_sk#283 = c_customer_sk#101) && (c_first_shipto_date_sk#106 = d_date_sk#141))) Project [d_date_sk#141,d_year#147,ss_customer_sk#283] Join Inner, None Project [ss_customer_sk#283] Relation[] ParquetRelation[store_sales] Project [d_date_sk#141,d_year#147] Relation[] ParquetRelation[date_dim] Project [c_customer_sk#101,c_first_shipto_date_sk#106] Relation[] ParquetRelation[customer] It will join store_sales and date_dim together without any condition , the condition c.c_first_shipto_date_sk = d.d_date_sk is not pushed to it because the bad order of joins. The optimizer should re-order the joins, join date_dim after customer, then it can pushed down the condition correctly. The plan should be Aggregate [d_year#147], [d_year#147,(count(1),mode=Complete,isDistinct=false) AS cnt#425L] Project [d_year#147] Join Inner, Some((c_first_shipto_date_sk#106 = d_date_sk#141)) Project [c_first_shipto_date_sk#106] Join Inner, Some((ss_customer_sk#283 = c_customer_sk#101)) Project [ss_customer_sk#283] Relation[store_sales] Project [c_first_shipto_date_sk#106,c_customer_sk#101] Relation[customer] Project [d_year#147,d_date_sk#141] Relation[date_dim] Code \u00b6 org.apache.spark.sql.catalyst.optimizer.Optimizer#defaultBatches /** * Defines the default rule batches in the Optimizer. * * Implementations of this class should override this method, and [[nonExcludableRules]] if * necessary, instead of [[batches]]. The rule batches that eventually run in the Optimizer, * i.e., returned by [[batches]], will be (defaultBatches - (excludedRules - nonExcludableRules)). */ def defaultBatches: Seq[Batch] = { val operatorOptimizationRuleSet = Seq( // Operator push down PushProjectionThroughUnion, ReorderJoin, ==> ReorderJoin rules org.apache.spark.sql.catalyst.optimizer.ReorderJoin /** * Reorder the joins and push all the conditions into join, so that the bottom ones have at least * one condition. * * The order of joins will not be changed if all of them already have at least one condition. * * If star schema detection is enabled, reorder the star join plans based on heuristics. */ object ReorderJoin extends Rule[LogicalPlan] with PredicateHelper { /** * Join a list of plans together and push down the conditions into them. * * The joined plan are picked from left to right, prefer those has at least one join condition. * * @param input a list of LogicalPlans to inner join and the type of inner join. * @param conditions a list of condition for join. */ @tailrec final def createOrderedJoin( input: Seq[(LogicalPlan, InnerLike)], conditions: Seq[Expression]): LogicalPlan = { def apply(plan: LogicalPlan): LogicalPlan = plan.transformWithPruning( _.containsPattern(INNER_LIKE_JOIN), ruleId) { case p @ ExtractFiltersAndInnerJoins(input, conditions) if input.size > 2 && conditions.nonEmpty => val reordered = if (conf.starSchemaDetection && !conf.cboEnabled) { val starJoinPlan = StarSchemaDetection.reorderStarJoins(input, conditions) if (starJoinPlan.nonEmpty) { val rest = input.filterNot(starJoinPlan.contains(_)) createOrderedJoin(starJoinPlan ++ rest, conditions) } else { createOrderedJoin(input, conditions) } } else { createOrderedJoin(input, conditions) } if (p.sameOutput(reordered)) { reordered } else { // Reordering the joins have changed the order of the columns. // Inject a projection to make sure we restore to the expected ordering. Project(p.output, reordered) } } org.apache.spark.sql.catalyst.planning.ExtractFiltersAndInnerJoins /** * A pattern that collects the filter and inner joins. * * Filter * | * inner Join * / \\ ----> (Seq(plan0, plan1, plan2), conditions) * Filter plan2 * | * inner join * / \\ * plan0 plan1 * * Note: This pattern currently only works for left-deep trees. */ object ExtractFiltersAndInnerJoins extends PredicateHelper { /** * Flatten all inner joins, which are next to each other. * Return a list of logical plans to be joined with a boolean for each plan indicating if it * was involved in an explicit cross join. Also returns the entire list of join conditions for * the left-deep tree. */ def flattenJoin(plan: LogicalPlan, parentJoinType: InnerLike = Inner) : (Seq[(LogicalPlan, InnerLike)], Seq[Expression]) = plan match { case Join(left, right, joinType: InnerLike, cond, hint) if hint == JoinHint.NONE => val (plans, conditions) = flattenJoin(left, joinType) (plans ++ Seq((right, joinType)), conditions ++ cond.toSeq.flatMap(splitConjunctivePredicates)) case Filter(filterCondition, j @ Join(_, _, _: InnerLike, _, hint)) if hint == JoinHint.NONE => val (plans, conditions) = flattenJoin(j) (plans, conditions ++ splitConjunctivePredicates(filterCondition)) case _ => (Seq((plan, parentJoinType)), Seq.empty) } def unapply(plan: LogicalPlan) : Option[(Seq[(LogicalPlan, InnerLike)], Seq[Expression])] = plan match { case f @ Filter(filterCondition, j @ Join(_, _, joinType: InnerLike, _, hint)) if hint == JoinHint.NONE => Some(flattenJoin(f)) case j @ Join(_, _, joinType, _, hint) if hint == JoinHint.NONE => Some(flattenJoin(j)) case _ => None } }","title":"Join"},{"location":"SparkSQL/Join/#join","text":"","title":"Join"},{"location":"SparkSQL/Join/#optimizer-re-order-join","text":"","title":"Optimizer: re-order Join"},{"location":"SparkSQL/Join/#jira","text":"SPARK-12032 Filter can\u2019t be pushed down to correct Join because of bad order of Join code: PR-10073 ref code: PR-10258 For this query: select d.d_year, count(*) cnt FROM store_sales, date_dim d, customer c WHERE ss_customer_sk = c.c_customer_sk AND c.c_first_shipto_date_sk = d.d_date_sk group by d.d_year Current optimized plan is == Optimized Logical Plan == Aggregate [d_year#147], [d_year#147,(count(1),mode=Complete,isDistinct=false) AS cnt#425L] Project [d_year#147] Join Inner, Some(((ss_customer_sk#283 = c_customer_sk#101) && (c_first_shipto_date_sk#106 = d_date_sk#141))) Project [d_date_sk#141,d_year#147,ss_customer_sk#283] Join Inner, None Project [ss_customer_sk#283] Relation[] ParquetRelation[store_sales] Project [d_date_sk#141,d_year#147] Relation[] ParquetRelation[date_dim] Project [c_customer_sk#101,c_first_shipto_date_sk#106] Relation[] ParquetRelation[customer] It will join store_sales and date_dim together without any condition , the condition c.c_first_shipto_date_sk = d.d_date_sk is not pushed to it because the bad order of joins. The optimizer should re-order the joins, join date_dim after customer, then it can pushed down the condition correctly. The plan should be Aggregate [d_year#147], [d_year#147,(count(1),mode=Complete,isDistinct=false) AS cnt#425L] Project [d_year#147] Join Inner, Some((c_first_shipto_date_sk#106 = d_date_sk#141)) Project [c_first_shipto_date_sk#106] Join Inner, Some((ss_customer_sk#283 = c_customer_sk#101)) Project [ss_customer_sk#283] Relation[store_sales] Project [c_first_shipto_date_sk#106,c_customer_sk#101] Relation[customer] Project [d_year#147,d_date_sk#141] Relation[date_dim]","title":"JIRA"},{"location":"SparkSQL/Join/#code","text":"org.apache.spark.sql.catalyst.optimizer.Optimizer#defaultBatches /** * Defines the default rule batches in the Optimizer. * * Implementations of this class should override this method, and [[nonExcludableRules]] if * necessary, instead of [[batches]]. The rule batches that eventually run in the Optimizer, * i.e., returned by [[batches]], will be (defaultBatches - (excludedRules - nonExcludableRules)). */ def defaultBatches: Seq[Batch] = { val operatorOptimizationRuleSet = Seq( // Operator push down PushProjectionThroughUnion, ReorderJoin, ==> ReorderJoin rules org.apache.spark.sql.catalyst.optimizer.ReorderJoin /** * Reorder the joins and push all the conditions into join, so that the bottom ones have at least * one condition. * * The order of joins will not be changed if all of them already have at least one condition. * * If star schema detection is enabled, reorder the star join plans based on heuristics. */ object ReorderJoin extends Rule[LogicalPlan] with PredicateHelper { /** * Join a list of plans together and push down the conditions into them. * * The joined plan are picked from left to right, prefer those has at least one join condition. * * @param input a list of LogicalPlans to inner join and the type of inner join. * @param conditions a list of condition for join. */ @tailrec final def createOrderedJoin( input: Seq[(LogicalPlan, InnerLike)], conditions: Seq[Expression]): LogicalPlan = { def apply(plan: LogicalPlan): LogicalPlan = plan.transformWithPruning( _.containsPattern(INNER_LIKE_JOIN), ruleId) { case p @ ExtractFiltersAndInnerJoins(input, conditions) if input.size > 2 && conditions.nonEmpty => val reordered = if (conf.starSchemaDetection && !conf.cboEnabled) { val starJoinPlan = StarSchemaDetection.reorderStarJoins(input, conditions) if (starJoinPlan.nonEmpty) { val rest = input.filterNot(starJoinPlan.contains(_)) createOrderedJoin(starJoinPlan ++ rest, conditions) } else { createOrderedJoin(input, conditions) } } else { createOrderedJoin(input, conditions) } if (p.sameOutput(reordered)) { reordered } else { // Reordering the joins have changed the order of the columns. // Inject a projection to make sure we restore to the expected ordering. Project(p.output, reordered) } } org.apache.spark.sql.catalyst.planning.ExtractFiltersAndInnerJoins /** * A pattern that collects the filter and inner joins. * * Filter * | * inner Join * / \\ ----> (Seq(plan0, plan1, plan2), conditions) * Filter plan2 * | * inner join * / \\ * plan0 plan1 * * Note: This pattern currently only works for left-deep trees. */ object ExtractFiltersAndInnerJoins extends PredicateHelper { /** * Flatten all inner joins, which are next to each other. * Return a list of logical plans to be joined with a boolean for each plan indicating if it * was involved in an explicit cross join. Also returns the entire list of join conditions for * the left-deep tree. */ def flattenJoin(plan: LogicalPlan, parentJoinType: InnerLike = Inner) : (Seq[(LogicalPlan, InnerLike)], Seq[Expression]) = plan match { case Join(left, right, joinType: InnerLike, cond, hint) if hint == JoinHint.NONE => val (plans, conditions) = flattenJoin(left, joinType) (plans ++ Seq((right, joinType)), conditions ++ cond.toSeq.flatMap(splitConjunctivePredicates)) case Filter(filterCondition, j @ Join(_, _, _: InnerLike, _, hint)) if hint == JoinHint.NONE => val (plans, conditions) = flattenJoin(j) (plans, conditions ++ splitConjunctivePredicates(filterCondition)) case _ => (Seq((plan, parentJoinType)), Seq.empty) } def unapply(plan: LogicalPlan) : Option[(Seq[(LogicalPlan, InnerLike)], Seq[Expression])] = plan match { case f @ Filter(filterCondition, j @ Join(_, _, joinType: InnerLike, _, hint)) if hint == JoinHint.NONE => Some(flattenJoin(f)) case j @ Join(_, _, joinType, _, hint) if hint == JoinHint.NONE => Some(flattenJoin(j)) case _ => None } }","title":"Code"},{"location":"SparkSQL/Pivot/","text":"Pivot JIRA code Pivot \u00b6 JIRA \u00b6 SPARK-8992 Add Pivot functionality to Spark SQL SPARK-13749 Faster pivot implementation for many distinct values with two phase aggregation code \u00b6 org.apache.spark.sql.catalyst.analysis.Analyzer#batches override def batches: Seq[Batch] = Seq( ... Batch(\"Resolution\", fixedPoint, ResolveTableValuedFunctions(v1SessionCatalog) :: ... ResolvePivot :: ResolveOrdinalInOrderByAndGroupBy :: ResolveAggAliasInGroupBy :: ResolveMissingReferences :: ExtractGenerator :: ResolveGenerate :: ResolveFunctions :: ResolveAliases :: ResolveSubquery :: ... org.apache.spark.sql.catalyst.analysis.Analyzer.ResolvePivot object ResolvePivot extends Rule[LogicalPlan] with AliasHelper { def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsWithPruning( org.apache.spark.sql.catalyst.analysis.Analyzer.ResolveAliases object ResolveAliases extends Rule[LogicalPlan] { def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUpWithPruning( _.containsPattern(UNRESOLVED_ALIAS), ruleId) { case Aggregate(groups, aggs, child) if child.resolved && hasUnresolvedAlias(aggs) => Aggregate(groups, assignAliases(aggs), child) case Pivot(groupByOpt, pivotColumn, pivotValues, aggregates, child) if child.resolved && groupByOpt.isDefined && hasUnresolvedAlias(groupByOpt.get) => Pivot(Some(assignAliases(groupByOpt.get)), pivotColumn, pivotValues, aggregates, child) org.apache.spark.sql.catalyst.plans.logical.Pivot /** * A constructor for creating a pivot, which will later be converted to a [[Project]] * or an [[Aggregate]] during the query analysis. * * @param groupByExprsOpt A sequence of group by expressions. This field should be None if coming * from SQL, in which group by expressions are not explicitly specified. * @param pivotColumn The pivot column. * @param pivotValues A sequence of values for the pivot column. * @param aggregates The aggregation expressions, each with or without an alias. * @param child Child operator */ case class Pivot( groupByExprsOpt: Option[Seq[NamedExpression]], pivotColumn: Expression, pivotValues: Seq[Expression], aggregates: Seq[Expression], child: LogicalPlan) extends UnaryNode { org.apache.spark.sql.RelationalGroupedDataset /** * A set of methods for aggregations on a `DataFrame`, created by [[Dataset#groupBy groupBy]], * [[Dataset#cube cube]] or [[Dataset#rollup rollup]] (and also `pivot`). * * The main method is the `agg` function, which has multiple variants. This class also contains * some first-order statistics such as `mean`, `sum` for convenience. * * @note This class was named `GroupedData` in Spark 1.x. * * @since 2.0.0 */ @Stable class RelationalGroupedDataset protected[sql]( private[sql] val df: DataFrame, private[sql] val groupingExprs: Seq[Expression], groupType: RelationalGroupedDataset.GroupType) { /** * Pivots a column of the current `DataFrame` and performs the specified aggregation. * * There are two versions of `pivot` function: one that requires the caller to specify the list * of distinct values to pivot on, and one that does not. The latter is more concise but less * efficient, because Spark needs to first compute the list of distinct values internally. * * {{{ * // Compute the sum of earnings for each year by course with each course as a separate column * df.groupBy(\"year\").pivot(\"course\", Seq(\"dotNET\", \"Java\")).sum(\"earnings\") * * // Or without specifying column values (less efficient) * df.groupBy(\"year\").pivot(\"course\").sum(\"earnings\") * }}} * * @param pivotColumn Name of the column to pivot. * @since 1.6.0 */ def pivot(pivotColumn: String): RelationalGroupedDataset = pivot(Column(pivotColumn)) /** * Pivots a column of the current `DataFrame` and performs the specified aggregation. * There are two versions of pivot function: one that requires the caller to specify the list * of distinct values to pivot on, and one that does not. The latter is more concise but less * efficient, because Spark needs to first compute the list of distinct values internally. * * {{{ * // Compute the sum of earnings for each year by course with each course as a separate column * df.groupBy(\"year\").pivot(\"course\", Seq(\"dotNET\", \"Java\")).sum(\"earnings\") * * // Or without specifying column values (less efficient) * df.groupBy(\"year\").pivot(\"course\").sum(\"earnings\") * }}} * * From Spark 3.0.0, values can be literal columns, for instance, struct. For pivoting by * multiple columns, use the `struct` function to combine the columns and values: * * {{{ * df.groupBy(\"year\") * .pivot(\"trainingCourse\", Seq(struct(lit(\"java\"), lit(\"Experts\")))) * .agg(sum($\"earnings\")) * }}} * * @param pivotColumn Name of the column to pivot. * @param values List of values that will be translated to columns in the output DataFrame. * @since 1.6.0 */ def pivot(pivotColumn: String, values: Seq[Any]): RelationalGroupedDataset = { pivot(Column(pivotColumn), values) } /** * (Java-specific) Pivots a column of the current `DataFrame` and performs the specified * aggregation. * * There are two versions of pivot function: one that requires the caller to specify the list * of distinct values to pivot on, and one that does not. The latter is more concise but less * efficient, because Spark needs to first compute the list of distinct values internally. * * {{{ * // Compute the sum of earnings for each year by course with each course as a separate column * df.groupBy(\"year\").pivot(\"course\", Arrays.<Object>asList(\"dotNET\", \"Java\")).sum(\"earnings\"); * * // Or without specifying column values (less efficient) * df.groupBy(\"year\").pivot(\"course\").sum(\"earnings\"); * }}} * * @param pivotColumn Name of the column to pivot. * @param values List of values that will be translated to columns in the output DataFrame. * @since 1.6.0 */ def pivot(pivotColumn: String, values: java.util.List[Any]): RelationalGroupedDataset = { pivot(Column(pivotColumn), values) } /** * Pivots a column of the current `DataFrame` and performs the specified aggregation. * This is an overloaded version of the `pivot` method with `pivotColumn` of the `String` type. * * {{{ * // Or without specifying column values (less efficient) * df.groupBy($\"year\").pivot($\"course\").sum($\"earnings\"); * }}} * * @param pivotColumn he column to pivot. * @since 2.4.0 */ def pivot(pivotColumn: Column): RelationalGroupedDataset = { // This is to prevent unintended OOM errors when the number of distinct values is large val maxValues = df.sparkSession.sessionState.conf.dataFramePivotMaxValues // Get the distinct values of the column and sort them so its consistent val values = df.select(pivotColumn) .distinct() .limit(maxValues + 1) .sort(pivotColumn) // ensure that the output columns are in a consistent logical order .collect() .map(_.get(0)) .toSeq if (values.length > maxValues) { throw QueryCompilationErrors.aggregationFunctionAppliedOnNonNumericColumnError( pivotColumn.toString, maxValues) } pivot(pivotColumn, values) } /** * Pivots a column of the current `DataFrame` and performs the specified aggregation. * This is an overloaded version of the `pivot` method with `pivotColumn` of the `String` type. * * {{{ * // Compute the sum of earnings for each year by course with each course as a separate column * df.groupBy($\"year\").pivot($\"course\", Seq(\"dotNET\", \"Java\")).sum($\"earnings\") * }}} * * @param pivotColumn the column to pivot. * @param values List of values that will be translated to columns in the output DataFrame. * @since 2.4.0 */ def pivot(pivotColumn: Column, values: Seq[Any]): RelationalGroupedDataset = { groupType match { case RelationalGroupedDataset.GroupByType => val valueExprs = values.map(_ match { case c: Column => c.expr case v => try { Literal.apply(v) } catch { case _: SparkRuntimeException => throw QueryExecutionErrors.pivotColumnUnsupportedError(v, pivotColumn.expr.dataType) } }) new RelationalGroupedDataset( df, groupingExprs, RelationalGroupedDataset.PivotType(pivotColumn.expr, valueExprs)) case _: RelationalGroupedDataset.PivotType => throw QueryExecutionErrors.repeatedPivotsUnsupportedError() case _ => throw QueryExecutionErrors.pivotNotAfterGroupByUnsupportedError() } } /** * (Java-specific) Pivots a column of the current `DataFrame` and performs the specified * aggregation. This is an overloaded version of the `pivot` method with `pivotColumn` of * the `String` type. * * @param pivotColumn the column to pivot. * @param values List of values that will be translated to columns in the output DataFrame. * @since 2.4.0 */ def pivot(pivotColumn: Column, values: java.util.List[Any]): RelationalGroupedDataset = { pivot(pivotColumn, values.asScala.toSeq) } org.apache.spark.sql.RelationalGroupedDataset.PivotType private[sql] case class PivotType(pivotCol: Expression, values: Seq[Expression]) extends GroupType","title":"Pivot"},{"location":"SparkSQL/Pivot/#pivot","text":"","title":"Pivot"},{"location":"SparkSQL/Pivot/#jira","text":"SPARK-8992 Add Pivot functionality to Spark SQL SPARK-13749 Faster pivot implementation for many distinct values with two phase aggregation","title":"JIRA"},{"location":"SparkSQL/Pivot/#code","text":"org.apache.spark.sql.catalyst.analysis.Analyzer#batches override def batches: Seq[Batch] = Seq( ... Batch(\"Resolution\", fixedPoint, ResolveTableValuedFunctions(v1SessionCatalog) :: ... ResolvePivot :: ResolveOrdinalInOrderByAndGroupBy :: ResolveAggAliasInGroupBy :: ResolveMissingReferences :: ExtractGenerator :: ResolveGenerate :: ResolveFunctions :: ResolveAliases :: ResolveSubquery :: ... org.apache.spark.sql.catalyst.analysis.Analyzer.ResolvePivot object ResolvePivot extends Rule[LogicalPlan] with AliasHelper { def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsWithPruning( org.apache.spark.sql.catalyst.analysis.Analyzer.ResolveAliases object ResolveAliases extends Rule[LogicalPlan] { def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUpWithPruning( _.containsPattern(UNRESOLVED_ALIAS), ruleId) { case Aggregate(groups, aggs, child) if child.resolved && hasUnresolvedAlias(aggs) => Aggregate(groups, assignAliases(aggs), child) case Pivot(groupByOpt, pivotColumn, pivotValues, aggregates, child) if child.resolved && groupByOpt.isDefined && hasUnresolvedAlias(groupByOpt.get) => Pivot(Some(assignAliases(groupByOpt.get)), pivotColumn, pivotValues, aggregates, child) org.apache.spark.sql.catalyst.plans.logical.Pivot /** * A constructor for creating a pivot, which will later be converted to a [[Project]] * or an [[Aggregate]] during the query analysis. * * @param groupByExprsOpt A sequence of group by expressions. This field should be None if coming * from SQL, in which group by expressions are not explicitly specified. * @param pivotColumn The pivot column. * @param pivotValues A sequence of values for the pivot column. * @param aggregates The aggregation expressions, each with or without an alias. * @param child Child operator */ case class Pivot( groupByExprsOpt: Option[Seq[NamedExpression]], pivotColumn: Expression, pivotValues: Seq[Expression], aggregates: Seq[Expression], child: LogicalPlan) extends UnaryNode { org.apache.spark.sql.RelationalGroupedDataset /** * A set of methods for aggregations on a `DataFrame`, created by [[Dataset#groupBy groupBy]], * [[Dataset#cube cube]] or [[Dataset#rollup rollup]] (and also `pivot`). * * The main method is the `agg` function, which has multiple variants. This class also contains * some first-order statistics such as `mean`, `sum` for convenience. * * @note This class was named `GroupedData` in Spark 1.x. * * @since 2.0.0 */ @Stable class RelationalGroupedDataset protected[sql]( private[sql] val df: DataFrame, private[sql] val groupingExprs: Seq[Expression], groupType: RelationalGroupedDataset.GroupType) { /** * Pivots a column of the current `DataFrame` and performs the specified aggregation. * * There are two versions of `pivot` function: one that requires the caller to specify the list * of distinct values to pivot on, and one that does not. The latter is more concise but less * efficient, because Spark needs to first compute the list of distinct values internally. * * {{{ * // Compute the sum of earnings for each year by course with each course as a separate column * df.groupBy(\"year\").pivot(\"course\", Seq(\"dotNET\", \"Java\")).sum(\"earnings\") * * // Or without specifying column values (less efficient) * df.groupBy(\"year\").pivot(\"course\").sum(\"earnings\") * }}} * * @param pivotColumn Name of the column to pivot. * @since 1.6.0 */ def pivot(pivotColumn: String): RelationalGroupedDataset = pivot(Column(pivotColumn)) /** * Pivots a column of the current `DataFrame` and performs the specified aggregation. * There are two versions of pivot function: one that requires the caller to specify the list * of distinct values to pivot on, and one that does not. The latter is more concise but less * efficient, because Spark needs to first compute the list of distinct values internally. * * {{{ * // Compute the sum of earnings for each year by course with each course as a separate column * df.groupBy(\"year\").pivot(\"course\", Seq(\"dotNET\", \"Java\")).sum(\"earnings\") * * // Or without specifying column values (less efficient) * df.groupBy(\"year\").pivot(\"course\").sum(\"earnings\") * }}} * * From Spark 3.0.0, values can be literal columns, for instance, struct. For pivoting by * multiple columns, use the `struct` function to combine the columns and values: * * {{{ * df.groupBy(\"year\") * .pivot(\"trainingCourse\", Seq(struct(lit(\"java\"), lit(\"Experts\")))) * .agg(sum($\"earnings\")) * }}} * * @param pivotColumn Name of the column to pivot. * @param values List of values that will be translated to columns in the output DataFrame. * @since 1.6.0 */ def pivot(pivotColumn: String, values: Seq[Any]): RelationalGroupedDataset = { pivot(Column(pivotColumn), values) } /** * (Java-specific) Pivots a column of the current `DataFrame` and performs the specified * aggregation. * * There are two versions of pivot function: one that requires the caller to specify the list * of distinct values to pivot on, and one that does not. The latter is more concise but less * efficient, because Spark needs to first compute the list of distinct values internally. * * {{{ * // Compute the sum of earnings for each year by course with each course as a separate column * df.groupBy(\"year\").pivot(\"course\", Arrays.<Object>asList(\"dotNET\", \"Java\")).sum(\"earnings\"); * * // Or without specifying column values (less efficient) * df.groupBy(\"year\").pivot(\"course\").sum(\"earnings\"); * }}} * * @param pivotColumn Name of the column to pivot. * @param values List of values that will be translated to columns in the output DataFrame. * @since 1.6.0 */ def pivot(pivotColumn: String, values: java.util.List[Any]): RelationalGroupedDataset = { pivot(Column(pivotColumn), values) } /** * Pivots a column of the current `DataFrame` and performs the specified aggregation. * This is an overloaded version of the `pivot` method with `pivotColumn` of the `String` type. * * {{{ * // Or without specifying column values (less efficient) * df.groupBy($\"year\").pivot($\"course\").sum($\"earnings\"); * }}} * * @param pivotColumn he column to pivot. * @since 2.4.0 */ def pivot(pivotColumn: Column): RelationalGroupedDataset = { // This is to prevent unintended OOM errors when the number of distinct values is large val maxValues = df.sparkSession.sessionState.conf.dataFramePivotMaxValues // Get the distinct values of the column and sort them so its consistent val values = df.select(pivotColumn) .distinct() .limit(maxValues + 1) .sort(pivotColumn) // ensure that the output columns are in a consistent logical order .collect() .map(_.get(0)) .toSeq if (values.length > maxValues) { throw QueryCompilationErrors.aggregationFunctionAppliedOnNonNumericColumnError( pivotColumn.toString, maxValues) } pivot(pivotColumn, values) } /** * Pivots a column of the current `DataFrame` and performs the specified aggregation. * This is an overloaded version of the `pivot` method with `pivotColumn` of the `String` type. * * {{{ * // Compute the sum of earnings for each year by course with each course as a separate column * df.groupBy($\"year\").pivot($\"course\", Seq(\"dotNET\", \"Java\")).sum($\"earnings\") * }}} * * @param pivotColumn the column to pivot. * @param values List of values that will be translated to columns in the output DataFrame. * @since 2.4.0 */ def pivot(pivotColumn: Column, values: Seq[Any]): RelationalGroupedDataset = { groupType match { case RelationalGroupedDataset.GroupByType => val valueExprs = values.map(_ match { case c: Column => c.expr case v => try { Literal.apply(v) } catch { case _: SparkRuntimeException => throw QueryExecutionErrors.pivotColumnUnsupportedError(v, pivotColumn.expr.dataType) } }) new RelationalGroupedDataset( df, groupingExprs, RelationalGroupedDataset.PivotType(pivotColumn.expr, valueExprs)) case _: RelationalGroupedDataset.PivotType => throw QueryExecutionErrors.repeatedPivotsUnsupportedError() case _ => throw QueryExecutionErrors.pivotNotAfterGroupByUnsupportedError() } } /** * (Java-specific) Pivots a column of the current `DataFrame` and performs the specified * aggregation. This is an overloaded version of the `pivot` method with `pivotColumn` of * the `String` type. * * @param pivotColumn the column to pivot. * @param values List of values that will be translated to columns in the output DataFrame. * @since 2.4.0 */ def pivot(pivotColumn: Column, values: java.util.List[Any]): RelationalGroupedDataset = { pivot(pivotColumn, values.asScala.toSeq) } org.apache.spark.sql.RelationalGroupedDataset.PivotType private[sql] case class PivotType(pivotCol: Expression, values: Seq[Expression]) extends GroupType","title":"code"},{"location":"SparkSQL/SparkPlan/","text":"SparkPlan Spark\u2019s Planner Call stack Code of SessionState to create QueryExecution Code of SparkPlan generation Code of prepared SparkPlan generation SparkPlan \u00b6 Spark\u2019s Planner \u00b6 1st Phase: Transforms the logical plan to the physical plan using Strategies QueryExecution.sparkPlan SparkPlanner.plan 2nd Phase: use a Rule Executor to make the Physical Plan ready for execution QueryExecution.prepareForExecution Call stack \u00b6 How is QueryExecution.sparkPlan and QueryExecution.prepareForExecution invoked? QueryExecution.sparkPlan stack at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:141)=====> lazy val sparkPlan: SparkPlan - locked <0x328c> (a org.apache.spark.sql.execution.QueryExecution) at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:138) at org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:158) at org.apache.spark.sql.execution.QueryExecution$$Lambda$1861.30604162.apply(Unknown Source:-1) at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111) at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185) at org.apache.spark.sql.execution.QueryExecution$$Lambda$1461.609375192.apply(Unknown Source:-1) at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:512) at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185) at org.apache.spark.sql.execution.QueryExecution$$Lambda$1460.911201454.apply(Unknown Source:-1) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184) at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:158) at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151) at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:204) at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:251) at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:220) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103) at org.apache.spark.sql.execution.SQLExecution$$$Lambda$1698.2050525584.apply(Unknown Source:-1) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:171) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95) at org.apache.spark.sql.execution.SQLExecution$$$Lambda$1682.2000856156.apply(Unknown Source:-1) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3924) at org.apache.spark.sql.Dataset.collect(Dataset.scala:3188) org.apache.spark.sql.Dataset class Dataset[T] private[sql]( @DeveloperApi @Unstable @transient val queryExecution: QueryExecution, @DeveloperApi @Unstable @transient val encoder: Encoder[T]) extends Serializable { // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure // you wrap it with `withNewExecutionId` if this actions doesn't call other action. def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sparkSession.sessionState.executePlan(logicalPlan), encoder) ====> executePlan() is to create the QueryExecution } def collect(): Array[T] = withAction(\"collect\", queryExecution)(collectFromPlan) private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = { SQLExecution.withNewExecutionId(qe, Some(name)) { QueryExecution.withInternalError(s\"\"\"The \"$name\" action failed.\"\"\") { qe.executedPlan.resetMetrics() action(qe.executedPlan) } } } org.apache.spark.sql.execution.SQLExecution#withNewExecutionId /** * Wrap an action that will execute \"queryExecution\" to track all Spark jobs in the body so that * we can connect them with an execution. */ def withNewExecutionId[T]( queryExecution: QueryExecution, name: Option[String] = None)(body: => T): T = queryExecution.sparkSession.withActive { org.apache.spark.sql.execution.QueryExecution#explainString def explainString( mode: ExplainMode, maxFields: Int = SQLConf.get.maxToStringFields): String = { val concat = new PlanStringConcat() explainString(mode, maxFields, concat.append) withRedaction { concat.toString } } org.apache.spark.sql.execution.QueryExecution#executedPlan // executedPlan should not be used to initialize any SparkPlan. It should be // only used for execution. =====> ??? lazy val executedPlan: SparkPlan = { // We need to materialize the optimizedPlan here, before tracking the planning phase, to ensure // that the optimization time is not counted as part of the planning phase. assertOptimized() executePhase(QueryPlanningTracker.PLANNING) { // clone the plan to avoid sharing the plan instance between different stages like analyzing, // optimizing and planning. QueryExecution.prepareForExecution(preparations, sparkPlan.clone()) =====> sparkPlan is initialized here and then call QueryExecution.prepareForExecution } } Code of SessionState to create QueryExecution \u00b6 org.apache.spark.sql.SparkSession#sessionState /** * State isolated across sessions, including SQL configurations, temporary tables, registered * functions, and everything else that accepts a [[org.apache.spark.sql.internal.SQLConf]]. * If `parentSessionState` is not null, the `SessionState` will be a copy of the parent. * * This is internal to Spark and there is no guarantee on interface stability. * * @since 2.2.0 */ @Unstable @transient lazy val sessionState: SessionState = { parentSessionState .map(_.clone(this)) .getOrElse { val state = SparkSession.instantiateSessionState( SparkSession.sessionStateClassName(sharedState.conf), self) state } } org.apache.spark.sql.SparkSession#instantiateSessionState /** * Helper method to create an instance of `SessionState` based on `className` from conf. * The result is either `SessionState` or a Hive based `SessionState`. */ private def instantiateSessionState( className: String, sparkSession: SparkSession): SessionState = { try { // invoke new [Hive]SessionStateBuilder( // SparkSession, // Option[SessionState]) val clazz = Utils.classForName(className) val ctor = clazz.getConstructors.head ctor.newInstance(sparkSession, None).asInstanceOf[BaseSessionStateBuilder].build() } catch { case NonFatal(e) => throw new IllegalArgumentException(s\"Error while instantiating '$className':\", e) } } org.apache.spark.sql.SparkSession#sessionStateClassName private val HIVE_SESSION_STATE_BUILDER_CLASS_NAME = \"org.apache.spark.sql.hive.HiveSessionStateBuilder\" private def sessionStateClassName(conf: SparkConf): String = { conf.get(CATALOG_IMPLEMENTATION) match { case \"hive\" => HIVE_SESSION_STATE_BUILDER_CLASS_NAME case \"in-memory\" => classOf[SessionStateBuilder].getCanonicalName } } class SessionStateBuilder( session: SparkSession, parentState: Option[SessionState]) extends BaseSessionStateBuilder(session, parentState) { override protected def newBuilder: NewBuilder = new SessionStateBuilder(_, _) } or Hive /** * Builder that produces a Hive-aware `SessionState`. */ class HiveSessionStateBuilder( session: SparkSession, parentState: Option[SessionState]) extends BaseSessionStateBuilder(session, parentState) { org.apache.spark.sql.internal.BaseSessionStateBuilder#build def build(): SessionState = { new SessionState( session.sharedState, conf, experimentalMethods, functionRegistry, tableFunctionRegistry, udfRegistration, () => catalog, sqlParser, () => analyzer, () => optimizer, planner, () => streamingQueryManager, listenerManager, () => resourceLoader, createQueryExecution, createClone, columnarRules, adaptiveRulesHolder) } } org.apache.spark.sql.internal.BaseSessionStateBuilder#createQueryExecution protected def createQueryExecution: (LogicalPlan, CommandExecutionMode.Value) => QueryExecution = (plan, mode) => new QueryExecution(session, plan, mode = mode) org.apache.spark.sql.execution.QueryExecution /** * The primary workflow for executing relational queries using Spark. Designed to allow easy * access to the intermediate phases of query execution for developers. * * While this is not a public class, we should avoid changing the function names for the sake of * changing them, because a lot of developers use the feature for debugging. */ class QueryExecution( val sparkSession: SparkSession, val logical: LogicalPlan, val tracker: QueryPlanningTracker = new QueryPlanningTracker, val mode: CommandExecutionMode.Value = CommandExecutionMode.ALL) extends Logging { Code of SparkPlan generation \u00b6 QueryExecution.sparkPlan => QueryExecution.createSparkPlan SparkPlanner.plan (strategies defined in SparkPlanner) => SparkStrategies.plan => QueryPlanner.plan org.apache.spark.sql.execution.QueryExecution /** * The primary workflow for executing relational queries using Spark. Designed to allow easy * access to the intermediate phases of query execution for developers. * * While this is not a public class, we should avoid changing the function names for the sake of * changing them, because a lot of developers use the feature for debugging. */ class QueryExecution( val sparkSession: SparkSession, val logical: LogicalPlan, val tracker: QueryPlanningTracker = new QueryPlanningTracker, val mode: CommandExecutionMode.Value = CommandExecutionMode.ALL) extends Logging { lazy val sparkPlan: SparkPlan = { // We need to materialize the optimizedPlan here because sparkPlan is also tracked under // the planning phase assertOptimized() executePhase(QueryPlanningTracker.PLANNING) { // Clone the logical plan here, in case the planner rules change the states of the logical // plan. QueryExecution.createSparkPlan(sparkSession, planner, optimizedPlan.clone()) } } /** * Transform a [[LogicalPlan]] into a [[SparkPlan]]. * * Note that the returned physical plan still needs to be prepared for execution. */ def createSparkPlan( sparkSession: SparkSession, planner: SparkPlanner, plan: LogicalPlan): SparkPlan = { // TODO: We use next(), i.e. take the first plan returned by the planner, here for now, // but we will implement to choose the best plan. planner.plan(ReturnAnswer(plan)).next() } org.apache.spark.sql.execution.SparkPlanner class SparkPlanner(val session: SparkSession, val experimentalMethods: ExperimentalMethods) extends SparkStrategies with SQLConfHelper { override def strategies: Seq[Strategy] = experimentalMethods.extraStrategies ++ extraPlanningStrategies ++ ( LogicalQueryStageStrategy :: PythonEvals :: new DataSourceV2Strategy(session) :: FileSourceStrategy :: DataSourceStrategy :: SpecialLimits :: Aggregation :: Window :: JoinSelection :: InMemoryScans :: SparkScripts :: BasicOperators :: Nil) abstract class SparkStrategies extends QueryPlanner[SparkPlan] { self: SparkPlanner => override def plan(plan: LogicalPlan): Iterator[SparkPlan] = { super.plan(plan).map { p => val logicalPlan = plan match { case ReturnAnswer(rootPlan) => rootPlan case _ => plan } p.setLogicalLink(logicalPlan) p } } org.apache.spark.sql.catalyst.planning.QueryPlanner /** * Abstract class for transforming [[LogicalPlan]]s into physical plans. * Child classes are responsible for specifying a list of [[GenericStrategy]] objects that * each of which can return a list of possible physical plan options. * If a given strategy is unable to plan all of the remaining operators in the tree, * it can call [[GenericStrategy#planLater planLater]], which returns a placeholder * object that will be [[collectPlaceholders collected]] and filled in * using other available strategies. * * TODO: RIGHT NOW ONLY ONE PLAN IS RETURNED EVER... * PLAN SPACE EXPLORATION WILL BE IMPLEMENTED LATER. * * @tparam PhysicalPlan The type of physical plan produced by this [[QueryPlanner]] */ abstract class QueryPlanner[PhysicalPlan <: TreeNode[PhysicalPlan]] { /** A list of execution strategies that can be used by the planner */ def strategies: Seq[GenericStrategy[PhysicalPlan]] def plan(plan: LogicalPlan): Iterator[PhysicalPlan] = { // Obviously a lot to do here still... // Collect physical plan candidates. val candidates = strategies.iterator.flatMap(_(plan)) // The candidates may contain placeholders marked as [[planLater]], // so try to replace them by their child plans. val plans = candidates.flatMap { candidate => val placeholders = collectPlaceholders(candidate) if (placeholders.isEmpty) { // Take the candidate as is because it does not contain placeholders. Iterator(candidate) } else { // Plan the logical plan marked as [[planLater]] and replace the placeholders. placeholders.iterator.foldLeft(Iterator(candidate)) { case (candidatesWithPlaceholders, (placeholder, logicalPlan)) => // Plan the logical plan for the placeholder. val childPlans = this.plan(logicalPlan) ====> if there is planLater, recursively apply all the strategies again candidatesWithPlaceholders.flatMap { candidateWithPlaceholders => childPlans.map { childPlan => // Replace the placeholder by the child plan candidateWithPlaceholders.transformUp { case p if p.eq(placeholder) => childPlan } } } } } } val pruned = prunePlans(plans) assert(pruned.hasNext, s\"No plan for $plan\") pruned Code of prepared SparkPlan generation \u00b6 In QueryExecution.prepareForExecution(), rules (Rule[SparkPlan]) are applied => rules are defined in QueryExecution.preparations() org.apache.spark.sql.execution.QueryExecution#prepareForExecution protected def preparations: Seq[Rule[SparkPlan]] = { QueryExecution.preparations(sparkSession, Option(InsertAdaptiveSparkPlan(AdaptiveExecutionContext(sparkSession, this))), false) } /** * Construct a sequence of rules that are used to prepare a planned [[SparkPlan]] for execution. * These rules will make sure subqueries are planned, make use the data partitioning and ordering * are correct, insert whole stage code gen, and try to reduce the work done by reusing exchanges * and subqueries. */ private[execution] def preparations( sparkSession: SparkSession, adaptiveExecutionRule: Option[InsertAdaptiveSparkPlan] = None, subquery: Boolean): Seq[Rule[SparkPlan]] = { // `AdaptiveSparkPlanExec` is a leaf node. If inserted, all the following rules will be no-op // as the original plan is hidden behind `AdaptiveSparkPlanExec`. adaptiveExecutionRule.toSeq ++ Seq( CoalesceBucketsInJoin, PlanDynamicPruningFilters(sparkSession), PlanSubqueries(sparkSession), RemoveRedundantProjects, EnsureRequirements(), // `ReplaceHashWithSortAgg` needs to be added after `EnsureRequirements` to guarantee the // sort order of each node is checked to be valid. ReplaceHashWithSortAgg, // `RemoveRedundantSorts` needs to be added after `EnsureRequirements` to guarantee the same // number of partitions when instantiating PartitioningCollection. RemoveRedundantSorts, DisableUnnecessaryBucketedScan, ApplyColumnarRulesAndInsertTransitions( sparkSession.sessionState.columnarRules, outputsColumnar = false), CollapseCodegenStages()) ++ (if (subquery) { Nil } else { Seq(ReuseExchangeAndSubquery) }) } /** * Prepares a planned [[SparkPlan]] for execution by inserting shuffle operations and internal * row format conversions as needed. */ private[execution] def prepareForExecution( preparations: Seq[Rule[SparkPlan]], plan: SparkPlan): SparkPlan = { val planChangeLogger = new PlanChangeLogger[SparkPlan]() val preparedPlan = preparations.foldLeft(plan) { case (sp, rule) => val result = rule.apply(sp) planChangeLogger.logRule(rule.ruleName, sp, result) result } planChangeLogger.logBatch(\"Preparations\", plan, preparedPlan) preparedPlan }","title":"SparkPlan"},{"location":"SparkSQL/SparkPlan/#sparkplan","text":"","title":"SparkPlan"},{"location":"SparkSQL/SparkPlan/#sparks-planner","text":"1st Phase: Transforms the logical plan to the physical plan using Strategies QueryExecution.sparkPlan SparkPlanner.plan 2nd Phase: use a Rule Executor to make the Physical Plan ready for execution QueryExecution.prepareForExecution","title":"Spark's Planner"},{"location":"SparkSQL/SparkPlan/#call-stack","text":"How is QueryExecution.sparkPlan and QueryExecution.prepareForExecution invoked? QueryExecution.sparkPlan stack at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:141)=====> lazy val sparkPlan: SparkPlan - locked <0x328c> (a org.apache.spark.sql.execution.QueryExecution) at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:138) at org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:158) at org.apache.spark.sql.execution.QueryExecution$$Lambda$1861.30604162.apply(Unknown Source:-1) at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111) at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185) at org.apache.spark.sql.execution.QueryExecution$$Lambda$1461.609375192.apply(Unknown Source:-1) at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:512) at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185) at org.apache.spark.sql.execution.QueryExecution$$Lambda$1460.911201454.apply(Unknown Source:-1) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184) at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:158) at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151) at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:204) at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:251) at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:220) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103) at org.apache.spark.sql.execution.SQLExecution$$$Lambda$1698.2050525584.apply(Unknown Source:-1) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:171) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95) at org.apache.spark.sql.execution.SQLExecution$$$Lambda$1682.2000856156.apply(Unknown Source:-1) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3924) at org.apache.spark.sql.Dataset.collect(Dataset.scala:3188) org.apache.spark.sql.Dataset class Dataset[T] private[sql]( @DeveloperApi @Unstable @transient val queryExecution: QueryExecution, @DeveloperApi @Unstable @transient val encoder: Encoder[T]) extends Serializable { // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure // you wrap it with `withNewExecutionId` if this actions doesn't call other action. def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sparkSession.sessionState.executePlan(logicalPlan), encoder) ====> executePlan() is to create the QueryExecution } def collect(): Array[T] = withAction(\"collect\", queryExecution)(collectFromPlan) private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = { SQLExecution.withNewExecutionId(qe, Some(name)) { QueryExecution.withInternalError(s\"\"\"The \"$name\" action failed.\"\"\") { qe.executedPlan.resetMetrics() action(qe.executedPlan) } } } org.apache.spark.sql.execution.SQLExecution#withNewExecutionId /** * Wrap an action that will execute \"queryExecution\" to track all Spark jobs in the body so that * we can connect them with an execution. */ def withNewExecutionId[T]( queryExecution: QueryExecution, name: Option[String] = None)(body: => T): T = queryExecution.sparkSession.withActive { org.apache.spark.sql.execution.QueryExecution#explainString def explainString( mode: ExplainMode, maxFields: Int = SQLConf.get.maxToStringFields): String = { val concat = new PlanStringConcat() explainString(mode, maxFields, concat.append) withRedaction { concat.toString } } org.apache.spark.sql.execution.QueryExecution#executedPlan // executedPlan should not be used to initialize any SparkPlan. It should be // only used for execution. =====> ??? lazy val executedPlan: SparkPlan = { // We need to materialize the optimizedPlan here, before tracking the planning phase, to ensure // that the optimization time is not counted as part of the planning phase. assertOptimized() executePhase(QueryPlanningTracker.PLANNING) { // clone the plan to avoid sharing the plan instance between different stages like analyzing, // optimizing and planning. QueryExecution.prepareForExecution(preparations, sparkPlan.clone()) =====> sparkPlan is initialized here and then call QueryExecution.prepareForExecution } }","title":"Call stack"},{"location":"SparkSQL/SparkPlan/#code-of-sessionstate-to-create-queryexecution","text":"org.apache.spark.sql.SparkSession#sessionState /** * State isolated across sessions, including SQL configurations, temporary tables, registered * functions, and everything else that accepts a [[org.apache.spark.sql.internal.SQLConf]]. * If `parentSessionState` is not null, the `SessionState` will be a copy of the parent. * * This is internal to Spark and there is no guarantee on interface stability. * * @since 2.2.0 */ @Unstable @transient lazy val sessionState: SessionState = { parentSessionState .map(_.clone(this)) .getOrElse { val state = SparkSession.instantiateSessionState( SparkSession.sessionStateClassName(sharedState.conf), self) state } } org.apache.spark.sql.SparkSession#instantiateSessionState /** * Helper method to create an instance of `SessionState` based on `className` from conf. * The result is either `SessionState` or a Hive based `SessionState`. */ private def instantiateSessionState( className: String, sparkSession: SparkSession): SessionState = { try { // invoke new [Hive]SessionStateBuilder( // SparkSession, // Option[SessionState]) val clazz = Utils.classForName(className) val ctor = clazz.getConstructors.head ctor.newInstance(sparkSession, None).asInstanceOf[BaseSessionStateBuilder].build() } catch { case NonFatal(e) => throw new IllegalArgumentException(s\"Error while instantiating '$className':\", e) } } org.apache.spark.sql.SparkSession#sessionStateClassName private val HIVE_SESSION_STATE_BUILDER_CLASS_NAME = \"org.apache.spark.sql.hive.HiveSessionStateBuilder\" private def sessionStateClassName(conf: SparkConf): String = { conf.get(CATALOG_IMPLEMENTATION) match { case \"hive\" => HIVE_SESSION_STATE_BUILDER_CLASS_NAME case \"in-memory\" => classOf[SessionStateBuilder].getCanonicalName } } class SessionStateBuilder( session: SparkSession, parentState: Option[SessionState]) extends BaseSessionStateBuilder(session, parentState) { override protected def newBuilder: NewBuilder = new SessionStateBuilder(_, _) } or Hive /** * Builder that produces a Hive-aware `SessionState`. */ class HiveSessionStateBuilder( session: SparkSession, parentState: Option[SessionState]) extends BaseSessionStateBuilder(session, parentState) { org.apache.spark.sql.internal.BaseSessionStateBuilder#build def build(): SessionState = { new SessionState( session.sharedState, conf, experimentalMethods, functionRegistry, tableFunctionRegistry, udfRegistration, () => catalog, sqlParser, () => analyzer, () => optimizer, planner, () => streamingQueryManager, listenerManager, () => resourceLoader, createQueryExecution, createClone, columnarRules, adaptiveRulesHolder) } } org.apache.spark.sql.internal.BaseSessionStateBuilder#createQueryExecution protected def createQueryExecution: (LogicalPlan, CommandExecutionMode.Value) => QueryExecution = (plan, mode) => new QueryExecution(session, plan, mode = mode) org.apache.spark.sql.execution.QueryExecution /** * The primary workflow for executing relational queries using Spark. Designed to allow easy * access to the intermediate phases of query execution for developers. * * While this is not a public class, we should avoid changing the function names for the sake of * changing them, because a lot of developers use the feature for debugging. */ class QueryExecution( val sparkSession: SparkSession, val logical: LogicalPlan, val tracker: QueryPlanningTracker = new QueryPlanningTracker, val mode: CommandExecutionMode.Value = CommandExecutionMode.ALL) extends Logging {","title":"Code of SessionState to create QueryExecution"},{"location":"SparkSQL/SparkPlan/#code-of-sparkplan-generation","text":"QueryExecution.sparkPlan => QueryExecution.createSparkPlan SparkPlanner.plan (strategies defined in SparkPlanner) => SparkStrategies.plan => QueryPlanner.plan org.apache.spark.sql.execution.QueryExecution /** * The primary workflow for executing relational queries using Spark. Designed to allow easy * access to the intermediate phases of query execution for developers. * * While this is not a public class, we should avoid changing the function names for the sake of * changing them, because a lot of developers use the feature for debugging. */ class QueryExecution( val sparkSession: SparkSession, val logical: LogicalPlan, val tracker: QueryPlanningTracker = new QueryPlanningTracker, val mode: CommandExecutionMode.Value = CommandExecutionMode.ALL) extends Logging { lazy val sparkPlan: SparkPlan = { // We need to materialize the optimizedPlan here because sparkPlan is also tracked under // the planning phase assertOptimized() executePhase(QueryPlanningTracker.PLANNING) { // Clone the logical plan here, in case the planner rules change the states of the logical // plan. QueryExecution.createSparkPlan(sparkSession, planner, optimizedPlan.clone()) } } /** * Transform a [[LogicalPlan]] into a [[SparkPlan]]. * * Note that the returned physical plan still needs to be prepared for execution. */ def createSparkPlan( sparkSession: SparkSession, planner: SparkPlanner, plan: LogicalPlan): SparkPlan = { // TODO: We use next(), i.e. take the first plan returned by the planner, here for now, // but we will implement to choose the best plan. planner.plan(ReturnAnswer(plan)).next() } org.apache.spark.sql.execution.SparkPlanner class SparkPlanner(val session: SparkSession, val experimentalMethods: ExperimentalMethods) extends SparkStrategies with SQLConfHelper { override def strategies: Seq[Strategy] = experimentalMethods.extraStrategies ++ extraPlanningStrategies ++ ( LogicalQueryStageStrategy :: PythonEvals :: new DataSourceV2Strategy(session) :: FileSourceStrategy :: DataSourceStrategy :: SpecialLimits :: Aggregation :: Window :: JoinSelection :: InMemoryScans :: SparkScripts :: BasicOperators :: Nil) abstract class SparkStrategies extends QueryPlanner[SparkPlan] { self: SparkPlanner => override def plan(plan: LogicalPlan): Iterator[SparkPlan] = { super.plan(plan).map { p => val logicalPlan = plan match { case ReturnAnswer(rootPlan) => rootPlan case _ => plan } p.setLogicalLink(logicalPlan) p } } org.apache.spark.sql.catalyst.planning.QueryPlanner /** * Abstract class for transforming [[LogicalPlan]]s into physical plans. * Child classes are responsible for specifying a list of [[GenericStrategy]] objects that * each of which can return a list of possible physical plan options. * If a given strategy is unable to plan all of the remaining operators in the tree, * it can call [[GenericStrategy#planLater planLater]], which returns a placeholder * object that will be [[collectPlaceholders collected]] and filled in * using other available strategies. * * TODO: RIGHT NOW ONLY ONE PLAN IS RETURNED EVER... * PLAN SPACE EXPLORATION WILL BE IMPLEMENTED LATER. * * @tparam PhysicalPlan The type of physical plan produced by this [[QueryPlanner]] */ abstract class QueryPlanner[PhysicalPlan <: TreeNode[PhysicalPlan]] { /** A list of execution strategies that can be used by the planner */ def strategies: Seq[GenericStrategy[PhysicalPlan]] def plan(plan: LogicalPlan): Iterator[PhysicalPlan] = { // Obviously a lot to do here still... // Collect physical plan candidates. val candidates = strategies.iterator.flatMap(_(plan)) // The candidates may contain placeholders marked as [[planLater]], // so try to replace them by their child plans. val plans = candidates.flatMap { candidate => val placeholders = collectPlaceholders(candidate) if (placeholders.isEmpty) { // Take the candidate as is because it does not contain placeholders. Iterator(candidate) } else { // Plan the logical plan marked as [[planLater]] and replace the placeholders. placeholders.iterator.foldLeft(Iterator(candidate)) { case (candidatesWithPlaceholders, (placeholder, logicalPlan)) => // Plan the logical plan for the placeholder. val childPlans = this.plan(logicalPlan) ====> if there is planLater, recursively apply all the strategies again candidatesWithPlaceholders.flatMap { candidateWithPlaceholders => childPlans.map { childPlan => // Replace the placeholder by the child plan candidateWithPlaceholders.transformUp { case p if p.eq(placeholder) => childPlan } } } } } } val pruned = prunePlans(plans) assert(pruned.hasNext, s\"No plan for $plan\") pruned","title":"Code of SparkPlan generation"},{"location":"SparkSQL/SparkPlan/#code-of-prepared-sparkplan-generation","text":"In QueryExecution.prepareForExecution(), rules (Rule[SparkPlan]) are applied => rules are defined in QueryExecution.preparations() org.apache.spark.sql.execution.QueryExecution#prepareForExecution protected def preparations: Seq[Rule[SparkPlan]] = { QueryExecution.preparations(sparkSession, Option(InsertAdaptiveSparkPlan(AdaptiveExecutionContext(sparkSession, this))), false) } /** * Construct a sequence of rules that are used to prepare a planned [[SparkPlan]] for execution. * These rules will make sure subqueries are planned, make use the data partitioning and ordering * are correct, insert whole stage code gen, and try to reduce the work done by reusing exchanges * and subqueries. */ private[execution] def preparations( sparkSession: SparkSession, adaptiveExecutionRule: Option[InsertAdaptiveSparkPlan] = None, subquery: Boolean): Seq[Rule[SparkPlan]] = { // `AdaptiveSparkPlanExec` is a leaf node. If inserted, all the following rules will be no-op // as the original plan is hidden behind `AdaptiveSparkPlanExec`. adaptiveExecutionRule.toSeq ++ Seq( CoalesceBucketsInJoin, PlanDynamicPruningFilters(sparkSession), PlanSubqueries(sparkSession), RemoveRedundantProjects, EnsureRequirements(), // `ReplaceHashWithSortAgg` needs to be added after `EnsureRequirements` to guarantee the // sort order of each node is checked to be valid. ReplaceHashWithSortAgg, // `RemoveRedundantSorts` needs to be added after `EnsureRequirements` to guarantee the same // number of partitions when instantiating PartitioningCollection. RemoveRedundantSorts, DisableUnnecessaryBucketedScan, ApplyColumnarRulesAndInsertTransitions( sparkSession.sessionState.columnarRules, outputsColumnar = false), CollapseCodegenStages()) ++ (if (subquery) { Nil } else { Seq(ReuseExchangeAndSubquery) }) } /** * Prepares a planned [[SparkPlan]] for execution by inserting shuffle operations and internal * row format conversions as needed. */ private[execution] def prepareForExecution( preparations: Seq[Rule[SparkPlan]], plan: SparkPlan): SparkPlan = { val planChangeLogger = new PlanChangeLogger[SparkPlan]() val preparedPlan = preparations.foldLeft(plan) { case (sp, rule) => val result = rule.apply(sp) planChangeLogger.logRule(rule.ruleName, sp, result) result } planChangeLogger.logBatch(\"Preparations\", plan, preparedPlan) preparedPlan }","title":"Code of prepared SparkPlan generation"},{"location":"SparkStructuredStreaming/KafkaSqlConnector/","text":"InternalKafkaProducerPool KafkaOffsetReaderAdmin InternalKafkaProducerPool \u00b6 org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool#InternalKafkaProducerPool(org.apache.spark.SparkConf) /** * Provides object pool for [[CachedKafkaProducer]] which is grouped by * [[org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.CacheKey]]. */ private[producer] class InternalKafkaProducerPool( executorService: ScheduledExecutorService, val clock: Clock, conf: SparkConf) extends Logging { ... ShutdownHookManager.addShutdownHook { () => try { pool.shutdown() } catch { case e: Throwable => logWarning(\"Ignoring Exception while shutting down pools from shutdown hook\", e) } } KafkaOffsetReaderAdmin \u00b6 org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin#withRetries /** * Helper function that does multiple retries on a body of code that returns offsets. * Retries are needed to handle transient failures. For e.g. race conditions between getting * assignment and getting position while topics/partitions are deleted can cause NPEs. */ private def withRetries(body: => Map[TopicPartition, Long]): Map[TopicPartition, Long] = { synchronized { var result: Option[Map[TopicPartition, Long]] = None var attempt = 1 var lastException: Throwable = null while (result.isEmpty && attempt <= maxOffsetFetchAttempts && !Thread.currentThread().isInterrupted) { try { result = Some(body) } catch { case NonFatal(e) => lastException = e logWarning(s\"Error in attempt $attempt getting Kafka offsets: \", e) attempt += 1 Thread.sleep(offsetFetchAttemptIntervalMs) resetAdmin() } } if (Thread.interrupted()) { throw new InterruptedException() } if (result.isEmpty) { assert(attempt > maxOffsetFetchAttempts) assert(lastException != null) throw lastException } result.get } }","title":"KafkaSqlConnector"},{"location":"SparkStructuredStreaming/KafkaSqlConnector/#internalkafkaproducerpool","text":"org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool#InternalKafkaProducerPool(org.apache.spark.SparkConf) /** * Provides object pool for [[CachedKafkaProducer]] which is grouped by * [[org.apache.spark.sql.kafka010.producer.InternalKafkaProducerPool.CacheKey]]. */ private[producer] class InternalKafkaProducerPool( executorService: ScheduledExecutorService, val clock: Clock, conf: SparkConf) extends Logging { ... ShutdownHookManager.addShutdownHook { () => try { pool.shutdown() } catch { case e: Throwable => logWarning(\"Ignoring Exception while shutting down pools from shutdown hook\", e) } }","title":"InternalKafkaProducerPool"},{"location":"SparkStructuredStreaming/KafkaSqlConnector/#kafkaoffsetreaderadmin","text":"org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin#withRetries /** * Helper function that does multiple retries on a body of code that returns offsets. * Retries are needed to handle transient failures. For e.g. race conditions between getting * assignment and getting position while topics/partitions are deleted can cause NPEs. */ private def withRetries(body: => Map[TopicPartition, Long]): Map[TopicPartition, Long] = { synchronized { var result: Option[Map[TopicPartition, Long]] = None var attempt = 1 var lastException: Throwable = null while (result.isEmpty && attempt <= maxOffsetFetchAttempts && !Thread.currentThread().isInterrupted) { try { result = Some(body) } catch { case NonFatal(e) => lastException = e logWarning(s\"Error in attempt $attempt getting Kafka offsets: \", e) attempt += 1 Thread.sleep(offsetFetchAttemptIntervalMs) resetAdmin() } } if (Thread.interrupted()) { throw new InterruptedException() } if (result.isEmpty) { assert(attempt > maxOffsetFetchAttempts) assert(lastException != null) throw lastException } result.get } }","title":"KafkaOffsetReaderAdmin"},{"location":"SparkStructuredStreaming/MicroBatchExecution/","text":"StreamingQueryManager MicroBatchExecution Watermark Support removeKeysOlderThanWatermark StreamingQueryManager \u00b6 SparkSession -> SessionState -> StreamingQueryManager private[sql] class SessionState( ...... // The streamingQueryManager is lazy to avoid creating a StreamingQueryManager for each session // when connecting to ThriftServer. lazy val streamingQueryManager: StreamingQueryManager = streamingQueryManagerBuilder() => org.apache.spark.sql.streaming.DataStreamWriter#startQuery private def startQuery( sink: Table, newOptions: CaseInsensitiveMap[String], recoverFromCheckpoint: Boolean = true, catalogAndIdent: Option[(TableCatalog, Identifier)] = None, catalogTable: Option[CatalogTable] = None): StreamingQuery = { val useTempCheckpointLocation = SOURCES_ALLOW_ONE_TIME_QUERY.contains(source) df.sparkSession.sessionState.streamingQueryManager.startQuery( newOptions.get(\"queryName\"), newOptions.get(\"checkpointLocation\"), df, newOptions.originalMap, sink, outputMode, useTempCheckpointLocation = useTempCheckpointLocation, recoverFromCheckpointLocation = recoverFromCheckpoint, trigger = trigger, catalogAndIdent = catalogAndIdent, catalogTable = catalogTable) } org.apache.spark.sql.streaming.StreamingQueryManager#startQuery /** * Start a [[StreamingQuery]]. * * @param userSpecifiedName Query name optionally specified by the user. * @param userSpecifiedCheckpointLocation Checkpoint location optionally specified by the user. * @param df Streaming DataFrame. * @param sink Sink to write the streaming outputs. * @param outputMode Output mode for the sink. * @param useTempCheckpointLocation Whether to use a temporary checkpoint location when the user * has not specified one. If false, then error will be thrown. * @param recoverFromCheckpointLocation Whether to recover query from the checkpoint location. * If false and the checkpoint location exists, then error * will be thrown. * @param trigger [[Trigger]] for the query. * @param triggerClock [[Clock]] to use for the triggering. * @param catalogAndIdent Catalog and identifier for the sink, set when it is a V2 catalog table */ @throws[TimeoutException] private[sql] def startQuery( userSpecifiedName: Option[String], userSpecifiedCheckpointLocation: Option[String], df: DataFrame, extraOptions: Map[String, String], sink: Table, outputMode: OutputMode, useTempCheckpointLocation: Boolean = false, recoverFromCheckpointLocation: Boolean = true, trigger: Trigger = Trigger.ProcessingTime(0), triggerClock: Clock = new SystemClock(), catalogAndIdent: Option[(TableCatalog, Identifier)] = None, catalogTable: Option[CatalogTable] = None): StreamingQuery = { val query = createQuery( userSpecifiedName, userSpecifiedCheckpointLocation, df, extraOptions, sink, outputMode, useTempCheckpointLocation, recoverFromCheckpointLocation, trigger, triggerClock, catalogAndIdent, catalogTable) // scalastyle:on argcount // The following code block checks if a stream with the same name or id is running. Then it // returns an Option of an already active stream to stop outside of the lock // to avoid a deadlock. val activeRunOpt = activeQueriesSharedLock.synchronized { // Make sure no other query with same name is active userSpecifiedName.foreach { name => if (activeQueries.values.exists(_.name == name)) { throw new IllegalArgumentException(s\"Cannot start query with name $name as a query \" + s\"with that name is already active in this SparkSession\") } } // Make sure no other query with same id is active across all sessions val activeOption = Option(sparkSession.sharedState.activeStreamingQueries.get(query.id)) .orElse(activeQueries.get(query.id)) // shouldn't be needed but paranoia ... val shouldStopActiveRun = sparkSession.conf.get(SQLConf.STREAMING_STOP_ACTIVE_RUN_ON_RESTART) if (activeOption.isDefined) { if (shouldStopActiveRun) { val oldQuery = activeOption.get logWarning(s\"Stopping existing streaming query [id=${query.id}, \" + s\"runId=${oldQuery.runId}], as a new run is being started.\") Some(oldQuery) } else { throw new IllegalStateException( s\"Cannot start query with id ${query.id} as another query with same id is \" + s\"already active. Perhaps you are attempting to restart a query from checkpoint \" + s\"that is already active. You may stop the old query by setting the SQL \" + \"configuration: \" + s\"\"\"spark.conf.set(\"${SQLConf.STREAMING_STOP_ACTIVE_RUN_ON_RESTART.key}\", true) \"\"\" + \"and retry.\") } } else { // nothing to stop so, no-op None } } // stop() will clear the queryId from activeStreamingQueries as well as activeQueries activeRunOpt.foreach(_.stop()) activeQueriesSharedLock.synchronized { // We still can have a race condition when two concurrent instances try to start the same // stream, while a third one was already active and stopped above. In this case, we throw a // ConcurrentModificationException. val oldActiveQuery = sparkSession.sharedState.activeStreamingQueries.put( query.id, query.streamingQuery) // we need to put the StreamExecution, not the wrapper if (oldActiveQuery != null) { throw QueryExecutionErrors.concurrentQueryInstanceError() } activeQueries.put(query.id, query) } try { // When starting a query, it will call `StreamingQueryListener.onQueryStarted` synchronously. // As it's provided by the user and can run arbitrary codes, we must not hold any lock here. // Otherwise, it's easy to cause dead-lock, or block too long if the user codes take a long // time to finish. query.streamingQuery.start() ===> org.apache.spark.sql.execution.streaming.StreamExecution#start } catch { case e: Throwable => unregisterTerminatedStream(query) throw e } query } org.apache.spark.sql.execution.streaming.StreamExecution#start /** * Manages the execution of a streaming Spark SQL query that is occurring in a separate thread. * Unlike a standard query, a streaming query executes repeatedly each time new data arrives at any * [[Source]] present in the query plan. Whenever new data arrives, a [[QueryExecution]] is created * and the results are committed transactionally to the given [[Sink]]. * * @param deleteCheckpointOnStop whether to delete the checkpoint if the query is stopped without * errors. Checkpoint deletion can be forced with the appropriate * Spark configuration. */ abstract class StreamExecution( override val sparkSession: SparkSession, override val name: String, val resolvedCheckpointRoot: String, val analyzedPlan: LogicalPlan, val sink: Table, val trigger: Trigger, val triggerClock: Clock, val outputMode: OutputMode, deleteCheckpointOnStop: Boolean) extends StreamingQuery with ProgressReporter with Logging { /** * Starts the execution. This returns only after the thread has started and [[QueryStartedEvent]] * has been posted to all the listeners. */ def start(): Unit = { logInfo(s\"Starting $prettyIdString. Use $resolvedCheckpointRoot to store the query checkpoint.\") queryExecutionThread.setDaemon(true) queryExecutionThread.start() startLatch.await() // Wait until thread started and QueryStart event has been posted } /** * The thread that runs the micro-batches of this stream. Note that this thread must be * [[org.apache.spark.util.UninterruptibleThread]] to workaround KAFKA-1894: interrupting a * running `KafkaConsumer` may cause endless loop. */ val queryExecutionThread: QueryExecutionThread = new QueryExecutionThread(s\"stream execution thread for $prettyIdString\") { override def run(): Unit = { // To fix call site like \"run at <unknown>:0\", we bridge the call site from the caller // thread to this micro batch thread sparkSession.sparkContext.setCallSite(callSite) runStream() } } /** * Activate the stream and then wrap a callout to runActivatedStream, handling start and stop. * * Note that this method ensures that [[QueryStartedEvent]] and [[QueryTerminatedEvent]] are * posted such that listeners are guaranteed to get a start event before a termination. * Furthermore, this method also ensures that [[QueryStartedEvent]] event is posted before the * `start()` method returns. */ private def runStream(): Unit = { try { ... if (state.compareAndSet(INITIALIZING, ACTIVE)) { // Unblock `awaitInitialization` initializationLatch.countDown() runActivatedStream(sparkSessionForStream) ===> implemented by MicroBatchExecution or ContinuousExecution updateStatusMessage(\"Stopped\") } else { // `stop()` is already called. Let `finally` finish the cleanup. } ...... } catch { case e if isInterruptedByStop(e, sparkSession.sparkContext) => // interrupted by stop() updateStatusMessage(\"Stopped\") case e: IOException if e.getMessage != null && e.getMessage.startsWith(classOf[InterruptedException].getName) && state.get == TERMINATED => // This is a workaround for HADOOP-12074: `Shell.runCommand` converts `InterruptedException` // to `new IOException(ie.toString())` before Hadoop 2.8. updateStatusMessage(\"Stopped\") case e: Throwable => val message = if (e.getMessage == null) \"\" else e.getMessage streamDeathCause = new StreamingQueryException( toDebugString(includeLogicalPlan = isInitialized), cause = e, committedOffsets.toOffsetSeq(sources, offsetSeqMetadata).toString, availableOffsets.toOffsetSeq(sources, offsetSeqMetadata).toString, errorClass = \"STREAM_FAILED\", messageParameters = Map( \"id\" -> id.toString, \"runId\" -> runId.toString, \"message\" -> message)) logError(s\"Query $prettyIdString terminated with error\", e) updateStatusMessage(s\"Terminated with exception: $message\") // Rethrow the fatal errors to allow the user using `Thread.UncaughtExceptionHandler` to // handle them if (!NonFatal(e)) { throw e } } finally queryExecutionThread.runUninterruptibly { // The whole `finally` block must run inside `runUninterruptibly` to avoid being interrupted // when a query is stopped by the user. We need to make sure the following codes finish // otherwise it may throw `InterruptedException` to `UncaughtExceptionHandler` (SPARK-21248). // Release latches to unblock the user codes since exception can happen in any place and we // may not get a chance to release them startLatch.countDown() initializationLatch.countDown() try { stopSources() cleanup() state.set(TERMINATED) currentStatus = status.copy(isTriggerActive = false, isDataAvailable = false) // Update metrics and status sparkSession.sparkContext.env.metricsSystem.removeSource(streamMetrics) // Notify others sparkSession.streams.notifyQueryTermination(StreamExecution.this) postEvent( new QueryTerminatedEvent(id, runId, exception.map(_.cause).map(Utils.exceptionString))) // Delete the temp checkpoint when either force delete enabled or the query didn't fail if (deleteCheckpointOnStop && (sparkSession.sessionState.conf .getConf(SQLConf.FORCE_DELETE_TEMP_CHECKPOINT_LOCATION) || exception.isEmpty)) { val checkpointPath = new Path(resolvedCheckpointRoot) try { logInfo(s\"Deleting checkpoint $checkpointPath.\") fileManager.delete(checkpointPath) } catch { case NonFatal(e) => // Deleting temp checkpoint folder is best effort, don't throw non fatal exceptions // when we cannot delete them. logWarning(s\"Cannot delete $checkpointPath\", e) } } } finally { awaitProgressLock.lock() try { // Wake up any threads that are waiting for the stream to progress. awaitProgressLockCondition.signalAll() } finally { awaitProgressLock.unlock() } terminationLatch.countDown() } } } MicroBatchExecution \u00b6 class MicroBatchExecution( sparkSession: SparkSession, trigger: Trigger, triggerClock: Clock, extraOptions: Map[String, String], plan: WriteToStream) extends StreamExecution( sparkSession, plan.name, plan.resolvedCheckpointLocation, plan.inputQuery, plan.sink, trigger, triggerClock, plan.outputMode, plan.deleteCheckpointOnStop) with AsyncLogPurge { /** * Repeatedly attempts to run batches as data arrives. */ protected def runActivatedStream(sparkSessionForStream: SparkSession): Unit = { val noDataBatchesEnabled = sparkSessionForStream.sessionState.conf.streamingNoDataMicroBatchesEnabled triggerExecutor.execute(() => { if (isActive) { // check if there are any previous errors and bubble up any existing async operations errorNotifier.throwErrorIfExists var currentBatchHasNewData = false // Whether the current batch had new data startTrigger() reportTimeTaken(\"triggerExecution\") { // We'll do this initialization only once every start / restart if (currentBatchId < 0) { AcceptsLatestSeenOffsetHandler.setLatestSeenOffsetOnSources( offsetLog.getLatest().map(_._2), sources) populateStartOffsets(sparkSessionForStream) logInfo(s\"Stream started from $committedOffsets\") } // Set this before calling constructNextBatch() so any Spark jobs executed by sources // while getting new data have the correct description sparkSession.sparkContext.setJobDescription(getBatchDescriptionString) // Try to construct the next batch. This will return true only if the next batch is // ready and runnable. Note that the current batch may be runnable even without // new data to process as `constructNextBatch` may decide to run a batch for // state cleanup, etc. `isNewDataAvailable` will be updated to reflect whether new data // is available or not. if (!isCurrentBatchConstructed) { isCurrentBatchConstructed = constructNextBatch(noDataBatchesEnabled) } // Record the trigger offset range for progress reporting *before* processing the batch recordTriggerOffsets( from = committedOffsets, to = availableOffsets, latest = latestOffsets) // Remember whether the current batch has data or not. This will be required later // for bookkeeping after running the batch, when `isNewDataAvailable` will have changed // to false as the batch would have already processed the available data. currentBatchHasNewData = isNewDataAvailable currentStatus = currentStatus.copy(isDataAvailable = isNewDataAvailable) if (isCurrentBatchConstructed) { if (currentBatchHasNewData) updateStatusMessage(\"Processing new data\") else updateStatusMessage(\"No new data but cleaning up state\") runBatch(sparkSessionForStream) } else { updateStatusMessage(\"Waiting for data to arrive\") } } // Must be outside reportTimeTaken so it is recorded finishTrigger(currentBatchHasNewData, isCurrentBatchConstructed) // Signal waiting threads. Note this must be after finishTrigger() to ensure all // activities (progress generation, etc.) have completed before signaling. withProgressLocked { awaitProgressLockCondition.signalAll() } // If the current batch has been executed, then increment the batch id and reset flag. // Otherwise, there was no data to execute the batch and sleep for some time if (isCurrentBatchConstructed) { currentBatchId += 1 isCurrentBatchConstructed = false } else if (triggerExecutor.isInstanceOf[MultiBatchExecutor]) { logInfo(\"Finished processing all available data for the trigger, terminating this \" + \"Trigger.AvailableNow query\") state.set(TERMINATED) } else Thread.sleep(pollingDelayMs) } updateStatusMessage(\"Waiting for next trigger\") isActive }) } Watermark Support \u00b6 removeKeysOlderThanWatermark \u00b6 org.apache.spark.sql.execution.streaming.WatermarkSupport#removeKeysOlderThanWatermark protected def removeKeysOlderThanWatermark(store: StateStore): Unit = { if (watermarkPredicateForKeysForEviction.nonEmpty) { val numRemovedStateRows = longMetric(\"numRemovedStateRows\") store.iterator().foreach { rowPair => if (watermarkPredicateForKeysForEviction.get.eval(rowPair.key)) { store.remove(rowPair.key) numRemovedStateRows += 1 } } } } protected def removeKeysOlderThanWatermark( storeManager: StreamingAggregationStateManager, store: StateStore): Unit = { if (watermarkPredicateForKeysForEviction.nonEmpty) { val numRemovedStateRows = longMetric(\"numRemovedStateRows\") storeManager.keys(store).foreach { keyRow => if (watermarkPredicateForKeysForEviction.get.eval(keyRow)) { storeManager.remove(store, keyRow) numRemovedStateRows += 1 } } } } org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider.RocksDBStateStore#iterator override def iterator(): Iterator[UnsafeRowPair] = { rocksDB.iterator().map { kv => val rowPair = encoder.decode(kv) if (!isValidated && rowPair.value != null) { StateStoreProvider.validateStateRowFormat( rowPair.key, keySchema, rowPair.value, valueSchema, storeConf) isValidated = true } rowPair } } org.apache.spark.sql.execution.streaming.state.RocksDB#iterator def iterator(): Iterator[ByteArrayPair] = { val iter = writeBatch.newIteratorWithBase(db.newIterator()) logInfo(s\"Getting iterator from version $loadedVersion\") iter.seekToFirst() // Attempt to close this iterator if there is a task failure, or a task interruption. // This is a hack because it assumes that the RocksDB is running inside a task. Option(TaskContext.get()).foreach { tc => tc.addTaskCompletionListener[Unit] { _ => iter.close() } } new NextIterator[ByteArrayPair] { override protected def getNext(): ByteArrayPair = { if (iter.isValid) { byteArrayPair.set(iter.key, iter.value) iter.next() byteArrayPair } else { finished = true iter.close() null } } override protected def close(): Unit = { iter.close() } } }","title":"MicroBatchExecution"},{"location":"SparkStructuredStreaming/MicroBatchExecution/#streamingquerymanager","text":"SparkSession -> SessionState -> StreamingQueryManager private[sql] class SessionState( ...... // The streamingQueryManager is lazy to avoid creating a StreamingQueryManager for each session // when connecting to ThriftServer. lazy val streamingQueryManager: StreamingQueryManager = streamingQueryManagerBuilder() => org.apache.spark.sql.streaming.DataStreamWriter#startQuery private def startQuery( sink: Table, newOptions: CaseInsensitiveMap[String], recoverFromCheckpoint: Boolean = true, catalogAndIdent: Option[(TableCatalog, Identifier)] = None, catalogTable: Option[CatalogTable] = None): StreamingQuery = { val useTempCheckpointLocation = SOURCES_ALLOW_ONE_TIME_QUERY.contains(source) df.sparkSession.sessionState.streamingQueryManager.startQuery( newOptions.get(\"queryName\"), newOptions.get(\"checkpointLocation\"), df, newOptions.originalMap, sink, outputMode, useTempCheckpointLocation = useTempCheckpointLocation, recoverFromCheckpointLocation = recoverFromCheckpoint, trigger = trigger, catalogAndIdent = catalogAndIdent, catalogTable = catalogTable) } org.apache.spark.sql.streaming.StreamingQueryManager#startQuery /** * Start a [[StreamingQuery]]. * * @param userSpecifiedName Query name optionally specified by the user. * @param userSpecifiedCheckpointLocation Checkpoint location optionally specified by the user. * @param df Streaming DataFrame. * @param sink Sink to write the streaming outputs. * @param outputMode Output mode for the sink. * @param useTempCheckpointLocation Whether to use a temporary checkpoint location when the user * has not specified one. If false, then error will be thrown. * @param recoverFromCheckpointLocation Whether to recover query from the checkpoint location. * If false and the checkpoint location exists, then error * will be thrown. * @param trigger [[Trigger]] for the query. * @param triggerClock [[Clock]] to use for the triggering. * @param catalogAndIdent Catalog and identifier for the sink, set when it is a V2 catalog table */ @throws[TimeoutException] private[sql] def startQuery( userSpecifiedName: Option[String], userSpecifiedCheckpointLocation: Option[String], df: DataFrame, extraOptions: Map[String, String], sink: Table, outputMode: OutputMode, useTempCheckpointLocation: Boolean = false, recoverFromCheckpointLocation: Boolean = true, trigger: Trigger = Trigger.ProcessingTime(0), triggerClock: Clock = new SystemClock(), catalogAndIdent: Option[(TableCatalog, Identifier)] = None, catalogTable: Option[CatalogTable] = None): StreamingQuery = { val query = createQuery( userSpecifiedName, userSpecifiedCheckpointLocation, df, extraOptions, sink, outputMode, useTempCheckpointLocation, recoverFromCheckpointLocation, trigger, triggerClock, catalogAndIdent, catalogTable) // scalastyle:on argcount // The following code block checks if a stream with the same name or id is running. Then it // returns an Option of an already active stream to stop outside of the lock // to avoid a deadlock. val activeRunOpt = activeQueriesSharedLock.synchronized { // Make sure no other query with same name is active userSpecifiedName.foreach { name => if (activeQueries.values.exists(_.name == name)) { throw new IllegalArgumentException(s\"Cannot start query with name $name as a query \" + s\"with that name is already active in this SparkSession\") } } // Make sure no other query with same id is active across all sessions val activeOption = Option(sparkSession.sharedState.activeStreamingQueries.get(query.id)) .orElse(activeQueries.get(query.id)) // shouldn't be needed but paranoia ... val shouldStopActiveRun = sparkSession.conf.get(SQLConf.STREAMING_STOP_ACTIVE_RUN_ON_RESTART) if (activeOption.isDefined) { if (shouldStopActiveRun) { val oldQuery = activeOption.get logWarning(s\"Stopping existing streaming query [id=${query.id}, \" + s\"runId=${oldQuery.runId}], as a new run is being started.\") Some(oldQuery) } else { throw new IllegalStateException( s\"Cannot start query with id ${query.id} as another query with same id is \" + s\"already active. Perhaps you are attempting to restart a query from checkpoint \" + s\"that is already active. You may stop the old query by setting the SQL \" + \"configuration: \" + s\"\"\"spark.conf.set(\"${SQLConf.STREAMING_STOP_ACTIVE_RUN_ON_RESTART.key}\", true) \"\"\" + \"and retry.\") } } else { // nothing to stop so, no-op None } } // stop() will clear the queryId from activeStreamingQueries as well as activeQueries activeRunOpt.foreach(_.stop()) activeQueriesSharedLock.synchronized { // We still can have a race condition when two concurrent instances try to start the same // stream, while a third one was already active and stopped above. In this case, we throw a // ConcurrentModificationException. val oldActiveQuery = sparkSession.sharedState.activeStreamingQueries.put( query.id, query.streamingQuery) // we need to put the StreamExecution, not the wrapper if (oldActiveQuery != null) { throw QueryExecutionErrors.concurrentQueryInstanceError() } activeQueries.put(query.id, query) } try { // When starting a query, it will call `StreamingQueryListener.onQueryStarted` synchronously. // As it's provided by the user and can run arbitrary codes, we must not hold any lock here. // Otherwise, it's easy to cause dead-lock, or block too long if the user codes take a long // time to finish. query.streamingQuery.start() ===> org.apache.spark.sql.execution.streaming.StreamExecution#start } catch { case e: Throwable => unregisterTerminatedStream(query) throw e } query } org.apache.spark.sql.execution.streaming.StreamExecution#start /** * Manages the execution of a streaming Spark SQL query that is occurring in a separate thread. * Unlike a standard query, a streaming query executes repeatedly each time new data arrives at any * [[Source]] present in the query plan. Whenever new data arrives, a [[QueryExecution]] is created * and the results are committed transactionally to the given [[Sink]]. * * @param deleteCheckpointOnStop whether to delete the checkpoint if the query is stopped without * errors. Checkpoint deletion can be forced with the appropriate * Spark configuration. */ abstract class StreamExecution( override val sparkSession: SparkSession, override val name: String, val resolvedCheckpointRoot: String, val analyzedPlan: LogicalPlan, val sink: Table, val trigger: Trigger, val triggerClock: Clock, val outputMode: OutputMode, deleteCheckpointOnStop: Boolean) extends StreamingQuery with ProgressReporter with Logging { /** * Starts the execution. This returns only after the thread has started and [[QueryStartedEvent]] * has been posted to all the listeners. */ def start(): Unit = { logInfo(s\"Starting $prettyIdString. Use $resolvedCheckpointRoot to store the query checkpoint.\") queryExecutionThread.setDaemon(true) queryExecutionThread.start() startLatch.await() // Wait until thread started and QueryStart event has been posted } /** * The thread that runs the micro-batches of this stream. Note that this thread must be * [[org.apache.spark.util.UninterruptibleThread]] to workaround KAFKA-1894: interrupting a * running `KafkaConsumer` may cause endless loop. */ val queryExecutionThread: QueryExecutionThread = new QueryExecutionThread(s\"stream execution thread for $prettyIdString\") { override def run(): Unit = { // To fix call site like \"run at <unknown>:0\", we bridge the call site from the caller // thread to this micro batch thread sparkSession.sparkContext.setCallSite(callSite) runStream() } } /** * Activate the stream and then wrap a callout to runActivatedStream, handling start and stop. * * Note that this method ensures that [[QueryStartedEvent]] and [[QueryTerminatedEvent]] are * posted such that listeners are guaranteed to get a start event before a termination. * Furthermore, this method also ensures that [[QueryStartedEvent]] event is posted before the * `start()` method returns. */ private def runStream(): Unit = { try { ... if (state.compareAndSet(INITIALIZING, ACTIVE)) { // Unblock `awaitInitialization` initializationLatch.countDown() runActivatedStream(sparkSessionForStream) ===> implemented by MicroBatchExecution or ContinuousExecution updateStatusMessage(\"Stopped\") } else { // `stop()` is already called. Let `finally` finish the cleanup. } ...... } catch { case e if isInterruptedByStop(e, sparkSession.sparkContext) => // interrupted by stop() updateStatusMessage(\"Stopped\") case e: IOException if e.getMessage != null && e.getMessage.startsWith(classOf[InterruptedException].getName) && state.get == TERMINATED => // This is a workaround for HADOOP-12074: `Shell.runCommand` converts `InterruptedException` // to `new IOException(ie.toString())` before Hadoop 2.8. updateStatusMessage(\"Stopped\") case e: Throwable => val message = if (e.getMessage == null) \"\" else e.getMessage streamDeathCause = new StreamingQueryException( toDebugString(includeLogicalPlan = isInitialized), cause = e, committedOffsets.toOffsetSeq(sources, offsetSeqMetadata).toString, availableOffsets.toOffsetSeq(sources, offsetSeqMetadata).toString, errorClass = \"STREAM_FAILED\", messageParameters = Map( \"id\" -> id.toString, \"runId\" -> runId.toString, \"message\" -> message)) logError(s\"Query $prettyIdString terminated with error\", e) updateStatusMessage(s\"Terminated with exception: $message\") // Rethrow the fatal errors to allow the user using `Thread.UncaughtExceptionHandler` to // handle them if (!NonFatal(e)) { throw e } } finally queryExecutionThread.runUninterruptibly { // The whole `finally` block must run inside `runUninterruptibly` to avoid being interrupted // when a query is stopped by the user. We need to make sure the following codes finish // otherwise it may throw `InterruptedException` to `UncaughtExceptionHandler` (SPARK-21248). // Release latches to unblock the user codes since exception can happen in any place and we // may not get a chance to release them startLatch.countDown() initializationLatch.countDown() try { stopSources() cleanup() state.set(TERMINATED) currentStatus = status.copy(isTriggerActive = false, isDataAvailable = false) // Update metrics and status sparkSession.sparkContext.env.metricsSystem.removeSource(streamMetrics) // Notify others sparkSession.streams.notifyQueryTermination(StreamExecution.this) postEvent( new QueryTerminatedEvent(id, runId, exception.map(_.cause).map(Utils.exceptionString))) // Delete the temp checkpoint when either force delete enabled or the query didn't fail if (deleteCheckpointOnStop && (sparkSession.sessionState.conf .getConf(SQLConf.FORCE_DELETE_TEMP_CHECKPOINT_LOCATION) || exception.isEmpty)) { val checkpointPath = new Path(resolvedCheckpointRoot) try { logInfo(s\"Deleting checkpoint $checkpointPath.\") fileManager.delete(checkpointPath) } catch { case NonFatal(e) => // Deleting temp checkpoint folder is best effort, don't throw non fatal exceptions // when we cannot delete them. logWarning(s\"Cannot delete $checkpointPath\", e) } } } finally { awaitProgressLock.lock() try { // Wake up any threads that are waiting for the stream to progress. awaitProgressLockCondition.signalAll() } finally { awaitProgressLock.unlock() } terminationLatch.countDown() } } }","title":"StreamingQueryManager"},{"location":"SparkStructuredStreaming/MicroBatchExecution/#microbatchexecution","text":"class MicroBatchExecution( sparkSession: SparkSession, trigger: Trigger, triggerClock: Clock, extraOptions: Map[String, String], plan: WriteToStream) extends StreamExecution( sparkSession, plan.name, plan.resolvedCheckpointLocation, plan.inputQuery, plan.sink, trigger, triggerClock, plan.outputMode, plan.deleteCheckpointOnStop) with AsyncLogPurge { /** * Repeatedly attempts to run batches as data arrives. */ protected def runActivatedStream(sparkSessionForStream: SparkSession): Unit = { val noDataBatchesEnabled = sparkSessionForStream.sessionState.conf.streamingNoDataMicroBatchesEnabled triggerExecutor.execute(() => { if (isActive) { // check if there are any previous errors and bubble up any existing async operations errorNotifier.throwErrorIfExists var currentBatchHasNewData = false // Whether the current batch had new data startTrigger() reportTimeTaken(\"triggerExecution\") { // We'll do this initialization only once every start / restart if (currentBatchId < 0) { AcceptsLatestSeenOffsetHandler.setLatestSeenOffsetOnSources( offsetLog.getLatest().map(_._2), sources) populateStartOffsets(sparkSessionForStream) logInfo(s\"Stream started from $committedOffsets\") } // Set this before calling constructNextBatch() so any Spark jobs executed by sources // while getting new data have the correct description sparkSession.sparkContext.setJobDescription(getBatchDescriptionString) // Try to construct the next batch. This will return true only if the next batch is // ready and runnable. Note that the current batch may be runnable even without // new data to process as `constructNextBatch` may decide to run a batch for // state cleanup, etc. `isNewDataAvailable` will be updated to reflect whether new data // is available or not. if (!isCurrentBatchConstructed) { isCurrentBatchConstructed = constructNextBatch(noDataBatchesEnabled) } // Record the trigger offset range for progress reporting *before* processing the batch recordTriggerOffsets( from = committedOffsets, to = availableOffsets, latest = latestOffsets) // Remember whether the current batch has data or not. This will be required later // for bookkeeping after running the batch, when `isNewDataAvailable` will have changed // to false as the batch would have already processed the available data. currentBatchHasNewData = isNewDataAvailable currentStatus = currentStatus.copy(isDataAvailable = isNewDataAvailable) if (isCurrentBatchConstructed) { if (currentBatchHasNewData) updateStatusMessage(\"Processing new data\") else updateStatusMessage(\"No new data but cleaning up state\") runBatch(sparkSessionForStream) } else { updateStatusMessage(\"Waiting for data to arrive\") } } // Must be outside reportTimeTaken so it is recorded finishTrigger(currentBatchHasNewData, isCurrentBatchConstructed) // Signal waiting threads. Note this must be after finishTrigger() to ensure all // activities (progress generation, etc.) have completed before signaling. withProgressLocked { awaitProgressLockCondition.signalAll() } // If the current batch has been executed, then increment the batch id and reset flag. // Otherwise, there was no data to execute the batch and sleep for some time if (isCurrentBatchConstructed) { currentBatchId += 1 isCurrentBatchConstructed = false } else if (triggerExecutor.isInstanceOf[MultiBatchExecutor]) { logInfo(\"Finished processing all available data for the trigger, terminating this \" + \"Trigger.AvailableNow query\") state.set(TERMINATED) } else Thread.sleep(pollingDelayMs) } updateStatusMessage(\"Waiting for next trigger\") isActive }) }","title":"MicroBatchExecution"},{"location":"SparkStructuredStreaming/MicroBatchExecution/#watermark-support","text":"","title":"Watermark Support"},{"location":"SparkStructuredStreaming/MicroBatchExecution/#removekeysolderthanwatermark","text":"org.apache.spark.sql.execution.streaming.WatermarkSupport#removeKeysOlderThanWatermark protected def removeKeysOlderThanWatermark(store: StateStore): Unit = { if (watermarkPredicateForKeysForEviction.nonEmpty) { val numRemovedStateRows = longMetric(\"numRemovedStateRows\") store.iterator().foreach { rowPair => if (watermarkPredicateForKeysForEviction.get.eval(rowPair.key)) { store.remove(rowPair.key) numRemovedStateRows += 1 } } } } protected def removeKeysOlderThanWatermark( storeManager: StreamingAggregationStateManager, store: StateStore): Unit = { if (watermarkPredicateForKeysForEviction.nonEmpty) { val numRemovedStateRows = longMetric(\"numRemovedStateRows\") storeManager.keys(store).foreach { keyRow => if (watermarkPredicateForKeysForEviction.get.eval(keyRow)) { storeManager.remove(store, keyRow) numRemovedStateRows += 1 } } } } org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider.RocksDBStateStore#iterator override def iterator(): Iterator[UnsafeRowPair] = { rocksDB.iterator().map { kv => val rowPair = encoder.decode(kv) if (!isValidated && rowPair.value != null) { StateStoreProvider.validateStateRowFormat( rowPair.key, keySchema, rowPair.value, valueSchema, storeConf) isValidated = true } rowPair } } org.apache.spark.sql.execution.streaming.state.RocksDB#iterator def iterator(): Iterator[ByteArrayPair] = { val iter = writeBatch.newIteratorWithBase(db.newIterator()) logInfo(s\"Getting iterator from version $loadedVersion\") iter.seekToFirst() // Attempt to close this iterator if there is a task failure, or a task interruption. // This is a hack because it assumes that the RocksDB is running inside a task. Option(TaskContext.get()).foreach { tc => tc.addTaskCompletionListener[Unit] { _ => iter.close() } } new NextIterator[ByteArrayPair] { override protected def getNext(): ByteArrayPair = { if (iter.isValid) { byteArrayPair.set(iter.key, iter.value) iter.next() byteArrayPair } else { finished = true iter.close() null } } override protected def close(): Unit = { iter.close() } } }","title":"removeKeysOlderThanWatermark"},{"location":"SparkStructuredStreaming/RocksDBFileManager/","text":"RocksDB StateStore Jira RocksDB StateStore \u00b6 Jira \u00b6 SPARK-34198 Add RocksDB StateStore implementation Design Doc","title":"RocksDBFileManager"},{"location":"SparkStructuredStreaming/RocksDBFileManager/#rocksdb-statestore","text":"","title":"RocksDB StateStore"},{"location":"SparkStructuredStreaming/RocksDBFileManager/#jira","text":"SPARK-34198 Add RocksDB StateStore implementation Design Doc","title":"Jira"}]}