{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6","title":"Introduction"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"SparkCore/ShuffleService/","text":"Shuffle Service BlockTranserService Code Improve Spark shuffle server responsiveness to non-ChunkFetch requests push-based shuffle Use remote storage for persisting shuffle data Shuffle Service \u00b6 BlockTranserService \u00b6 Code \u00b6 /** * The BlockTransferService that used for fetching a set of blocks at time. Each instance of * BlockTransferService contains both client and server inside. */ private[spark] abstract class BlockTransferService extends BlockStoreClient { /** * A BlockTransferService that uses Netty to fetch a set of blocks at time. */ private[spark] class NettyBlockTransferService( conf: SparkConf, securityManager: SecurityManager, bindAddress: String, override val hostName: String, _port: Int, numCores: Int, driverEndPointRef: RpcEndpointRef = null) extends BlockTransferService { override def init(blockDataManager: BlockDataManager): Unit = { val rpcHandler = new NettyBlockRpcServer(conf.getAppId, serializer, blockDataManager) var serverBootstrap: Option[TransportServerBootstrap] = None var clientBootstrap: Option[TransportClientBootstrap] = None this.transportConf = SparkTransportConf.fromSparkConf(conf, \"shuffle\", numCores) if (authEnabled) { serverBootstrap = Some(new AuthServerBootstrap(transportConf, securityManager)) clientBootstrap = Some(new AuthClientBootstrap(transportConf, conf.getAppId, securityManager)) } transportContext = new TransportContext(transportConf, rpcHandler) clientFactory = transportContext.createClientFactory(clientBootstrap.toSeq.asJava) server = createServer(serverBootstrap.toList) appId = conf.getAppId if (hostName.equals(bindAddress)) { logger.info(s\"Server created on $hostName:${server.getPort}\") } else { logger.info(s\"Server created on $hostName $bindAddress:${server.getPort}\") } } Improve Spark shuffle server responsiveness to non-ChunkFetch requests \u00b6 SPARK-24355 Improve Spark shuffle server responsiveness to non-ChunkFetch requests SPARK-30512 Use a dedicated boss event group loop in the netty pipeline for external shuffle service SPARK-30623 Spark external shuffle allow disable of separate event loop group What changes were proposed in this pull request? Fix the regression caused by PR #22173. The original PR changes the logic of handling ChunkFetchReqeust from async to sync, that\u2019s causes the shuffle benchmark regression. This PR fixes the regression back to the async mode by reusing the config spark.shuffle.server.chunkFetchHandlerThreadsPercent . When the user sets the config, ChunkFetchReqeust will be processed in a separate event loop group, otherwise, the code path is exactly the same as before. Performance regression described in [comment](https://github.com/apache/spark/pull/22173#issuecomment-572459561 org.apache.spark.network.server.ChunkFetchRequestHandler#respond /** * The invocation to channel.writeAndFlush is async, and the actual I/O on the * channel will be handled by the EventLoop the channel is registered to. So even * though we are processing the ChunkFetchRequest in a separate thread pool, the actual I/O, * which is the potentially blocking call that could deplete server handler threads, is still * being processed by TransportServer's default EventLoopGroup. * * When syncModeEnabled is true, Spark will throttle the max number of threads that channel I/O * for sending response to ChunkFetchRequest, the thread calling channel.writeAndFlush will wait * for the completion of sending response back to client by invoking await(). This will throttle * the rate at which threads from ChunkFetchRequest dedicated EventLoopGroup submit channel I/O * requests to TransportServer's default EventLoopGroup, thus making sure that we can reserve * some threads in TransportServer's default EventLoopGroup for handling other RPC messages. */ private ChannelFuture respond( final Channel channel, final Encodable result) throws InterruptedException { final SocketAddress remoteAddress = channel.remoteAddress(); ChannelFuture channelFuture; if (syncModeEnabled) { channelFuture = channel.writeAndFlush(result).await(); } else { channelFuture = channel.writeAndFlush(result); } return channelFuture.addListener((ChannelFutureListener) future -> { if (future.isSuccess()) { logger.trace(\"Sent result {} to client {}\", result, remoteAddress); } else { logger.error(String.format(\"Error sending result %s to %s; closing connection\", result, remoteAddress), future.cause()); channel.close(); } }); } Q: Why await() needed? I think await does\u2019t provide any benefit and could be removed. When the chunk fetch event loop runs channel.writeAndFlush(result) This adds a WriteAndFlushTask in the pendingQueue of the default server-IO thread registered with that channel. The code in NioEventLoop.run() itself throttles the number of tasks that can be run at a time from its pending queue. Here is the code: final long ioStartTime = System.nanoTime(); try { processSelectedKeys(); } finally { // Ensure we always run tasks. final long ioTime = System.nanoTime() - ioStartTime; runAllTasks(ioTime * (100 - ioRatio) / ioRatio); } } Here it records how much time it took to perform the IO operations, that is, execute processSelectedKeys(). runAllTasks, which is the method that processes the tasks from pendingQueue, will be performed for the same amount of time. runAllTasks() does process 64 tasks and then checks the time. // Check timeout every 64 tasks because nanoTime() is relatively expensive. // XXX: Hard-coded value - will make it configurable if it is really a problem. if ((runTasks & 0x3F) == 0) { lastExecutionTime = ScheduledFutureTask.nanoTime(); if (lastExecutionTime >= deadline) { break; } } This ensures that the default server-IO thread always gets time to process the ready channels. Its not always busy processing WriteAndFlushTask Answer: I removed the await and tested with our internal stress testing framework. I started seeing SASL requests timing out. In this test, I observed more than 2 minutes delay between channel registration and when the first bytes are read from the channel. 2020-01-24 22:53:34,019 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] REGISTERED 2020-01-24 22:53:34,019 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] ACTIVE 2020-01-24 22:55:05,207 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] READ: 48B 2020-01-24 22:55:05,207 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] WRITE: org.apache.spark.network.protocol.MessageWithHeader@27e59ee9 2020-01-24 22:55:05,207 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] FLUSH 2020-01-24 22:55:05,207 INFO org.apache.spark.network.server.OutgoingChannelHandler: OUTPUT request 5929104419960968526 channel d475f5ff request_rec 1579906505207 transport_rec 1579906505207 flush 1579906505207 receive-transport 0 transport-flush 0 total 0 Since there is a delay in reading the channel, I suspect this is because the hardcoding in netty code SingleThreadEventExecutor.runAllTask() that checks time only after 64 tasks. WriteAndFlush tasks are bulky tasks. With await there will be just 1 WriteAndFlushTask per channel in the IO thread\u2019s pending queue and the rest of the tasks will be smaller tasks. However, without await there are more WriteAndFlush tasks per channel in the IO thread\u2019s queue. Since it processes 64 tasks and then checks time, this time increases with more WriteAndFlush tasks. / Check timeout every 64 tasks because nanoTime() is relatively expensive. // XXX: Hard-coded value - will make it configurable if it is really a problem. if ((runTasks & 0x3F) == 0) { lastExecutionTime = ScheduledFutureTask.nanoTime(); if (lastExecutionTime >= deadline) { break; } } I can test this theory by lowering this number in a fork of netty and building spark against it. However, for now we can\u2019t remove await(). Note: This test was with a dedicated boss event loop group which is why we don\u2019t see any delay in channel registration. push-based shuffle \u00b6 SPARK-30602 SPIP: Support push-based shuffle to improve shuffle efficiency doc: SPIP: Spark Push-Based Shuffle Consolidated reference PR for Push-based shuffle SPARK-32917 Add support for executors to push shuffle blocks after successful map task completion PR SPARK-32917 Use remote storage for persisting shuffle data \u00b6 architecture discussion - Use remote storage for persisting shuffle data SPIP: `SPARK-25299 - An API For Writing Shuffle Data To Remote Storage","title":"ShuffleService"},{"location":"SparkCore/ShuffleService/#shuffle-service","text":"","title":"Shuffle Service"},{"location":"SparkCore/ShuffleService/#blocktranserservice","text":"","title":"BlockTranserService"},{"location":"SparkCore/ShuffleService/#code","text":"/** * The BlockTransferService that used for fetching a set of blocks at time. Each instance of * BlockTransferService contains both client and server inside. */ private[spark] abstract class BlockTransferService extends BlockStoreClient { /** * A BlockTransferService that uses Netty to fetch a set of blocks at time. */ private[spark] class NettyBlockTransferService( conf: SparkConf, securityManager: SecurityManager, bindAddress: String, override val hostName: String, _port: Int, numCores: Int, driverEndPointRef: RpcEndpointRef = null) extends BlockTransferService { override def init(blockDataManager: BlockDataManager): Unit = { val rpcHandler = new NettyBlockRpcServer(conf.getAppId, serializer, blockDataManager) var serverBootstrap: Option[TransportServerBootstrap] = None var clientBootstrap: Option[TransportClientBootstrap] = None this.transportConf = SparkTransportConf.fromSparkConf(conf, \"shuffle\", numCores) if (authEnabled) { serverBootstrap = Some(new AuthServerBootstrap(transportConf, securityManager)) clientBootstrap = Some(new AuthClientBootstrap(transportConf, conf.getAppId, securityManager)) } transportContext = new TransportContext(transportConf, rpcHandler) clientFactory = transportContext.createClientFactory(clientBootstrap.toSeq.asJava) server = createServer(serverBootstrap.toList) appId = conf.getAppId if (hostName.equals(bindAddress)) { logger.info(s\"Server created on $hostName:${server.getPort}\") } else { logger.info(s\"Server created on $hostName $bindAddress:${server.getPort}\") } }","title":"Code"},{"location":"SparkCore/ShuffleService/#improve-spark-shuffle-server-responsiveness-to-non-chunkfetch-requests","text":"SPARK-24355 Improve Spark shuffle server responsiveness to non-ChunkFetch requests SPARK-30512 Use a dedicated boss event group loop in the netty pipeline for external shuffle service SPARK-30623 Spark external shuffle allow disable of separate event loop group What changes were proposed in this pull request? Fix the regression caused by PR #22173. The original PR changes the logic of handling ChunkFetchReqeust from async to sync, that\u2019s causes the shuffle benchmark regression. This PR fixes the regression back to the async mode by reusing the config spark.shuffle.server.chunkFetchHandlerThreadsPercent . When the user sets the config, ChunkFetchReqeust will be processed in a separate event loop group, otherwise, the code path is exactly the same as before. Performance regression described in [comment](https://github.com/apache/spark/pull/22173#issuecomment-572459561 org.apache.spark.network.server.ChunkFetchRequestHandler#respond /** * The invocation to channel.writeAndFlush is async, and the actual I/O on the * channel will be handled by the EventLoop the channel is registered to. So even * though we are processing the ChunkFetchRequest in a separate thread pool, the actual I/O, * which is the potentially blocking call that could deplete server handler threads, is still * being processed by TransportServer's default EventLoopGroup. * * When syncModeEnabled is true, Spark will throttle the max number of threads that channel I/O * for sending response to ChunkFetchRequest, the thread calling channel.writeAndFlush will wait * for the completion of sending response back to client by invoking await(). This will throttle * the rate at which threads from ChunkFetchRequest dedicated EventLoopGroup submit channel I/O * requests to TransportServer's default EventLoopGroup, thus making sure that we can reserve * some threads in TransportServer's default EventLoopGroup for handling other RPC messages. */ private ChannelFuture respond( final Channel channel, final Encodable result) throws InterruptedException { final SocketAddress remoteAddress = channel.remoteAddress(); ChannelFuture channelFuture; if (syncModeEnabled) { channelFuture = channel.writeAndFlush(result).await(); } else { channelFuture = channel.writeAndFlush(result); } return channelFuture.addListener((ChannelFutureListener) future -> { if (future.isSuccess()) { logger.trace(\"Sent result {} to client {}\", result, remoteAddress); } else { logger.error(String.format(\"Error sending result %s to %s; closing connection\", result, remoteAddress), future.cause()); channel.close(); } }); } Q: Why await() needed? I think await does\u2019t provide any benefit and could be removed. When the chunk fetch event loop runs channel.writeAndFlush(result) This adds a WriteAndFlushTask in the pendingQueue of the default server-IO thread registered with that channel. The code in NioEventLoop.run() itself throttles the number of tasks that can be run at a time from its pending queue. Here is the code: final long ioStartTime = System.nanoTime(); try { processSelectedKeys(); } finally { // Ensure we always run tasks. final long ioTime = System.nanoTime() - ioStartTime; runAllTasks(ioTime * (100 - ioRatio) / ioRatio); } } Here it records how much time it took to perform the IO operations, that is, execute processSelectedKeys(). runAllTasks, which is the method that processes the tasks from pendingQueue, will be performed for the same amount of time. runAllTasks() does process 64 tasks and then checks the time. // Check timeout every 64 tasks because nanoTime() is relatively expensive. // XXX: Hard-coded value - will make it configurable if it is really a problem. if ((runTasks & 0x3F) == 0) { lastExecutionTime = ScheduledFutureTask.nanoTime(); if (lastExecutionTime >= deadline) { break; } } This ensures that the default server-IO thread always gets time to process the ready channels. Its not always busy processing WriteAndFlushTask Answer: I removed the await and tested with our internal stress testing framework. I started seeing SASL requests timing out. In this test, I observed more than 2 minutes delay between channel registration and when the first bytes are read from the channel. 2020-01-24 22:53:34,019 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] REGISTERED 2020-01-24 22:53:34,019 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] ACTIVE 2020-01-24 22:55:05,207 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] READ: 48B 2020-01-24 22:55:05,207 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] WRITE: org.apache.spark.network.protocol.MessageWithHeader@27e59ee9 2020-01-24 22:55:05,207 DEBUG org.spark_project.io.netty.handler.logging.LoggingHandler: [id: 0xd475f5ff, L:/10.150.16.27:7337 - R:/10.150.16.44:11388] FLUSH 2020-01-24 22:55:05,207 INFO org.apache.spark.network.server.OutgoingChannelHandler: OUTPUT request 5929104419960968526 channel d475f5ff request_rec 1579906505207 transport_rec 1579906505207 flush 1579906505207 receive-transport 0 transport-flush 0 total 0 Since there is a delay in reading the channel, I suspect this is because the hardcoding in netty code SingleThreadEventExecutor.runAllTask() that checks time only after 64 tasks. WriteAndFlush tasks are bulky tasks. With await there will be just 1 WriteAndFlushTask per channel in the IO thread\u2019s pending queue and the rest of the tasks will be smaller tasks. However, without await there are more WriteAndFlush tasks per channel in the IO thread\u2019s queue. Since it processes 64 tasks and then checks time, this time increases with more WriteAndFlush tasks. / Check timeout every 64 tasks because nanoTime() is relatively expensive. // XXX: Hard-coded value - will make it configurable if it is really a problem. if ((runTasks & 0x3F) == 0) { lastExecutionTime = ScheduledFutureTask.nanoTime(); if (lastExecutionTime >= deadline) { break; } } I can test this theory by lowering this number in a fork of netty and building spark against it. However, for now we can\u2019t remove await(). Note: This test was with a dedicated boss event loop group which is why we don\u2019t see any delay in channel registration.","title":"Improve Spark shuffle server responsiveness to non-ChunkFetch requests"},{"location":"SparkCore/ShuffleService/#push-based-shuffle","text":"SPARK-30602 SPIP: Support push-based shuffle to improve shuffle efficiency doc: SPIP: Spark Push-Based Shuffle Consolidated reference PR for Push-based shuffle SPARK-32917 Add support for executors to push shuffle blocks after successful map task completion PR SPARK-32917","title":"push-based shuffle"},{"location":"SparkCore/ShuffleService/#use-remote-storage-for-persisting-shuffle-data","text":"architecture discussion - Use remote storage for persisting shuffle data SPIP: `SPARK-25299 - An API For Writing Shuffle Data To Remote Storage","title":"Use remote storage for persisting shuffle data"},{"location":"SparkCore/YarnAllocator/","text":"Yarn Allocator Code YarnAllocator Yarn Allocator \u00b6 Code \u00b6 YarnAllocator \u00b6 org.apache.spark.deploy.yarn.YarnAllocator /** * YarnAllocator is charged with requesting containers from the YARN ResourceManager and deciding * what to do with containers when YARN fulfills these requests. * * This class makes use of YARN's AMRMClient APIs. We interact with the AMRMClient in three ways: * * Making our resource needs known, which updates local bookkeeping about containers requested. * * Calling \"allocate\", which syncs our local container requests with the RM, and returns any * containers that YARN has granted to us. This also functions as a heartbeat. * * Processing the containers granted to us to possibly launch executors inside of them. * * The public methods of this class are thread-safe. All methods that mutate state are * synchronized. */ private[yarn] class YarnAllocator( driverUrl: String, driverRef: RpcEndpointRef, conf: YarnConfiguration, sparkConf: SparkConf, amClient: AMRMClient[ContainerRequest], appAttemptId: ApplicationAttemptId, securityMgr: SecurityManager, localResources: Map[String, LocalResource], resolver: SparkRackResolver, clock: Clock = new SystemClock) extends Logging {","title":"YarnAllocator"},{"location":"SparkCore/YarnAllocator/#yarn-allocator","text":"","title":"Yarn Allocator"},{"location":"SparkCore/YarnAllocator/#code","text":"","title":"Code"},{"location":"SparkCore/YarnAllocator/#yarnallocator","text":"org.apache.spark.deploy.yarn.YarnAllocator /** * YarnAllocator is charged with requesting containers from the YARN ResourceManager and deciding * what to do with containers when YARN fulfills these requests. * * This class makes use of YARN's AMRMClient APIs. We interact with the AMRMClient in three ways: * * Making our resource needs known, which updates local bookkeeping about containers requested. * * Calling \"allocate\", which syncs our local container requests with the RM, and returns any * containers that YARN has granted to us. This also functions as a heartbeat. * * Processing the containers granted to us to possibly launch executors inside of them. * * The public methods of this class are thread-safe. All methods that mutate state are * synchronized. */ private[yarn] class YarnAllocator( driverUrl: String, driverRef: RpcEndpointRef, conf: YarnConfiguration, sparkConf: SparkConf, amClient: AMRMClient[ContainerRequest], appAttemptId: ApplicationAttemptId, securityMgr: SecurityManager, localResources: Map[String, LocalResource], resolver: SparkRackResolver, clock: Clock = new SystemClock) extends Logging {","title":"YarnAllocator"},{"location":"SparkSQL/AQE/","text":"Adaptive execution in Spark Jira QueryExecution#preparations InsertAdaptiveSparkPlan AdaptiveSparkPlanExec QueryStageExec reuseQueryStage Adaptive coalesce partitions Adaptive execution in Spark \u00b6 Jira \u00b6 SPARK-31412 Feature requirement (with subtasks list) Design Doc SPARK-23128 The basic framework for the new Adaptive Query Execution SPARK-28177 Adjust post shuffle partition number in adaptive execution SPARK-29544 Optimize skewed join at runtime with new Adaptive Execution SPARK-31865 Fix complex AQE query stage not reused SPARK-35552 Make query stage materialized more readable SPARK-9850 Adaptive execution in Spark (original idea) Design Doc SPARK-9851 Support submitting map stages individually in DAGScheduler QueryExecution#preparations \u00b6 org.apache.spark.sql.execution.QueryExecution#preparations private[execution] def preparations( sparkSession: SparkSession, adaptiveExecutionRule: Option[InsertAdaptiveSparkPlan] = None, subquery: Boolean): Seq[Rule[SparkPlan]] = { // `AdaptiveSparkPlanExec` is a leaf node. If inserted, all the following rules will be no-op // as the original plan is hidden behind `AdaptiveSparkPlanExec`. adaptiveExecutionRule.toSeq ++ InsertAdaptiveSparkPlan \u00b6 org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan /** * This rule wraps the query plan with an [[AdaptiveSparkPlanExec]], which executes the query plan * and re-optimize the plan during execution based on runtime data statistics. * * Note that this rule is stateful and thus should not be reused across query executions. */ case class InsertAdaptiveSparkPlan( adaptiveExecutionContext: AdaptiveExecutionContext) extends Rule[SparkPlan] { override def apply(plan: SparkPlan): SparkPlan = applyInternal(plan, false) private def applyInternal(plan: SparkPlan, isSubquery: Boolean): SparkPlan = plan match { case _ if !conf.adaptiveExecutionEnabled => plan case _: ExecutedCommandExec => plan case _: CommandResultExec => plan case c: DataWritingCommandExec => c.copy(child = apply(c.child)) case c: V2CommandExec => c.withNewChildren(c.children.map(apply)) case _ if shouldApplyAQE(plan, isSubquery) => if (supportAdaptive(plan)) { try { // Plan sub-queries recursively and pass in the shared stage cache for exchange reuse. // Fall back to non-AQE mode if AQE is not supported in any of the sub-queries. val subqueryMap = buildSubqueryMap(plan) val planSubqueriesRule = PlanAdaptiveSubqueries(subqueryMap) val preprocessingRules = Seq( planSubqueriesRule) // Run pre-processing rules. val newPlan = AdaptiveSparkPlanExec.applyPhysicalRules(plan, preprocessingRules) logDebug(s\"Adaptive execution enabled for plan: $plan\") AdaptiveSparkPlanExec(newPlan, adaptiveExecutionContext, preprocessingRules, isSubquery) } catch { case SubqueryAdaptiveNotSupportedException(subquery) => logWarning(s\"${SQLConf.ADAPTIVE_EXECUTION_ENABLED.key} is enabled \" + s\"but is not supported for sub-query: $subquery.\") plan } } else { logDebug(s\"${SQLConf.ADAPTIVE_EXECUTION_ENABLED.key} is enabled \" + s\"but is not supported for query: $plan.\") plan } case _ => plan } // AQE is only useful when the query has exchanges or sub-queries. This method returns true if // one of the following conditions is satisfied: // - The config ADAPTIVE_EXECUTION_FORCE_APPLY is true. // - The input query is from a sub-query. When this happens, it means we've already decided to // apply AQE for the main query and we must continue to do it. // - The query contains exchanges. // - The query may need to add exchanges. It's an overkill to run `EnsureRequirements` here, so // we just check `SparkPlan.requiredChildDistribution` and see if it's possible that the // the query needs to add exchanges later. // - The query contains sub-query. private def shouldApplyAQE(plan: SparkPlan, isSubquery: Boolean): Boolean = { conf.getConf(SQLConf.ADAPTIVE_EXECUTION_FORCE_APPLY) || isSubquery || { plan.exists { case _: Exchange => true case p if !p.requiredChildDistribution.forall(_ == UnspecifiedDistribution) => true case p => p.expressions.exists(_.exists { case _: SubqueryExpression => true case _ => false }) } } } AdaptiveSparkPlanExec \u00b6 org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec /** * A root node to execute the query plan adaptively. It splits the query plan into independent * stages and executes them in order according to their dependencies. The query stage * materializes its output at the end. When one stage completes, the data statistics of the * materialized output will be used to optimize the remainder of the query. * * To create query stages, we traverse the query tree bottom up. When we hit an exchange node, * and if all the child query stages of this exchange node are materialized, we create a new * query stage for this exchange node. The new stage is then materialized asynchronously once it * is created. * * When one query stage finishes materialization, the rest query is re-optimized and planned based * on the latest statistics provided by all materialized stages. Then we traverse the query plan * again and create more stages if possible. After all stages have been materialized, we execute * the rest of the plan. */ case class AdaptiveSparkPlanExec( inputPlan: SparkPlan, @transient context: AdaptiveExecutionContext, @transient preprocessingRules: Seq[Rule[SparkPlan]], @transient isSubquery: Boolean, @transient override val supportsColumnar: Boolean = false) extends LeafExecNode { override def doExecute(): RDD[InternalRow] = { withFinalPlanUpdate(_.execute()) } private def withFinalPlanUpdate[T](fun: SparkPlan => T): T = { val plan = getFinalPhysicalPlan() val result = fun(plan) finalPlanUpdate result } private def getFinalPhysicalPlan(): SparkPlan = lock.synchronized { if (isFinalPlan) return currentPhysicalPlan // In case of this adaptive plan being executed out of `withActive` scoped functions, e.g., // `plan.queryExecution.rdd`, we need to set active session here as new plan nodes can be // created in the middle of the execution. context.session.withActive { val executionId = getExecutionId // Use inputPlan logicalLink here in case some top level physical nodes may be removed // during `initialPlan` var currentLogicalPlan = inputPlan.logicalLink.get var result = createQueryStages(currentPhysicalPlan) val events = new LinkedBlockingQueue[StageMaterializationEvent]() val errors = new mutable.ArrayBuffer[Throwable]() var stagesToReplace = Seq.empty[QueryStageExec] while (!result.allChildStagesMaterialized) { currentPhysicalPlan = result.newPlan if (result.newStages.nonEmpty) { stagesToReplace = result.newStages ++ stagesToReplace executionId.foreach(onUpdatePlan(_, result.newStages.map(_.plan))) // SPARK-33933: we should submit tasks of broadcast stages first, to avoid waiting // for tasks to be scheduled and leading to broadcast timeout. // This partial fix only guarantees the start of materialization for BroadcastQueryStage // is prior to others, but because the submission of collect job for broadcasting is // running in another thread, the issue is not completely resolved. val reorderedNewStages = result.newStages .sortWith { case (_: BroadcastQueryStageExec, _: BroadcastQueryStageExec) => false case (_: BroadcastQueryStageExec, _) => true case _ => false } ==================== stage.materialize() is run as Future async ========================= // Start materialization of all new stages and fail fast if any stages failed eagerly reorderedNewStages.foreach { stage => try { stage.materialize().onComplete { res => if (res.isSuccess) { events.offer(StageSuccess(stage, res.get)) } else { events.offer(StageFailure(stage, res.failed.get)) } }(AdaptiveSparkPlanExec.executionContext) } catch { case e: Throwable => cleanUpAndThrowException(Seq(e), Some(stage.id)) } } ========================================================================================== } // Wait on the next completed stage, which indicates new stats are available and probably // new stages can be created. There might be other stages that finish at around the same // time, so we process those stages too in order to reduce re-planning. val nextMsg = events.take() val rem = new util.ArrayList[StageMaterializationEvent]() events.drainTo(rem) (Seq(nextMsg) ++ rem.asScala).foreach { case StageSuccess(stage, res) => stage.resultOption.set(Some(res)) case StageFailure(stage, ex) => errors.append(ex) } // In case of errors, we cancel all running stages and throw exception. if (errors.nonEmpty) { cleanUpAndThrowException(errors.toSeq, None) } // Try re-optimizing and re-planning. Adopt the new plan if its cost is equal to or less // than that of the current plan; otherwise keep the current physical plan together with // the current logical plan since the physical plan's logical links point to the logical // plan it has originated from. // Meanwhile, we keep a list of the query stages that have been created since last plan // update, which stands for the \"semantic gap\" between the current logical and physical // plans. And each time before re-planning, we replace the corresponding nodes in the // current logical plan with logical query stages to make it semantically in sync with // the current physical plan. Once a new plan is adopted and both logical and physical // plans are updated, we can clear the query stage list because at this point the two plans // are semantically and physically in sync again. val logicalPlan = replaceWithQueryStagesInLogicalPlan(currentLogicalPlan, stagesToReplace) val afterReOptimize = reOptimize(logicalPlan) if (afterReOptimize.isDefined) { val (newPhysicalPlan, newLogicalPlan) = afterReOptimize.get val origCost = costEvaluator.evaluateCost(currentPhysicalPlan) val newCost = costEvaluator.evaluateCost(newPhysicalPlan) if (newCost < origCost || (newCost == origCost && currentPhysicalPlan != newPhysicalPlan)) { logOnLevel(\"Plan changed:\\n\" + sideBySide(currentPhysicalPlan.treeString, newPhysicalPlan.treeString).mkString(\"\\n\")) cleanUpTempTags(newPhysicalPlan) currentPhysicalPlan = newPhysicalPlan currentLogicalPlan = newLogicalPlan stagesToReplace = Seq.empty[QueryStageExec] } } // Now that some stages have finished, we can try creating new stages. result = createQueryStages(currentPhysicalPlan) } // Run the final plan when there's no more unfinished stages. currentPhysicalPlan = applyPhysicalRules( optimizeQueryStage(result.newPlan, isFinalStage = true), postStageCreationRules(supportsColumnar), Some((planChangeLogger, \"AQE Post Stage Creation\"))) isFinalPlan = true executionId.foreach(onUpdatePlan(_, Seq(currentPhysicalPlan))) currentPhysicalPlan } } /** * This method is called recursively to traverse the plan tree bottom-up and create a new query * stage or try reusing an existing stage if the current node is an [[Exchange]] node and all of * its child stages have been materialized. * * With each call, it returns: * 1) The new plan replaced with [[QueryStageExec]] nodes where new stages are created. * 2) Whether the child query stages (if any) of the current node have all been materialized. * 3) A list of the new query stages that have been created. */ private def createQueryStages(plan: SparkPlan): CreateStageResult = plan match { case e: Exchange => // First have a quick check in the `stageCache` without having to traverse down the node. context.stageCache.get(e.canonicalized) match { case Some(existingStage) if conf.exchangeReuseEnabled => val stage = reuseQueryStage(existingStage, e) val isMaterialized = stage.isMaterialized CreateStageResult( newPlan = stage, allChildStagesMaterialized = isMaterialized, newStages = if (isMaterialized) Seq.empty else Seq(stage)) case _ => val result = createQueryStages(e.child) val newPlan = e.withNewChildren(Seq(result.newPlan)).asInstanceOf[Exchange] // Create a query stage only when all the child query stages are ready. if (result.allChildStagesMaterialized) { var newStage = newQueryStage(newPlan) if (conf.exchangeReuseEnabled) { // Check the `stageCache` again for reuse. If a match is found, ditch the new stage // and reuse the existing stage found in the `stageCache`, otherwise update the // `stageCache` with the new stage. val queryStage = context.stageCache.getOrElseUpdate( newStage.plan.canonicalized, newStage) if (queryStage.ne(newStage)) { newStage = reuseQueryStage(queryStage, e) } } val isMaterialized = newStage.isMaterialized CreateStageResult( newPlan = newStage, allChildStagesMaterialized = isMaterialized, newStages = if (isMaterialized) Seq.empty else Seq(newStage)) } else { CreateStageResult(newPlan = newPlan, allChildStagesMaterialized = false, newStages = result.newStages) } } case q: QueryStageExec => CreateStageResult(newPlan = q, allChildStagesMaterialized = q.isMaterialized, newStages = Seq.empty) case _ => if (plan.children.isEmpty) { CreateStageResult(newPlan = plan, allChildStagesMaterialized = true, newStages = Seq.empty) } else { val results = plan.children.map(createQueryStages) CreateStageResult( newPlan = plan.withNewChildren(results.map(_.newPlan)), allChildStagesMaterialized = results.forall(_.allChildStagesMaterialized), newStages = results.flatMap(_.newStages)) } } private def newQueryStage(e: Exchange): QueryStageExec = { val optimizedPlan = optimizeQueryStage(e.child, isFinalStage = false) val queryStage = e match { case s: ShuffleExchangeLike => val newShuffle = applyPhysicalRules( s.withNewChildren(Seq(optimizedPlan)), postStageCreationRules(outputsColumnar = s.supportsColumnar), Some((planChangeLogger, \"AQE Post Stage Creation\"))) if (!newShuffle.isInstanceOf[ShuffleExchangeLike]) { throw new IllegalStateException( \"Custom columnar rules cannot transform shuffle node to something else.\") } ShuffleQueryStageExec(currentStageId, newShuffle, s.canonicalized) case b: BroadcastExchangeLike => val newBroadcast = applyPhysicalRules( b.withNewChildren(Seq(optimizedPlan)), postStageCreationRules(outputsColumnar = b.supportsColumnar), Some((planChangeLogger, \"AQE Post Stage Creation\"))) if (!newBroadcast.isInstanceOf[BroadcastExchangeLike]) { throw new IllegalStateException( \"Custom columnar rules cannot transform broadcast node to something else.\") } BroadcastQueryStageExec(currentStageId, newBroadcast, b.canonicalized) } currentStageId += 1 setLogicalLinkForNewQueryStage(queryStage, e) queryStage } rules @transient private val costEvaluator = conf.getConf(SQLConf.ADAPTIVE_CUSTOM_COST_EVALUATOR_CLASS) match { case Some(className) => CostEvaluator.instantiate(className, session.sparkContext.getConf) case _ => SimpleCostEvaluator(conf.getConf(SQLConf.ADAPTIVE_FORCE_OPTIMIZE_SKEWED_JOIN)) } // A list of physical plan rules to be applied before creation of query stages. The physical // plan should reach a final status of query stages (i.e., no more addition or removal of // Exchange nodes) after running these rules. @transient private val queryStagePreparationRules: Seq[Rule[SparkPlan]] = { // For cases like `df.repartition(a, b).select(c)`, there is no distribution requirement for // the final plan, but we do need to respect the user-specified repartition. Here we ask // `EnsureRequirements` to not optimize out the user-specified repartition-by-col to work // around this case. val ensureRequirements = EnsureRequirements(requiredDistribution.isDefined, requiredDistribution) Seq( RemoveRedundantProjects, ensureRequirements, ValidateSparkPlan, ReplaceHashWithSortAgg, RemoveRedundantSorts, DisableUnnecessaryBucketedScan, OptimizeSkewedJoin(ensureRequirements) ) ++ context.session.sessionState.adaptiveRulesHolder.queryStagePrepRules } // A list of physical optimizer rules to be applied to a new stage before its execution. These // optimizations should be stage-independent. @transient private val queryStageOptimizerRules: Seq[Rule[SparkPlan]] = Seq( PlanAdaptiveDynamicPruningFilters(this), ReuseAdaptiveSubquery(context.subqueryCache), OptimizeSkewInRebalancePartitions, CoalesceShufflePartitions(context.session), // `OptimizeShuffleWithLocalRead` needs to make use of 'AQEShuffleReadExec.partitionSpecs' // added by `CoalesceShufflePartitions`, and must be executed after it. OptimizeShuffleWithLocalRead ) // This rule is stateful as it maintains the codegen stage ID. We can't create a fresh one every // time and need to keep it in a variable. @transient private val collapseCodegenStagesRule: Rule[SparkPlan] = CollapseCodegenStages() // A list of physical optimizer rules to be applied right after a new stage is created. The input // plan to these rules has exchange as its root node. private def postStageCreationRules(outputsColumnar: Boolean) = Seq( ApplyColumnarRulesAndInsertTransitions( context.session.sessionState.columnarRules, outputsColumnar), collapseCodegenStagesRule ) @transient val initialPlan = context.session.withActive { applyPhysicalRules( inputPlan, queryStagePreparationRules, Some((planChangeLogger, \"AQE Preparations\"))) } @volatile private var currentPhysicalPlan = initialPlan // The logical plan optimizer for re-optimizing the current logical plan. @transient private val optimizer = new AQEOptimizer(conf, session.sessionState.adaptiveRulesHolder.runtimeOptimizerRules) private def optimizeQueryStage(plan: SparkPlan, isFinalStage: Boolean): SparkPlan = { val optimized = queryStageOptimizerRules.foldLeft(plan) { case (latestPlan, rule) => val applied = rule.apply(latestPlan) val result = rule match { case _: AQEShuffleReadRule if !applied.fastEquals(latestPlan) => val distribution = if (isFinalStage) { // If `requiredDistribution` is None, it means `EnsureRequirements` will not optimize // out the user-specified repartition, thus we don't have a distribution requirement // for the final plan. requiredDistribution.getOrElse(UnspecifiedDistribution) } else { UnspecifiedDistribution } if (ValidateRequirements.validate(applied, distribution)) { applied } else { logDebug(s\"Rule ${rule.ruleName} is not applied as it breaks the \" + \"distribution requirement of the query plan.\") latestPlan } case _ => applied } planChangeLogger.logRule(rule.ruleName, latestPlan, result) result } planChangeLogger.logBatch(\"AQE Query Stage Optimization\", plan, optimized) optimized } /** * Re-optimize and run physical planning on the current logical plan based on the latest stats. */ private def reOptimize(logicalPlan: LogicalPlan): Option[(SparkPlan, LogicalPlan)] = { try { logicalPlan.invalidateStatsCache() val optimized = optimizer.execute(logicalPlan) val sparkPlan = context.session.sessionState.planner.plan(ReturnAnswer(optimized)).next() val newPlan = applyPhysicalRules( sparkPlan, preprocessingRules ++ queryStagePreparationRules, Some((planChangeLogger, \"AQE Replanning\"))) // When both enabling AQE and DPP, `PlanAdaptiveDynamicPruningFilters` rule will // add the `BroadcastExchangeExec` node manually in the DPP subquery, // not through `EnsureRequirements` rule. Therefore, when the DPP subquery is complicated // and need to be re-optimized, AQE also need to manually insert the `BroadcastExchangeExec` // node to prevent the loss of the `BroadcastExchangeExec` node in DPP subquery. // Here, we also need to avoid to insert the `BroadcastExchangeExec` node when the newPlan is // already the `BroadcastExchangeExec` plan after apply the `LogicalQueryStageStrategy` rule. val finalPlan = inputPlan match { case b: BroadcastExchangeLike if (!newPlan.isInstanceOf[BroadcastExchangeLike]) => b.withNewChildren(Seq(newPlan)) case _ => newPlan } Some((finalPlan, optimized)) } catch { case e: InvalidAQEPlanException[_] => logOnLevel(s\"Re-optimize - ${e.getMessage()}:\\n${e.plan}\") None } } QueryStageExec \u00b6 org.apache.spark.sql.execution.adaptive.QueryStageExec /** * A query stage is an independent subgraph of the query plan. Query stage materializes its output * before proceeding with further operators of the query plan. The data statistics of the * materialized output can be used to optimize subsequent query stages. * * There are 2 kinds of query stages: * 1. Shuffle query stage. This stage materializes its output to shuffle files, and Spark launches * another job to execute the further operators. * 2. Broadcast query stage. This stage materializes its output to an array in driver JVM. Spark * broadcasts the array before executing the further operators. */ abstract class QueryStageExec extends LeafExecNode { @transient @volatile protected var _resultOption = new AtomicReference[Option[Any]](None) private[adaptive] def resultOption: AtomicReference[Option[Any]] = _resultOption def isMaterialized: Boolean = resultOption.get().isDefined /** * Compute the statistics of the query stage if executed, otherwise None. */ def computeStats(): Option[Statistics] = if (isMaterialized) { val runtimeStats = getRuntimeStatistics val dataSize = runtimeStats.sizeInBytes.max(0) val numOutputRows = runtimeStats.rowCount.map(_.max(0)) Some(Statistics(dataSize, numOutputRows, isRuntime = true)) } else { None } /** * Materialize this query stage, to prepare for the execution, like submitting map stages, * broadcasting data, etc. The caller side can use the returned [[Future]] to wait until this * stage is ready. */ def doMaterialize(): Future[Any] ====== materialize() is called by org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#getFinalPhysicalPlan === /** * Materialize this query stage, to prepare for the execution, like submitting map stages, * broadcasting data, etc. The caller side can use the returned [[Future]] to wait until this * stage is ready. */ final def materialize(): Future[Any] = { logDebug(s\"Materialize query stage ${this.getClass.getSimpleName}: $id\") doMaterialize() } ========================================================================================================= /** * A shuffle query stage whose child is a [[ShuffleExchangeLike]] or [[ReusedExchangeExec]]. * * @param id the query stage id. * @param plan the underlying plan. * @param _canonicalized the canonicalized plan before applying query stage optimizer rules. */ case class ShuffleQueryStageExec( override val id: Int, override val plan: SparkPlan, override val _canonicalized: SparkPlan) extends QueryStageExec { @transient val shuffle = plan match { case s: ShuffleExchangeLike => s case ReusedExchangeExec(_, s: ShuffleExchangeLike) => s case _ => throw new IllegalStateException(s\"wrong plan for shuffle stage:\\n ${plan.treeString}\") } @transient private lazy val shuffleFuture = shuffle.submitShuffleJob override def doMaterialize(): Future[Any] = shuffleFuture override def getRuntimeStatistics: Statistics = shuffle.runtimeStatistics /** * A broadcast query stage whose child is a [[BroadcastExchangeLike]] or [[ReusedExchangeExec]]. * * @param id the query stage id. * @param plan the underlying plan. * @param _canonicalized the canonicalized plan before applying query stage optimizer rules. */ case class BroadcastQueryStageExec( override val id: Int, override val plan: SparkPlan, override val _canonicalized: SparkPlan) extends QueryStageExec { @transient val broadcast = plan match { case b: BroadcastExchangeLike => b case ReusedExchangeExec(_, b: BroadcastExchangeLike) => b case _ => throw new IllegalStateException(s\"wrong plan for broadcast stage:\\n ${plan.treeString}\") } override def doMaterialize(): Future[Any] = { broadcast.submitBroadcastJob } override def getRuntimeStatistics: Statistics = broadcast.runtimeStatistics reuseQueryStage \u00b6 org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#createQueryStages => reuseQueryStage org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#reuseQueryStage private def reuseQueryStage(existing: QueryStageExec, exchange: Exchange): QueryStageExec = { val queryStage = existing.newReuseInstance(currentStageId, exchange.output) currentStageId += 1 setLogicalLinkForNewQueryStage(queryStage, exchange) queryStage } org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec#newReuseInstance override def newReuseInstance(newStageId: Int, newOutput: Seq[Attribute]): QueryStageExec = { val reuse = BroadcastQueryStageExec( newStageId, ReusedExchangeExec(newOutput, broadcast), _canonicalized) reuse._resultOption = this._resultOption reuse } org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec#newReuseInstance override def newReuseInstance(newStageId: Int, newOutput: Seq[Attribute]): QueryStageExec = { val reuse = ShuffleQueryStageExec( newStageId, ReusedExchangeExec(newOutput, shuffle), _canonicalized) reuse._resultOption = this._resultOption reuse } Adaptive coalesce partitions \u00b6 SQLConf val COALESCE_PARTITIONS_ENABLED = buildConf(\"spark.sql.adaptive.coalescePartitions.enabled\") .doc(s\"When true and '${ADAPTIVE_EXECUTION_ENABLED.key}' is true, Spark will coalesce \" + \"contiguous shuffle partitions according to the target size (specified by \" + s\"'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}'), to avoid too many small tasks.\") .version(\"3.0.0\") .booleanConf .createWithDefault(true) val COALESCE_PARTITIONS_PARALLELISM_FIRST = buildConf(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\") .doc(\"When true, Spark does not respect the target size specified by \" + s\"'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}' (default 64MB) when coalescing contiguous \" + \"shuffle partitions, but adaptively calculate the target size according to the default \" + \"parallelism of the Spark cluster. The calculated size is usually smaller than the \" + \"configured target size. This is to maximize the parallelism and avoid performance \" + \"regression when enabling adaptive query execution. It's recommended to set this config \" + \"to false and respect the configured target size.\") .version(\"3.2.0\") .booleanConf .createWithDefault(true) val COALESCE_PARTITIONS_MIN_PARTITION_SIZE = buildConf(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\") .doc(\"The minimum size of shuffle partitions after coalescing. This is useful when the \" + \"adaptively calculated target size is too small during partition coalescing.\") .version(\"3.2.0\") .bytesConf(ByteUnit.BYTE) .checkValue(_ > 0, \"minPartitionSize must be positive\") .createWithDefaultString(\"1MB\") val COALESCE_PARTITIONS_MIN_PARTITION_NUM = buildConf(\"spark.sql.adaptive.coalescePartitions.minPartitionNum\") .internal() .doc(\"(deprecated) The suggested (not guaranteed) minimum number of shuffle partitions \" + \"after coalescing. If not set, the default value is the default parallelism of the \" + \"Spark cluster. This configuration only has an effect when \" + s\"'${ADAPTIVE_EXECUTION_ENABLED.key}' and \" + s\"'${COALESCE_PARTITIONS_ENABLED.key}' are both true.\") .version(\"3.0.0\") .intConf .checkValue(_ > 0, \"The minimum number of partitions must be positive.\") .createOptional val COALESCE_PARTITIONS_INITIAL_PARTITION_NUM = buildConf(\"spark.sql.adaptive.coalescePartitions.initialPartitionNum\") .doc(\"The initial number of shuffle partitions before coalescing. If not set, it equals to \" + s\"${SHUFFLE_PARTITIONS.key}. This configuration only has an effect when \" + s\"'${ADAPTIVE_EXECUTION_ENABLED.key}' and '${COALESCE_PARTITIONS_ENABLED.key}' \" + \"are both true.\") .version(\"3.0.0\") .intConf .checkValue(_ > 0, \"The initial number of partitions must be positive.\") .createOptional org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#queryStageOptimizerRules // A list of physical optimizer rules to be applied to a new stage before its execution. These // optimizations should be stage-independent. @transient private val queryStageOptimizerRules: Seq[Rule[SparkPlan]] = Seq( PlanAdaptiveDynamicPruningFilters(this), ReuseAdaptiveSubquery(context.subqueryCache), OptimizeSkewInRebalancePartitions, ====== rule for coalesce partitions ========== CoalesceShufflePartitions(context.session), ============================================== // `OptimizeShuffleWithLocalRead` needs to make use of 'AQEShuffleReadExec.partitionSpecs' // added by `CoalesceShufflePartitions`, and must be executed after it. OptimizeShuffleWithLocalRead ) org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions /** * A rule to coalesce the shuffle partitions based on the map output statistics, which can * avoid many small reduce tasks that hurt performance. */ case class CoalesceShufflePartitions(session: SparkSession) extends AQEShuffleReadRule { override def apply(plan: SparkPlan): SparkPlan = { if (!conf.coalesceShufflePartitionsEnabled) { return plan } // Ideally, this rule should simply coalesce partitions w.r.t. the target size specified by // ADVISORY_PARTITION_SIZE_IN_BYTES (default 64MB). To avoid perf regression in AQE, this // rule by default tries to maximize the parallelism and set the target size to // `total shuffle size / Spark default parallelism`. In case the `Spark default parallelism` // is too big, this rule also respect the minimum partition size specified by // COALESCE_PARTITIONS_MIN_PARTITION_SIZE (default 1MB). // For history reason, this rule also need to support the config // COALESCE_PARTITIONS_MIN_PARTITION_NUM. We should remove this config in the future. val minNumPartitions = conf.getConf(SQLConf.COALESCE_PARTITIONS_MIN_PARTITION_NUM).getOrElse { if (conf.getConf(SQLConf.COALESCE_PARTITIONS_PARALLELISM_FIRST)) { // We fall back to Spark default parallelism if the minimum number of coalesced partitions // is not set, so to avoid perf regressions compared to no coalescing. session.sparkContext.defaultParallelism } else { // If we don't need to maximize the parallelism, we set `minPartitionNum` to 1, so that // the specified advisory partition size will be respected. 1 } } val advisoryTargetSize = conf.getConf(SQLConf.ADVISORY_PARTITION_SIZE_IN_BYTES) val minPartitionSize = if (Utils.isTesting) { // In the tests, we usually set the target size to a very small value that is even smaller // than the default value of the min partition size. Here we also adjust the min partition // size to be not larger than 20% of the target size, so that the tests don't need to set // both configs all the time to check the coalescing behavior. conf.getConf(SQLConf.COALESCE_PARTITIONS_MIN_PARTITION_SIZE).min(advisoryTargetSize / 5) } else { conf.getConf(SQLConf.COALESCE_PARTITIONS_MIN_PARTITION_SIZE) } // Sub-plans under the Union operator can be coalesced independently, so we can divide them // into independent \"coalesce groups\", and all shuffle stages within each group have to be // coalesced together. val coalesceGroups = collectCoalesceGroups(plan) // Divide minimum task parallelism among coalesce groups according to their data sizes. val minNumPartitionsByGroup = if (coalesceGroups.length == 1) { Seq(math.max(minNumPartitions, 1)) } else { val sizes = coalesceGroups.map(_.flatMap(_.shuffleStage.mapStats.map(_.bytesByPartitionId.sum)).sum) val totalSize = sizes.sum sizes.map { size => val num = if (totalSize > 0) { math.round(minNumPartitions * 1.0 * size / totalSize) } else { minNumPartitions } math.max(num.toInt, 1) } } val specsMap = mutable.HashMap.empty[Int, Seq[ShufflePartitionSpec]] // Coalesce partitions for each coalesce group independently. coalesceGroups.zip(minNumPartitionsByGroup).foreach { case (shuffleStages, minNumPartitions) => val newPartitionSpecs = ShufflePartitionsUtil.coalescePartitions( shuffleStages.map(_.shuffleStage.mapStats), shuffleStages.map(_.partitionSpecs), advisoryTargetSize = advisoryTargetSize, minNumPartitions = minNumPartitions, minPartitionSize = minPartitionSize) if (newPartitionSpecs.nonEmpty) { shuffleStages.zip(newPartitionSpecs).map { case (stageInfo, partSpecs) => specsMap.put(stageInfo.shuffleStage.id, partSpecs) } } } if (specsMap.nonEmpty) { updateShuffleReads(plan, specsMap.toMap) } else { plan } } private def updateShuffleReads( plan: SparkPlan, specsMap: Map[Int, Seq[ShufflePartitionSpec]]): SparkPlan = plan match { // Even for shuffle exchange whose input RDD has 0 partition, we should still update its // `partitionStartIndices`, so that all the leaf shuffles in a stage have the same // number of output partitions. case ShuffleStageInfo(stage, _) => specsMap.get(stage.id).map { specs => AQEShuffleReadExec(stage, specs) }.getOrElse(plan) case other => other.mapChildren(updateShuffleReads(_, specsMap)) } org.apache.spark.sql.execution.adaptive.AQEShuffleReadRule /** * A rule that may create [[AQEShuffleReadExec]] on top of [[ShuffleQueryStageExec]] and change the * plan output partitioning. The AQE framework will skip the rule if it leads to extra shuffles. */ trait AQEShuffleReadRule extends Rule[SparkPlan] { /** * Returns the list of [[ShuffleOrigin]]s supported by this rule. */ protected def supportedShuffleOrigins: Seq[ShuffleOrigin] protected def isSupported(shuffle: ShuffleExchangeLike): Boolean = { supportedShuffleOrigins.contains(shuffle.shuffleOrigin) } } org.apache.spark.sql.execution.adaptive.AQEShuffleReadExec /** * A wrapper of shuffle query stage, which follows the given partition arrangement. * * @param child It is usually `ShuffleQueryStageExec`, but can be the shuffle exchange * node during canonicalization. * @param partitionSpecs The partition specs that defines the arrangement, requires at least one * partition. */ case class AQEShuffleReadExec private( child: SparkPlan, partitionSpecs: Seq[ShufflePartitionSpec]) extends UnaryExecNode { private def shuffleStage = child match { case stage: ShuffleQueryStageExec => Some(stage) case _ => None } private lazy val shuffleRDD: RDD[_] = { shuffleStage match { case Some(stage) => sendDriverMetrics() stage.shuffle.getShuffleRDD(partitionSpecs.toArray) case _ => throw new IllegalStateException(\"operating on canonicalized plan\") } } override protected def doExecute(): RDD[InternalRow] = { shuffleRDD.asInstanceOf[RDD[InternalRow]] } org.apache.spark.sql.execution.exchange.ShuffleExchangeExec#getShuffleRDD override def getShuffleRDD(partitionSpecs: Array[ShufflePartitionSpec]): RDD[InternalRow] = { new ShuffledRowRDD(shuffleDependency, readMetrics, partitionSpecs) }","title":"AQE"},{"location":"SparkSQL/AQE/#adaptive-execution-in-spark","text":"","title":"Adaptive execution in Spark"},{"location":"SparkSQL/AQE/#jira","text":"SPARK-31412 Feature requirement (with subtasks list) Design Doc SPARK-23128 The basic framework for the new Adaptive Query Execution SPARK-28177 Adjust post shuffle partition number in adaptive execution SPARK-29544 Optimize skewed join at runtime with new Adaptive Execution SPARK-31865 Fix complex AQE query stage not reused SPARK-35552 Make query stage materialized more readable SPARK-9850 Adaptive execution in Spark (original idea) Design Doc SPARK-9851 Support submitting map stages individually in DAGScheduler","title":"Jira"},{"location":"SparkSQL/AQE/#queryexecutionpreparations","text":"org.apache.spark.sql.execution.QueryExecution#preparations private[execution] def preparations( sparkSession: SparkSession, adaptiveExecutionRule: Option[InsertAdaptiveSparkPlan] = None, subquery: Boolean): Seq[Rule[SparkPlan]] = { // `AdaptiveSparkPlanExec` is a leaf node. If inserted, all the following rules will be no-op // as the original plan is hidden behind `AdaptiveSparkPlanExec`. adaptiveExecutionRule.toSeq ++","title":"QueryExecution#preparations"},{"location":"SparkSQL/AQE/#insertadaptivesparkplan","text":"org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan /** * This rule wraps the query plan with an [[AdaptiveSparkPlanExec]], which executes the query plan * and re-optimize the plan during execution based on runtime data statistics. * * Note that this rule is stateful and thus should not be reused across query executions. */ case class InsertAdaptiveSparkPlan( adaptiveExecutionContext: AdaptiveExecutionContext) extends Rule[SparkPlan] { override def apply(plan: SparkPlan): SparkPlan = applyInternal(plan, false) private def applyInternal(plan: SparkPlan, isSubquery: Boolean): SparkPlan = plan match { case _ if !conf.adaptiveExecutionEnabled => plan case _: ExecutedCommandExec => plan case _: CommandResultExec => plan case c: DataWritingCommandExec => c.copy(child = apply(c.child)) case c: V2CommandExec => c.withNewChildren(c.children.map(apply)) case _ if shouldApplyAQE(plan, isSubquery) => if (supportAdaptive(plan)) { try { // Plan sub-queries recursively and pass in the shared stage cache for exchange reuse. // Fall back to non-AQE mode if AQE is not supported in any of the sub-queries. val subqueryMap = buildSubqueryMap(plan) val planSubqueriesRule = PlanAdaptiveSubqueries(subqueryMap) val preprocessingRules = Seq( planSubqueriesRule) // Run pre-processing rules. val newPlan = AdaptiveSparkPlanExec.applyPhysicalRules(plan, preprocessingRules) logDebug(s\"Adaptive execution enabled for plan: $plan\") AdaptiveSparkPlanExec(newPlan, adaptiveExecutionContext, preprocessingRules, isSubquery) } catch { case SubqueryAdaptiveNotSupportedException(subquery) => logWarning(s\"${SQLConf.ADAPTIVE_EXECUTION_ENABLED.key} is enabled \" + s\"but is not supported for sub-query: $subquery.\") plan } } else { logDebug(s\"${SQLConf.ADAPTIVE_EXECUTION_ENABLED.key} is enabled \" + s\"but is not supported for query: $plan.\") plan } case _ => plan } // AQE is only useful when the query has exchanges or sub-queries. This method returns true if // one of the following conditions is satisfied: // - The config ADAPTIVE_EXECUTION_FORCE_APPLY is true. // - The input query is from a sub-query. When this happens, it means we've already decided to // apply AQE for the main query and we must continue to do it. // - The query contains exchanges. // - The query may need to add exchanges. It's an overkill to run `EnsureRequirements` here, so // we just check `SparkPlan.requiredChildDistribution` and see if it's possible that the // the query needs to add exchanges later. // - The query contains sub-query. private def shouldApplyAQE(plan: SparkPlan, isSubquery: Boolean): Boolean = { conf.getConf(SQLConf.ADAPTIVE_EXECUTION_FORCE_APPLY) || isSubquery || { plan.exists { case _: Exchange => true case p if !p.requiredChildDistribution.forall(_ == UnspecifiedDistribution) => true case p => p.expressions.exists(_.exists { case _: SubqueryExpression => true case _ => false }) } } }","title":"InsertAdaptiveSparkPlan"},{"location":"SparkSQL/AQE/#adaptivesparkplanexec","text":"org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec /** * A root node to execute the query plan adaptively. It splits the query plan into independent * stages and executes them in order according to their dependencies. The query stage * materializes its output at the end. When one stage completes, the data statistics of the * materialized output will be used to optimize the remainder of the query. * * To create query stages, we traverse the query tree bottom up. When we hit an exchange node, * and if all the child query stages of this exchange node are materialized, we create a new * query stage for this exchange node. The new stage is then materialized asynchronously once it * is created. * * When one query stage finishes materialization, the rest query is re-optimized and planned based * on the latest statistics provided by all materialized stages. Then we traverse the query plan * again and create more stages if possible. After all stages have been materialized, we execute * the rest of the plan. */ case class AdaptiveSparkPlanExec( inputPlan: SparkPlan, @transient context: AdaptiveExecutionContext, @transient preprocessingRules: Seq[Rule[SparkPlan]], @transient isSubquery: Boolean, @transient override val supportsColumnar: Boolean = false) extends LeafExecNode { override def doExecute(): RDD[InternalRow] = { withFinalPlanUpdate(_.execute()) } private def withFinalPlanUpdate[T](fun: SparkPlan => T): T = { val plan = getFinalPhysicalPlan() val result = fun(plan) finalPlanUpdate result } private def getFinalPhysicalPlan(): SparkPlan = lock.synchronized { if (isFinalPlan) return currentPhysicalPlan // In case of this adaptive plan being executed out of `withActive` scoped functions, e.g., // `plan.queryExecution.rdd`, we need to set active session here as new plan nodes can be // created in the middle of the execution. context.session.withActive { val executionId = getExecutionId // Use inputPlan logicalLink here in case some top level physical nodes may be removed // during `initialPlan` var currentLogicalPlan = inputPlan.logicalLink.get var result = createQueryStages(currentPhysicalPlan) val events = new LinkedBlockingQueue[StageMaterializationEvent]() val errors = new mutable.ArrayBuffer[Throwable]() var stagesToReplace = Seq.empty[QueryStageExec] while (!result.allChildStagesMaterialized) { currentPhysicalPlan = result.newPlan if (result.newStages.nonEmpty) { stagesToReplace = result.newStages ++ stagesToReplace executionId.foreach(onUpdatePlan(_, result.newStages.map(_.plan))) // SPARK-33933: we should submit tasks of broadcast stages first, to avoid waiting // for tasks to be scheduled and leading to broadcast timeout. // This partial fix only guarantees the start of materialization for BroadcastQueryStage // is prior to others, but because the submission of collect job for broadcasting is // running in another thread, the issue is not completely resolved. val reorderedNewStages = result.newStages .sortWith { case (_: BroadcastQueryStageExec, _: BroadcastQueryStageExec) => false case (_: BroadcastQueryStageExec, _) => true case _ => false } ==================== stage.materialize() is run as Future async ========================= // Start materialization of all new stages and fail fast if any stages failed eagerly reorderedNewStages.foreach { stage => try { stage.materialize().onComplete { res => if (res.isSuccess) { events.offer(StageSuccess(stage, res.get)) } else { events.offer(StageFailure(stage, res.failed.get)) } }(AdaptiveSparkPlanExec.executionContext) } catch { case e: Throwable => cleanUpAndThrowException(Seq(e), Some(stage.id)) } } ========================================================================================== } // Wait on the next completed stage, which indicates new stats are available and probably // new stages can be created. There might be other stages that finish at around the same // time, so we process those stages too in order to reduce re-planning. val nextMsg = events.take() val rem = new util.ArrayList[StageMaterializationEvent]() events.drainTo(rem) (Seq(nextMsg) ++ rem.asScala).foreach { case StageSuccess(stage, res) => stage.resultOption.set(Some(res)) case StageFailure(stage, ex) => errors.append(ex) } // In case of errors, we cancel all running stages and throw exception. if (errors.nonEmpty) { cleanUpAndThrowException(errors.toSeq, None) } // Try re-optimizing and re-planning. Adopt the new plan if its cost is equal to or less // than that of the current plan; otherwise keep the current physical plan together with // the current logical plan since the physical plan's logical links point to the logical // plan it has originated from. // Meanwhile, we keep a list of the query stages that have been created since last plan // update, which stands for the \"semantic gap\" between the current logical and physical // plans. And each time before re-planning, we replace the corresponding nodes in the // current logical plan with logical query stages to make it semantically in sync with // the current physical plan. Once a new plan is adopted and both logical and physical // plans are updated, we can clear the query stage list because at this point the two plans // are semantically and physically in sync again. val logicalPlan = replaceWithQueryStagesInLogicalPlan(currentLogicalPlan, stagesToReplace) val afterReOptimize = reOptimize(logicalPlan) if (afterReOptimize.isDefined) { val (newPhysicalPlan, newLogicalPlan) = afterReOptimize.get val origCost = costEvaluator.evaluateCost(currentPhysicalPlan) val newCost = costEvaluator.evaluateCost(newPhysicalPlan) if (newCost < origCost || (newCost == origCost && currentPhysicalPlan != newPhysicalPlan)) { logOnLevel(\"Plan changed:\\n\" + sideBySide(currentPhysicalPlan.treeString, newPhysicalPlan.treeString).mkString(\"\\n\")) cleanUpTempTags(newPhysicalPlan) currentPhysicalPlan = newPhysicalPlan currentLogicalPlan = newLogicalPlan stagesToReplace = Seq.empty[QueryStageExec] } } // Now that some stages have finished, we can try creating new stages. result = createQueryStages(currentPhysicalPlan) } // Run the final plan when there's no more unfinished stages. currentPhysicalPlan = applyPhysicalRules( optimizeQueryStage(result.newPlan, isFinalStage = true), postStageCreationRules(supportsColumnar), Some((planChangeLogger, \"AQE Post Stage Creation\"))) isFinalPlan = true executionId.foreach(onUpdatePlan(_, Seq(currentPhysicalPlan))) currentPhysicalPlan } } /** * This method is called recursively to traverse the plan tree bottom-up and create a new query * stage or try reusing an existing stage if the current node is an [[Exchange]] node and all of * its child stages have been materialized. * * With each call, it returns: * 1) The new plan replaced with [[QueryStageExec]] nodes where new stages are created. * 2) Whether the child query stages (if any) of the current node have all been materialized. * 3) A list of the new query stages that have been created. */ private def createQueryStages(plan: SparkPlan): CreateStageResult = plan match { case e: Exchange => // First have a quick check in the `stageCache` without having to traverse down the node. context.stageCache.get(e.canonicalized) match { case Some(existingStage) if conf.exchangeReuseEnabled => val stage = reuseQueryStage(existingStage, e) val isMaterialized = stage.isMaterialized CreateStageResult( newPlan = stage, allChildStagesMaterialized = isMaterialized, newStages = if (isMaterialized) Seq.empty else Seq(stage)) case _ => val result = createQueryStages(e.child) val newPlan = e.withNewChildren(Seq(result.newPlan)).asInstanceOf[Exchange] // Create a query stage only when all the child query stages are ready. if (result.allChildStagesMaterialized) { var newStage = newQueryStage(newPlan) if (conf.exchangeReuseEnabled) { // Check the `stageCache` again for reuse. If a match is found, ditch the new stage // and reuse the existing stage found in the `stageCache`, otherwise update the // `stageCache` with the new stage. val queryStage = context.stageCache.getOrElseUpdate( newStage.plan.canonicalized, newStage) if (queryStage.ne(newStage)) { newStage = reuseQueryStage(queryStage, e) } } val isMaterialized = newStage.isMaterialized CreateStageResult( newPlan = newStage, allChildStagesMaterialized = isMaterialized, newStages = if (isMaterialized) Seq.empty else Seq(newStage)) } else { CreateStageResult(newPlan = newPlan, allChildStagesMaterialized = false, newStages = result.newStages) } } case q: QueryStageExec => CreateStageResult(newPlan = q, allChildStagesMaterialized = q.isMaterialized, newStages = Seq.empty) case _ => if (plan.children.isEmpty) { CreateStageResult(newPlan = plan, allChildStagesMaterialized = true, newStages = Seq.empty) } else { val results = plan.children.map(createQueryStages) CreateStageResult( newPlan = plan.withNewChildren(results.map(_.newPlan)), allChildStagesMaterialized = results.forall(_.allChildStagesMaterialized), newStages = results.flatMap(_.newStages)) } } private def newQueryStage(e: Exchange): QueryStageExec = { val optimizedPlan = optimizeQueryStage(e.child, isFinalStage = false) val queryStage = e match { case s: ShuffleExchangeLike => val newShuffle = applyPhysicalRules( s.withNewChildren(Seq(optimizedPlan)), postStageCreationRules(outputsColumnar = s.supportsColumnar), Some((planChangeLogger, \"AQE Post Stage Creation\"))) if (!newShuffle.isInstanceOf[ShuffleExchangeLike]) { throw new IllegalStateException( \"Custom columnar rules cannot transform shuffle node to something else.\") } ShuffleQueryStageExec(currentStageId, newShuffle, s.canonicalized) case b: BroadcastExchangeLike => val newBroadcast = applyPhysicalRules( b.withNewChildren(Seq(optimizedPlan)), postStageCreationRules(outputsColumnar = b.supportsColumnar), Some((planChangeLogger, \"AQE Post Stage Creation\"))) if (!newBroadcast.isInstanceOf[BroadcastExchangeLike]) { throw new IllegalStateException( \"Custom columnar rules cannot transform broadcast node to something else.\") } BroadcastQueryStageExec(currentStageId, newBroadcast, b.canonicalized) } currentStageId += 1 setLogicalLinkForNewQueryStage(queryStage, e) queryStage } rules @transient private val costEvaluator = conf.getConf(SQLConf.ADAPTIVE_CUSTOM_COST_EVALUATOR_CLASS) match { case Some(className) => CostEvaluator.instantiate(className, session.sparkContext.getConf) case _ => SimpleCostEvaluator(conf.getConf(SQLConf.ADAPTIVE_FORCE_OPTIMIZE_SKEWED_JOIN)) } // A list of physical plan rules to be applied before creation of query stages. The physical // plan should reach a final status of query stages (i.e., no more addition or removal of // Exchange nodes) after running these rules. @transient private val queryStagePreparationRules: Seq[Rule[SparkPlan]] = { // For cases like `df.repartition(a, b).select(c)`, there is no distribution requirement for // the final plan, but we do need to respect the user-specified repartition. Here we ask // `EnsureRequirements` to not optimize out the user-specified repartition-by-col to work // around this case. val ensureRequirements = EnsureRequirements(requiredDistribution.isDefined, requiredDistribution) Seq( RemoveRedundantProjects, ensureRequirements, ValidateSparkPlan, ReplaceHashWithSortAgg, RemoveRedundantSorts, DisableUnnecessaryBucketedScan, OptimizeSkewedJoin(ensureRequirements) ) ++ context.session.sessionState.adaptiveRulesHolder.queryStagePrepRules } // A list of physical optimizer rules to be applied to a new stage before its execution. These // optimizations should be stage-independent. @transient private val queryStageOptimizerRules: Seq[Rule[SparkPlan]] = Seq( PlanAdaptiveDynamicPruningFilters(this), ReuseAdaptiveSubquery(context.subqueryCache), OptimizeSkewInRebalancePartitions, CoalesceShufflePartitions(context.session), // `OptimizeShuffleWithLocalRead` needs to make use of 'AQEShuffleReadExec.partitionSpecs' // added by `CoalesceShufflePartitions`, and must be executed after it. OptimizeShuffleWithLocalRead ) // This rule is stateful as it maintains the codegen stage ID. We can't create a fresh one every // time and need to keep it in a variable. @transient private val collapseCodegenStagesRule: Rule[SparkPlan] = CollapseCodegenStages() // A list of physical optimizer rules to be applied right after a new stage is created. The input // plan to these rules has exchange as its root node. private def postStageCreationRules(outputsColumnar: Boolean) = Seq( ApplyColumnarRulesAndInsertTransitions( context.session.sessionState.columnarRules, outputsColumnar), collapseCodegenStagesRule ) @transient val initialPlan = context.session.withActive { applyPhysicalRules( inputPlan, queryStagePreparationRules, Some((planChangeLogger, \"AQE Preparations\"))) } @volatile private var currentPhysicalPlan = initialPlan // The logical plan optimizer for re-optimizing the current logical plan. @transient private val optimizer = new AQEOptimizer(conf, session.sessionState.adaptiveRulesHolder.runtimeOptimizerRules) private def optimizeQueryStage(plan: SparkPlan, isFinalStage: Boolean): SparkPlan = { val optimized = queryStageOptimizerRules.foldLeft(plan) { case (latestPlan, rule) => val applied = rule.apply(latestPlan) val result = rule match { case _: AQEShuffleReadRule if !applied.fastEquals(latestPlan) => val distribution = if (isFinalStage) { // If `requiredDistribution` is None, it means `EnsureRequirements` will not optimize // out the user-specified repartition, thus we don't have a distribution requirement // for the final plan. requiredDistribution.getOrElse(UnspecifiedDistribution) } else { UnspecifiedDistribution } if (ValidateRequirements.validate(applied, distribution)) { applied } else { logDebug(s\"Rule ${rule.ruleName} is not applied as it breaks the \" + \"distribution requirement of the query plan.\") latestPlan } case _ => applied } planChangeLogger.logRule(rule.ruleName, latestPlan, result) result } planChangeLogger.logBatch(\"AQE Query Stage Optimization\", plan, optimized) optimized } /** * Re-optimize and run physical planning on the current logical plan based on the latest stats. */ private def reOptimize(logicalPlan: LogicalPlan): Option[(SparkPlan, LogicalPlan)] = { try { logicalPlan.invalidateStatsCache() val optimized = optimizer.execute(logicalPlan) val sparkPlan = context.session.sessionState.planner.plan(ReturnAnswer(optimized)).next() val newPlan = applyPhysicalRules( sparkPlan, preprocessingRules ++ queryStagePreparationRules, Some((planChangeLogger, \"AQE Replanning\"))) // When both enabling AQE and DPP, `PlanAdaptiveDynamicPruningFilters` rule will // add the `BroadcastExchangeExec` node manually in the DPP subquery, // not through `EnsureRequirements` rule. Therefore, when the DPP subquery is complicated // and need to be re-optimized, AQE also need to manually insert the `BroadcastExchangeExec` // node to prevent the loss of the `BroadcastExchangeExec` node in DPP subquery. // Here, we also need to avoid to insert the `BroadcastExchangeExec` node when the newPlan is // already the `BroadcastExchangeExec` plan after apply the `LogicalQueryStageStrategy` rule. val finalPlan = inputPlan match { case b: BroadcastExchangeLike if (!newPlan.isInstanceOf[BroadcastExchangeLike]) => b.withNewChildren(Seq(newPlan)) case _ => newPlan } Some((finalPlan, optimized)) } catch { case e: InvalidAQEPlanException[_] => logOnLevel(s\"Re-optimize - ${e.getMessage()}:\\n${e.plan}\") None } }","title":"AdaptiveSparkPlanExec"},{"location":"SparkSQL/AQE/#querystageexec","text":"org.apache.spark.sql.execution.adaptive.QueryStageExec /** * A query stage is an independent subgraph of the query plan. Query stage materializes its output * before proceeding with further operators of the query plan. The data statistics of the * materialized output can be used to optimize subsequent query stages. * * There are 2 kinds of query stages: * 1. Shuffle query stage. This stage materializes its output to shuffle files, and Spark launches * another job to execute the further operators. * 2. Broadcast query stage. This stage materializes its output to an array in driver JVM. Spark * broadcasts the array before executing the further operators. */ abstract class QueryStageExec extends LeafExecNode { @transient @volatile protected var _resultOption = new AtomicReference[Option[Any]](None) private[adaptive] def resultOption: AtomicReference[Option[Any]] = _resultOption def isMaterialized: Boolean = resultOption.get().isDefined /** * Compute the statistics of the query stage if executed, otherwise None. */ def computeStats(): Option[Statistics] = if (isMaterialized) { val runtimeStats = getRuntimeStatistics val dataSize = runtimeStats.sizeInBytes.max(0) val numOutputRows = runtimeStats.rowCount.map(_.max(0)) Some(Statistics(dataSize, numOutputRows, isRuntime = true)) } else { None } /** * Materialize this query stage, to prepare for the execution, like submitting map stages, * broadcasting data, etc. The caller side can use the returned [[Future]] to wait until this * stage is ready. */ def doMaterialize(): Future[Any] ====== materialize() is called by org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#getFinalPhysicalPlan === /** * Materialize this query stage, to prepare for the execution, like submitting map stages, * broadcasting data, etc. The caller side can use the returned [[Future]] to wait until this * stage is ready. */ final def materialize(): Future[Any] = { logDebug(s\"Materialize query stage ${this.getClass.getSimpleName}: $id\") doMaterialize() } ========================================================================================================= /** * A shuffle query stage whose child is a [[ShuffleExchangeLike]] or [[ReusedExchangeExec]]. * * @param id the query stage id. * @param plan the underlying plan. * @param _canonicalized the canonicalized plan before applying query stage optimizer rules. */ case class ShuffleQueryStageExec( override val id: Int, override val plan: SparkPlan, override val _canonicalized: SparkPlan) extends QueryStageExec { @transient val shuffle = plan match { case s: ShuffleExchangeLike => s case ReusedExchangeExec(_, s: ShuffleExchangeLike) => s case _ => throw new IllegalStateException(s\"wrong plan for shuffle stage:\\n ${plan.treeString}\") } @transient private lazy val shuffleFuture = shuffle.submitShuffleJob override def doMaterialize(): Future[Any] = shuffleFuture override def getRuntimeStatistics: Statistics = shuffle.runtimeStatistics /** * A broadcast query stage whose child is a [[BroadcastExchangeLike]] or [[ReusedExchangeExec]]. * * @param id the query stage id. * @param plan the underlying plan. * @param _canonicalized the canonicalized plan before applying query stage optimizer rules. */ case class BroadcastQueryStageExec( override val id: Int, override val plan: SparkPlan, override val _canonicalized: SparkPlan) extends QueryStageExec { @transient val broadcast = plan match { case b: BroadcastExchangeLike => b case ReusedExchangeExec(_, b: BroadcastExchangeLike) => b case _ => throw new IllegalStateException(s\"wrong plan for broadcast stage:\\n ${plan.treeString}\") } override def doMaterialize(): Future[Any] = { broadcast.submitBroadcastJob } override def getRuntimeStatistics: Statistics = broadcast.runtimeStatistics","title":"QueryStageExec"},{"location":"SparkSQL/AQE/#reusequerystage","text":"org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#createQueryStages => reuseQueryStage org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#reuseQueryStage private def reuseQueryStage(existing: QueryStageExec, exchange: Exchange): QueryStageExec = { val queryStage = existing.newReuseInstance(currentStageId, exchange.output) currentStageId += 1 setLogicalLinkForNewQueryStage(queryStage, exchange) queryStage } org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec#newReuseInstance override def newReuseInstance(newStageId: Int, newOutput: Seq[Attribute]): QueryStageExec = { val reuse = BroadcastQueryStageExec( newStageId, ReusedExchangeExec(newOutput, broadcast), _canonicalized) reuse._resultOption = this._resultOption reuse } org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec#newReuseInstance override def newReuseInstance(newStageId: Int, newOutput: Seq[Attribute]): QueryStageExec = { val reuse = ShuffleQueryStageExec( newStageId, ReusedExchangeExec(newOutput, shuffle), _canonicalized) reuse._resultOption = this._resultOption reuse }","title":"reuseQueryStage"},{"location":"SparkSQL/AQE/#adaptive-coalesce-partitions","text":"SQLConf val COALESCE_PARTITIONS_ENABLED = buildConf(\"spark.sql.adaptive.coalescePartitions.enabled\") .doc(s\"When true and '${ADAPTIVE_EXECUTION_ENABLED.key}' is true, Spark will coalesce \" + \"contiguous shuffle partitions according to the target size (specified by \" + s\"'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}'), to avoid too many small tasks.\") .version(\"3.0.0\") .booleanConf .createWithDefault(true) val COALESCE_PARTITIONS_PARALLELISM_FIRST = buildConf(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\") .doc(\"When true, Spark does not respect the target size specified by \" + s\"'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}' (default 64MB) when coalescing contiguous \" + \"shuffle partitions, but adaptively calculate the target size according to the default \" + \"parallelism of the Spark cluster. The calculated size is usually smaller than the \" + \"configured target size. This is to maximize the parallelism and avoid performance \" + \"regression when enabling adaptive query execution. It's recommended to set this config \" + \"to false and respect the configured target size.\") .version(\"3.2.0\") .booleanConf .createWithDefault(true) val COALESCE_PARTITIONS_MIN_PARTITION_SIZE = buildConf(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\") .doc(\"The minimum size of shuffle partitions after coalescing. This is useful when the \" + \"adaptively calculated target size is too small during partition coalescing.\") .version(\"3.2.0\") .bytesConf(ByteUnit.BYTE) .checkValue(_ > 0, \"minPartitionSize must be positive\") .createWithDefaultString(\"1MB\") val COALESCE_PARTITIONS_MIN_PARTITION_NUM = buildConf(\"spark.sql.adaptive.coalescePartitions.minPartitionNum\") .internal() .doc(\"(deprecated) The suggested (not guaranteed) minimum number of shuffle partitions \" + \"after coalescing. If not set, the default value is the default parallelism of the \" + \"Spark cluster. This configuration only has an effect when \" + s\"'${ADAPTIVE_EXECUTION_ENABLED.key}' and \" + s\"'${COALESCE_PARTITIONS_ENABLED.key}' are both true.\") .version(\"3.0.0\") .intConf .checkValue(_ > 0, \"The minimum number of partitions must be positive.\") .createOptional val COALESCE_PARTITIONS_INITIAL_PARTITION_NUM = buildConf(\"spark.sql.adaptive.coalescePartitions.initialPartitionNum\") .doc(\"The initial number of shuffle partitions before coalescing. If not set, it equals to \" + s\"${SHUFFLE_PARTITIONS.key}. This configuration only has an effect when \" + s\"'${ADAPTIVE_EXECUTION_ENABLED.key}' and '${COALESCE_PARTITIONS_ENABLED.key}' \" + \"are both true.\") .version(\"3.0.0\") .intConf .checkValue(_ > 0, \"The initial number of partitions must be positive.\") .createOptional org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#queryStageOptimizerRules // A list of physical optimizer rules to be applied to a new stage before its execution. These // optimizations should be stage-independent. @transient private val queryStageOptimizerRules: Seq[Rule[SparkPlan]] = Seq( PlanAdaptiveDynamicPruningFilters(this), ReuseAdaptiveSubquery(context.subqueryCache), OptimizeSkewInRebalancePartitions, ====== rule for coalesce partitions ========== CoalesceShufflePartitions(context.session), ============================================== // `OptimizeShuffleWithLocalRead` needs to make use of 'AQEShuffleReadExec.partitionSpecs' // added by `CoalesceShufflePartitions`, and must be executed after it. OptimizeShuffleWithLocalRead ) org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions /** * A rule to coalesce the shuffle partitions based on the map output statistics, which can * avoid many small reduce tasks that hurt performance. */ case class CoalesceShufflePartitions(session: SparkSession) extends AQEShuffleReadRule { override def apply(plan: SparkPlan): SparkPlan = { if (!conf.coalesceShufflePartitionsEnabled) { return plan } // Ideally, this rule should simply coalesce partitions w.r.t. the target size specified by // ADVISORY_PARTITION_SIZE_IN_BYTES (default 64MB). To avoid perf regression in AQE, this // rule by default tries to maximize the parallelism and set the target size to // `total shuffle size / Spark default parallelism`. In case the `Spark default parallelism` // is too big, this rule also respect the minimum partition size specified by // COALESCE_PARTITIONS_MIN_PARTITION_SIZE (default 1MB). // For history reason, this rule also need to support the config // COALESCE_PARTITIONS_MIN_PARTITION_NUM. We should remove this config in the future. val minNumPartitions = conf.getConf(SQLConf.COALESCE_PARTITIONS_MIN_PARTITION_NUM).getOrElse { if (conf.getConf(SQLConf.COALESCE_PARTITIONS_PARALLELISM_FIRST)) { // We fall back to Spark default parallelism if the minimum number of coalesced partitions // is not set, so to avoid perf regressions compared to no coalescing. session.sparkContext.defaultParallelism } else { // If we don't need to maximize the parallelism, we set `minPartitionNum` to 1, so that // the specified advisory partition size will be respected. 1 } } val advisoryTargetSize = conf.getConf(SQLConf.ADVISORY_PARTITION_SIZE_IN_BYTES) val minPartitionSize = if (Utils.isTesting) { // In the tests, we usually set the target size to a very small value that is even smaller // than the default value of the min partition size. Here we also adjust the min partition // size to be not larger than 20% of the target size, so that the tests don't need to set // both configs all the time to check the coalescing behavior. conf.getConf(SQLConf.COALESCE_PARTITIONS_MIN_PARTITION_SIZE).min(advisoryTargetSize / 5) } else { conf.getConf(SQLConf.COALESCE_PARTITIONS_MIN_PARTITION_SIZE) } // Sub-plans under the Union operator can be coalesced independently, so we can divide them // into independent \"coalesce groups\", and all shuffle stages within each group have to be // coalesced together. val coalesceGroups = collectCoalesceGroups(plan) // Divide minimum task parallelism among coalesce groups according to their data sizes. val minNumPartitionsByGroup = if (coalesceGroups.length == 1) { Seq(math.max(minNumPartitions, 1)) } else { val sizes = coalesceGroups.map(_.flatMap(_.shuffleStage.mapStats.map(_.bytesByPartitionId.sum)).sum) val totalSize = sizes.sum sizes.map { size => val num = if (totalSize > 0) { math.round(minNumPartitions * 1.0 * size / totalSize) } else { minNumPartitions } math.max(num.toInt, 1) } } val specsMap = mutable.HashMap.empty[Int, Seq[ShufflePartitionSpec]] // Coalesce partitions for each coalesce group independently. coalesceGroups.zip(minNumPartitionsByGroup).foreach { case (shuffleStages, minNumPartitions) => val newPartitionSpecs = ShufflePartitionsUtil.coalescePartitions( shuffleStages.map(_.shuffleStage.mapStats), shuffleStages.map(_.partitionSpecs), advisoryTargetSize = advisoryTargetSize, minNumPartitions = minNumPartitions, minPartitionSize = minPartitionSize) if (newPartitionSpecs.nonEmpty) { shuffleStages.zip(newPartitionSpecs).map { case (stageInfo, partSpecs) => specsMap.put(stageInfo.shuffleStage.id, partSpecs) } } } if (specsMap.nonEmpty) { updateShuffleReads(plan, specsMap.toMap) } else { plan } } private def updateShuffleReads( plan: SparkPlan, specsMap: Map[Int, Seq[ShufflePartitionSpec]]): SparkPlan = plan match { // Even for shuffle exchange whose input RDD has 0 partition, we should still update its // `partitionStartIndices`, so that all the leaf shuffles in a stage have the same // number of output partitions. case ShuffleStageInfo(stage, _) => specsMap.get(stage.id).map { specs => AQEShuffleReadExec(stage, specs) }.getOrElse(plan) case other => other.mapChildren(updateShuffleReads(_, specsMap)) } org.apache.spark.sql.execution.adaptive.AQEShuffleReadRule /** * A rule that may create [[AQEShuffleReadExec]] on top of [[ShuffleQueryStageExec]] and change the * plan output partitioning. The AQE framework will skip the rule if it leads to extra shuffles. */ trait AQEShuffleReadRule extends Rule[SparkPlan] { /** * Returns the list of [[ShuffleOrigin]]s supported by this rule. */ protected def supportedShuffleOrigins: Seq[ShuffleOrigin] protected def isSupported(shuffle: ShuffleExchangeLike): Boolean = { supportedShuffleOrigins.contains(shuffle.shuffleOrigin) } } org.apache.spark.sql.execution.adaptive.AQEShuffleReadExec /** * A wrapper of shuffle query stage, which follows the given partition arrangement. * * @param child It is usually `ShuffleQueryStageExec`, but can be the shuffle exchange * node during canonicalization. * @param partitionSpecs The partition specs that defines the arrangement, requires at least one * partition. */ case class AQEShuffleReadExec private( child: SparkPlan, partitionSpecs: Seq[ShufflePartitionSpec]) extends UnaryExecNode { private def shuffleStage = child match { case stage: ShuffleQueryStageExec => Some(stage) case _ => None } private lazy val shuffleRDD: RDD[_] = { shuffleStage match { case Some(stage) => sendDriverMetrics() stage.shuffle.getShuffleRDD(partitionSpecs.toArray) case _ => throw new IllegalStateException(\"operating on canonicalized plan\") } } override protected def doExecute(): RDD[InternalRow] = { shuffleRDD.asInstanceOf[RDD[InternalRow]] } org.apache.spark.sql.execution.exchange.ShuffleExchangeExec#getShuffleRDD override def getShuffleRDD(partitionSpecs: Array[ShufflePartitionSpec]): RDD[InternalRow] = { new ShuffledRowRDD(shuffleDependency, readMetrics, partitionSpecs) }","title":"Adaptive coalesce partitions"},{"location":"SparkSQL/Aggregation/","text":"Aggregation Aggregation \u00b6","title":"Aggregation"},{"location":"SparkSQL/Aggregation/#aggregation","text":"","title":"Aggregation"},{"location":"SparkSQL/Analyzer/","text":"Analyzer Logical query plan analyzer ResolveSessionCatalog analyzeTable Analyzer \u00b6 Logical query plan analyzer \u00b6 abstract class BaseSessionStateBuilder( val session: SparkSession, val parentState: Option[SessionState]) { /** * Logical query plan analyzer for resolving unresolved attributes and relations. * * Note: this depends on the `conf` and `catalog` fields. */ protected def analyzer: Analyzer = new Analyzer(catalogManager) { override val extendedResolutionRules: Seq[Rule[LogicalPlan]] = new FindDataSourceTable(session) +: new ResolveSQLOnFile(session) +: new FallBackFileSourceV2(session) +: ResolveEncodersInScalaAgg +: new ResolveSessionCatalog(catalogManager) +: =====> ResolveSessionCatalog Rule ResolveWriteToStream +: new EvalSubqueriesForTimeTravel +: customResolutionRules ResolveSessionCatalog \u00b6 /** * Resolves catalogs from the multi-part identifiers in SQL statements, and convert the statements * to the corresponding v1 or v2 commands if the resolved catalog is the session catalog. * * We can remove this rule once we implement all the catalog functionality in `V2SessionCatalog`. */ class ResolveSessionCatalog(val catalogManager: CatalogManager) extends Rule[LogicalPlan] with LookupCatalog { override def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUp { case AnalyzeTables(DatabaseInSessionCatalog(db), noScan) => AnalyzeTablesCommand(Some(db), noScan) /** * Analyzes all tables in the given database to generate statistics. */ case class AnalyzeTablesCommand( databaseName: Option[String], noScan: Boolean) extends LeafRunnableCommand { override def run(sparkSession: SparkSession): Seq[Row] = { val catalog = sparkSession.sessionState.catalog val db = databaseName.getOrElse(catalog.getCurrentDatabase) catalog.listTables(db).foreach { tbl => try { CommandUtils.analyzeTable(sparkSession, tbl, noScan) } catch { case NonFatal(e) => logWarning(s\"Failed to analyze table ${tbl.table} in the \" + s\"database $db because of ${e.toString}\", e) } } Seq.empty[Row] } } analyzeTable \u00b6 org.apache.spark.sql.execution.command.CommandUtils#analyzeTable def analyzeTable( sparkSession: SparkSession, tableIdent: TableIdentifier, noScan: Boolean): Unit = { val sessionState = sparkSession.sessionState val db = tableIdent.database.getOrElse(sessionState.catalog.getCurrentDatabase) val tableIdentWithDB = TableIdentifier(tableIdent.table, Some(db)) val tableMeta = sessionState.catalog.getTableMetadata(tableIdentWithDB) if (tableMeta.tableType == CatalogTableType.VIEW) { // Analyzes a catalog view if the view is cached val table = sparkSession.table(tableIdent.quotedString) val cacheManager = sparkSession.sharedState.cacheManager if (cacheManager.lookupCachedData(table.logicalPlan).isDefined) { if (!noScan) { // To collect table stats, materializes an underlying columnar RDD table.count() } } else { throw QueryCompilationErrors.analyzeTableNotSupportedOnViewsError() } } else { // Compute stats for the whole table val (newTotalSize, _) = CommandUtils.calculateTotalSize(sparkSession, tableMeta) =====> calculateTotalSize of the table val newRowCount = if (noScan) None else Some(BigInt(sparkSession.table(tableIdentWithDB).count())) // Update the metastore if the above statistics of the table are different from those // recorded in the metastore. val newStats = CommandUtils.compareAndGetNewStats(tableMeta.stats, newTotalSize, newRowCount) if (newStats.isDefined) { sessionState.catalog.alterTableStats(tableIdentWithDB, newStats) } } } def calculateTotalSize( spark: SparkSession, catalogTable: CatalogTable): (BigInt, Seq[CatalogTablePartition]) = { val sessionState = spark.sessionState val startTime = System.nanoTime() val (totalSize, newPartitions) = if (catalogTable.partitionColumnNames.isEmpty) { (calculateSingleLocationSize(sessionState, catalogTable.identifier, =====> calculateSingleLocationSize catalogTable.storage.locationUri), Seq()) } else { // Calculate table size as a sum of the visible partitions. See SPARK-21079 val partitions = sessionState.catalog.listPartitions(catalogTable.identifier) =====> listPartitions logInfo(s\"Starting to calculate sizes for ${partitions.length} partitions.\") val paths = partitions.map(_.storage.locationUri) val sizes = calculateMultipleLocationSizes(spark, catalogTable.identifier, paths) =====> calculateMultipleLocationSizes val newPartitions = partitions.zipWithIndex.flatMap { case (p, idx) => val newStats = CommandUtils.compareAndGetNewStats(p.stats, sizes(idx), None) newStats.map(_ => p.copy(stats = newStats)) } (sizes.sum, newPartitions) } logInfo(s\"It took ${(System.nanoTime() - startTime) / (1000 * 1000)} ms to calculate\" + s\" the total size for table ${catalogTable.identifier}.\") (totalSize, newPartitions) } def calculateMultipleLocationSizes( sparkSession: SparkSession, tid: TableIdentifier, paths: Seq[Option[URI]]): Seq[Long] = { if (sparkSession.sessionState.conf.parallelFileListingInStatsComputation) { calculateMultipleLocationSizesInParallel(sparkSession, paths.map(_.map(new Path(_)))) } else { paths.map(p => calculateSingleLocationSize(sparkSession.sessionState, tid, p)) } } def calculateSingleLocationSize( sessionState: SessionState, identifier: TableIdentifier, locationUri: Option[URI]): Long = { // This method is mainly based on // org.apache.hadoop.hive.ql.stats.StatsUtils.getFileSizeForTable(HiveConf, Table) // in Hive 0.13 (except that we do not use fs.getContentSummary). // TODO: Generalize statistics collection. // TODO: Why fs.getContentSummary returns wrong size on Jenkins? // Can we use fs.getContentSummary in future? // Seems fs.getContentSummary returns wrong table size on Jenkins. So we use // countFileSize to count the table size. val stagingDir = sessionState.conf.getConfString(\"hive.exec.stagingdir\", \".hive-staging\") def getPathSize(fs: FileSystem, path: Path): Long = { val fileStatus = fs.getFileStatus(path) val size = if (fileStatus.isDirectory) { fs.listStatus(path) .map { status => if (isDataPath(status.getPath, stagingDir)) { getPathSize(fs, status.getPath) } else { 0L } }.sum } else { fileStatus.getLen } size } val startTime = System.nanoTime() val size = locationUri.map { p => val path = new Path(p) try { val fs = path.getFileSystem(sessionState.newHadoopConf()) getPathSize(fs, path) } catch { case NonFatal(e) => logWarning( s\"Failed to get the size of table ${identifier.table} in the \" + s\"database ${identifier.database} because of ${e.toString}\", e) 0L } }.getOrElse(0L) val durationInMs = (System.nanoTime() - startTime) / (1000 * 1000) logDebug(s\"It took $durationInMs ms to calculate the total file size under path $locationUri.\") size } private[spark] object HadoopFSUtils extends Logging { /** * Lists a collection of paths recursively. Picks the listing strategy adaptively depending * on the number of paths to list. * * This may only be called on the driver. * * @param sc Spark context used to run parallel listing. * @param paths Input paths to list * @param hadoopConf Hadoop configuration * @param filter Path filter used to exclude leaf files from result * @param ignoreMissingFiles Ignore missing files that happen during recursive listing * (e.g., due to race conditions) * @param ignoreLocality Whether to fetch data locality info when listing leaf files. If false, * this will return `FileStatus` without `BlockLocation` info. * @param parallelismThreshold The threshold to enable parallelism. If the number of input paths * is smaller than this value, this will fallback to use * sequential listing. * @param parallelismMax The maximum parallelism for listing. If the number of input paths is * larger than this value, parallelism will be throttled to this value * to avoid generating too many tasks. * @return for each input path, the set of discovered files for the path */ def parallelListLeafFiles( sc: SparkContext, paths: Seq[Path], hadoopConf: Configuration, filter: PathFilter, ignoreMissingFiles: Boolean, ignoreLocality: Boolean, parallelismThreshold: Int, parallelismMax: Int): Seq[(Path, Seq[FileStatus])] = { parallelListLeafFilesInternal(sc, paths, hadoopConf, filter, isRootLevel = true, ignoreMissingFiles, ignoreLocality, parallelismThreshold, parallelismMax) }","title":"Analyzer"},{"location":"SparkSQL/Analyzer/#analyzer","text":"","title":"Analyzer"},{"location":"SparkSQL/Analyzer/#logical-query-plan-analyzer","text":"abstract class BaseSessionStateBuilder( val session: SparkSession, val parentState: Option[SessionState]) { /** * Logical query plan analyzer for resolving unresolved attributes and relations. * * Note: this depends on the `conf` and `catalog` fields. */ protected def analyzer: Analyzer = new Analyzer(catalogManager) { override val extendedResolutionRules: Seq[Rule[LogicalPlan]] = new FindDataSourceTable(session) +: new ResolveSQLOnFile(session) +: new FallBackFileSourceV2(session) +: ResolveEncodersInScalaAgg +: new ResolveSessionCatalog(catalogManager) +: =====> ResolveSessionCatalog Rule ResolveWriteToStream +: new EvalSubqueriesForTimeTravel +: customResolutionRules","title":"Logical query plan analyzer"},{"location":"SparkSQL/Analyzer/#resolvesessioncatalog","text":"/** * Resolves catalogs from the multi-part identifiers in SQL statements, and convert the statements * to the corresponding v1 or v2 commands if the resolved catalog is the session catalog. * * We can remove this rule once we implement all the catalog functionality in `V2SessionCatalog`. */ class ResolveSessionCatalog(val catalogManager: CatalogManager) extends Rule[LogicalPlan] with LookupCatalog { override def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUp { case AnalyzeTables(DatabaseInSessionCatalog(db), noScan) => AnalyzeTablesCommand(Some(db), noScan) /** * Analyzes all tables in the given database to generate statistics. */ case class AnalyzeTablesCommand( databaseName: Option[String], noScan: Boolean) extends LeafRunnableCommand { override def run(sparkSession: SparkSession): Seq[Row] = { val catalog = sparkSession.sessionState.catalog val db = databaseName.getOrElse(catalog.getCurrentDatabase) catalog.listTables(db).foreach { tbl => try { CommandUtils.analyzeTable(sparkSession, tbl, noScan) } catch { case NonFatal(e) => logWarning(s\"Failed to analyze table ${tbl.table} in the \" + s\"database $db because of ${e.toString}\", e) } } Seq.empty[Row] } }","title":"ResolveSessionCatalog"},{"location":"SparkSQL/Analyzer/#analyzetable","text":"org.apache.spark.sql.execution.command.CommandUtils#analyzeTable def analyzeTable( sparkSession: SparkSession, tableIdent: TableIdentifier, noScan: Boolean): Unit = { val sessionState = sparkSession.sessionState val db = tableIdent.database.getOrElse(sessionState.catalog.getCurrentDatabase) val tableIdentWithDB = TableIdentifier(tableIdent.table, Some(db)) val tableMeta = sessionState.catalog.getTableMetadata(tableIdentWithDB) if (tableMeta.tableType == CatalogTableType.VIEW) { // Analyzes a catalog view if the view is cached val table = sparkSession.table(tableIdent.quotedString) val cacheManager = sparkSession.sharedState.cacheManager if (cacheManager.lookupCachedData(table.logicalPlan).isDefined) { if (!noScan) { // To collect table stats, materializes an underlying columnar RDD table.count() } } else { throw QueryCompilationErrors.analyzeTableNotSupportedOnViewsError() } } else { // Compute stats for the whole table val (newTotalSize, _) = CommandUtils.calculateTotalSize(sparkSession, tableMeta) =====> calculateTotalSize of the table val newRowCount = if (noScan) None else Some(BigInt(sparkSession.table(tableIdentWithDB).count())) // Update the metastore if the above statistics of the table are different from those // recorded in the metastore. val newStats = CommandUtils.compareAndGetNewStats(tableMeta.stats, newTotalSize, newRowCount) if (newStats.isDefined) { sessionState.catalog.alterTableStats(tableIdentWithDB, newStats) } } } def calculateTotalSize( spark: SparkSession, catalogTable: CatalogTable): (BigInt, Seq[CatalogTablePartition]) = { val sessionState = spark.sessionState val startTime = System.nanoTime() val (totalSize, newPartitions) = if (catalogTable.partitionColumnNames.isEmpty) { (calculateSingleLocationSize(sessionState, catalogTable.identifier, =====> calculateSingleLocationSize catalogTable.storage.locationUri), Seq()) } else { // Calculate table size as a sum of the visible partitions. See SPARK-21079 val partitions = sessionState.catalog.listPartitions(catalogTable.identifier) =====> listPartitions logInfo(s\"Starting to calculate sizes for ${partitions.length} partitions.\") val paths = partitions.map(_.storage.locationUri) val sizes = calculateMultipleLocationSizes(spark, catalogTable.identifier, paths) =====> calculateMultipleLocationSizes val newPartitions = partitions.zipWithIndex.flatMap { case (p, idx) => val newStats = CommandUtils.compareAndGetNewStats(p.stats, sizes(idx), None) newStats.map(_ => p.copy(stats = newStats)) } (sizes.sum, newPartitions) } logInfo(s\"It took ${(System.nanoTime() - startTime) / (1000 * 1000)} ms to calculate\" + s\" the total size for table ${catalogTable.identifier}.\") (totalSize, newPartitions) } def calculateMultipleLocationSizes( sparkSession: SparkSession, tid: TableIdentifier, paths: Seq[Option[URI]]): Seq[Long] = { if (sparkSession.sessionState.conf.parallelFileListingInStatsComputation) { calculateMultipleLocationSizesInParallel(sparkSession, paths.map(_.map(new Path(_)))) } else { paths.map(p => calculateSingleLocationSize(sparkSession.sessionState, tid, p)) } } def calculateSingleLocationSize( sessionState: SessionState, identifier: TableIdentifier, locationUri: Option[URI]): Long = { // This method is mainly based on // org.apache.hadoop.hive.ql.stats.StatsUtils.getFileSizeForTable(HiveConf, Table) // in Hive 0.13 (except that we do not use fs.getContentSummary). // TODO: Generalize statistics collection. // TODO: Why fs.getContentSummary returns wrong size on Jenkins? // Can we use fs.getContentSummary in future? // Seems fs.getContentSummary returns wrong table size on Jenkins. So we use // countFileSize to count the table size. val stagingDir = sessionState.conf.getConfString(\"hive.exec.stagingdir\", \".hive-staging\") def getPathSize(fs: FileSystem, path: Path): Long = { val fileStatus = fs.getFileStatus(path) val size = if (fileStatus.isDirectory) { fs.listStatus(path) .map { status => if (isDataPath(status.getPath, stagingDir)) { getPathSize(fs, status.getPath) } else { 0L } }.sum } else { fileStatus.getLen } size } val startTime = System.nanoTime() val size = locationUri.map { p => val path = new Path(p) try { val fs = path.getFileSystem(sessionState.newHadoopConf()) getPathSize(fs, path) } catch { case NonFatal(e) => logWarning( s\"Failed to get the size of table ${identifier.table} in the \" + s\"database ${identifier.database} because of ${e.toString}\", e) 0L } }.getOrElse(0L) val durationInMs = (System.nanoTime() - startTime) / (1000 * 1000) logDebug(s\"It took $durationInMs ms to calculate the total file size under path $locationUri.\") size } private[spark] object HadoopFSUtils extends Logging { /** * Lists a collection of paths recursively. Picks the listing strategy adaptively depending * on the number of paths to list. * * This may only be called on the driver. * * @param sc Spark context used to run parallel listing. * @param paths Input paths to list * @param hadoopConf Hadoop configuration * @param filter Path filter used to exclude leaf files from result * @param ignoreMissingFiles Ignore missing files that happen during recursive listing * (e.g., due to race conditions) * @param ignoreLocality Whether to fetch data locality info when listing leaf files. If false, * this will return `FileStatus` without `BlockLocation` info. * @param parallelismThreshold The threshold to enable parallelism. If the number of input paths * is smaller than this value, this will fallback to use * sequential listing. * @param parallelismMax The maximum parallelism for listing. If the number of input paths is * larger than this value, parallelism will be throttled to this value * to avoid generating too many tasks. * @return for each input path, the set of discovered files for the path */ def parallelListLeafFiles( sc: SparkContext, paths: Seq[Path], hadoopConf: Configuration, filter: PathFilter, ignoreMissingFiles: Boolean, ignoreLocality: Boolean, parallelismThreshold: Int, parallelismMax: Int): Seq[(Path, Seq[FileStatus])] = { parallelListLeafFilesInternal(sc, paths, hadoopConf, filter, isRootLevel = true, ignoreMissingFiles, ignoreLocality, parallelismThreshold, parallelismMax) }","title":"analyzeTable"},{"location":"SparkSQL/Hive/","text":"Hive Hive \u00b6","title":"Hive"},{"location":"SparkSQL/Hive/#hive","text":"","title":"Hive"},{"location":"SparkSQL/Join/","text":"Join Optimizer: re-order Join JIRA Code Join \u00b6 Optimizer: re-order Join \u00b6 JIRA \u00b6 SPARK-12032 Filter can\u2019t be pushed down to correct Join because of bad order of Join code: PR-10073 ref code: PR-10258 For this query: select d.d_year, count(*) cnt FROM store_sales, date_dim d, customer c WHERE ss_customer_sk = c.c_customer_sk AND c.c_first_shipto_date_sk = d.d_date_sk group by d.d_year Current optimized plan is == Optimized Logical Plan == Aggregate [d_year#147], [d_year#147,(count(1),mode=Complete,isDistinct=false) AS cnt#425L] Project [d_year#147] Join Inner, Some(((ss_customer_sk#283 = c_customer_sk#101) && (c_first_shipto_date_sk#106 = d_date_sk#141))) Project [d_date_sk#141,d_year#147,ss_customer_sk#283] Join Inner, None Project [ss_customer_sk#283] Relation[] ParquetRelation[store_sales] Project [d_date_sk#141,d_year#147] Relation[] ParquetRelation[date_dim] Project [c_customer_sk#101,c_first_shipto_date_sk#106] Relation[] ParquetRelation[customer] It will join store_sales and date_dim together without any condition , the condition c.c_first_shipto_date_sk = d.d_date_sk is not pushed to it because the bad order of joins. The optimizer should re-order the joins, join date_dim after customer, then it can pushed down the condition correctly. The plan should be Aggregate [d_year#147], [d_year#147,(count(1),mode=Complete,isDistinct=false) AS cnt#425L] Project [d_year#147] Join Inner, Some((c_first_shipto_date_sk#106 = d_date_sk#141)) Project [c_first_shipto_date_sk#106] Join Inner, Some((ss_customer_sk#283 = c_customer_sk#101)) Project [ss_customer_sk#283] Relation[store_sales] Project [c_first_shipto_date_sk#106,c_customer_sk#101] Relation[customer] Project [d_year#147,d_date_sk#141] Relation[date_dim] Code \u00b6 org.apache.spark.sql.catalyst.optimizer.Optimizer#defaultBatches /** * Defines the default rule batches in the Optimizer. * * Implementations of this class should override this method, and [[nonExcludableRules]] if * necessary, instead of [[batches]]. The rule batches that eventually run in the Optimizer, * i.e., returned by [[batches]], will be (defaultBatches - (excludedRules - nonExcludableRules)). */ def defaultBatches: Seq[Batch] = { val operatorOptimizationRuleSet = Seq( // Operator push down PushProjectionThroughUnion, ReorderJoin, ==> ReorderJoin rules org.apache.spark.sql.catalyst.optimizer.ReorderJoin /** * Reorder the joins and push all the conditions into join, so that the bottom ones have at least * one condition. * * The order of joins will not be changed if all of them already have at least one condition. * * If star schema detection is enabled, reorder the star join plans based on heuristics. */ object ReorderJoin extends Rule[LogicalPlan] with PredicateHelper { /** * Join a list of plans together and push down the conditions into them. * * The joined plan are picked from left to right, prefer those has at least one join condition. * * @param input a list of LogicalPlans to inner join and the type of inner join. * @param conditions a list of condition for join. */ @tailrec final def createOrderedJoin( input: Seq[(LogicalPlan, InnerLike)], conditions: Seq[Expression]): LogicalPlan = { def apply(plan: LogicalPlan): LogicalPlan = plan.transformWithPruning( _.containsPattern(INNER_LIKE_JOIN), ruleId) { case p @ ExtractFiltersAndInnerJoins(input, conditions) if input.size > 2 && conditions.nonEmpty => val reordered = if (conf.starSchemaDetection && !conf.cboEnabled) { val starJoinPlan = StarSchemaDetection.reorderStarJoins(input, conditions) if (starJoinPlan.nonEmpty) { val rest = input.filterNot(starJoinPlan.contains(_)) createOrderedJoin(starJoinPlan ++ rest, conditions) } else { createOrderedJoin(input, conditions) } } else { createOrderedJoin(input, conditions) } if (p.sameOutput(reordered)) { reordered } else { // Reordering the joins have changed the order of the columns. // Inject a projection to make sure we restore to the expected ordering. Project(p.output, reordered) } } org.apache.spark.sql.catalyst.planning.ExtractFiltersAndInnerJoins /** * A pattern that collects the filter and inner joins. * * Filter * | * inner Join * / \\ ----> (Seq(plan0, plan1, plan2), conditions) * Filter plan2 * | * inner join * / \\ * plan0 plan1 * * Note: This pattern currently only works for left-deep trees. */ object ExtractFiltersAndInnerJoins extends PredicateHelper { /** * Flatten all inner joins, which are next to each other. * Return a list of logical plans to be joined with a boolean for each plan indicating if it * was involved in an explicit cross join. Also returns the entire list of join conditions for * the left-deep tree. */ def flattenJoin(plan: LogicalPlan, parentJoinType: InnerLike = Inner) : (Seq[(LogicalPlan, InnerLike)], Seq[Expression]) = plan match { case Join(left, right, joinType: InnerLike, cond, hint) if hint == JoinHint.NONE => val (plans, conditions) = flattenJoin(left, joinType) (plans ++ Seq((right, joinType)), conditions ++ cond.toSeq.flatMap(splitConjunctivePredicates)) case Filter(filterCondition, j @ Join(_, _, _: InnerLike, _, hint)) if hint == JoinHint.NONE => val (plans, conditions) = flattenJoin(j) (plans, conditions ++ splitConjunctivePredicates(filterCondition)) case _ => (Seq((plan, parentJoinType)), Seq.empty) } def unapply(plan: LogicalPlan) : Option[(Seq[(LogicalPlan, InnerLike)], Seq[Expression])] = plan match { case f @ Filter(filterCondition, j @ Join(_, _, joinType: InnerLike, _, hint)) if hint == JoinHint.NONE => Some(flattenJoin(f)) case j @ Join(_, _, joinType, _, hint) if hint == JoinHint.NONE => Some(flattenJoin(j)) case _ => None } }","title":"Join"},{"location":"SparkSQL/Join/#join","text":"","title":"Join"},{"location":"SparkSQL/Join/#optimizer-re-order-join","text":"","title":"Optimizer: re-order Join"},{"location":"SparkSQL/Join/#jira","text":"SPARK-12032 Filter can\u2019t be pushed down to correct Join because of bad order of Join code: PR-10073 ref code: PR-10258 For this query: select d.d_year, count(*) cnt FROM store_sales, date_dim d, customer c WHERE ss_customer_sk = c.c_customer_sk AND c.c_first_shipto_date_sk = d.d_date_sk group by d.d_year Current optimized plan is == Optimized Logical Plan == Aggregate [d_year#147], [d_year#147,(count(1),mode=Complete,isDistinct=false) AS cnt#425L] Project [d_year#147] Join Inner, Some(((ss_customer_sk#283 = c_customer_sk#101) && (c_first_shipto_date_sk#106 = d_date_sk#141))) Project [d_date_sk#141,d_year#147,ss_customer_sk#283] Join Inner, None Project [ss_customer_sk#283] Relation[] ParquetRelation[store_sales] Project [d_date_sk#141,d_year#147] Relation[] ParquetRelation[date_dim] Project [c_customer_sk#101,c_first_shipto_date_sk#106] Relation[] ParquetRelation[customer] It will join store_sales and date_dim together without any condition , the condition c.c_first_shipto_date_sk = d.d_date_sk is not pushed to it because the bad order of joins. The optimizer should re-order the joins, join date_dim after customer, then it can pushed down the condition correctly. The plan should be Aggregate [d_year#147], [d_year#147,(count(1),mode=Complete,isDistinct=false) AS cnt#425L] Project [d_year#147] Join Inner, Some((c_first_shipto_date_sk#106 = d_date_sk#141)) Project [c_first_shipto_date_sk#106] Join Inner, Some((ss_customer_sk#283 = c_customer_sk#101)) Project [ss_customer_sk#283] Relation[store_sales] Project [c_first_shipto_date_sk#106,c_customer_sk#101] Relation[customer] Project [d_year#147,d_date_sk#141] Relation[date_dim]","title":"JIRA"},{"location":"SparkSQL/Join/#code","text":"org.apache.spark.sql.catalyst.optimizer.Optimizer#defaultBatches /** * Defines the default rule batches in the Optimizer. * * Implementations of this class should override this method, and [[nonExcludableRules]] if * necessary, instead of [[batches]]. The rule batches that eventually run in the Optimizer, * i.e., returned by [[batches]], will be (defaultBatches - (excludedRules - nonExcludableRules)). */ def defaultBatches: Seq[Batch] = { val operatorOptimizationRuleSet = Seq( // Operator push down PushProjectionThroughUnion, ReorderJoin, ==> ReorderJoin rules org.apache.spark.sql.catalyst.optimizer.ReorderJoin /** * Reorder the joins and push all the conditions into join, so that the bottom ones have at least * one condition. * * The order of joins will not be changed if all of them already have at least one condition. * * If star schema detection is enabled, reorder the star join plans based on heuristics. */ object ReorderJoin extends Rule[LogicalPlan] with PredicateHelper { /** * Join a list of plans together and push down the conditions into them. * * The joined plan are picked from left to right, prefer those has at least one join condition. * * @param input a list of LogicalPlans to inner join and the type of inner join. * @param conditions a list of condition for join. */ @tailrec final def createOrderedJoin( input: Seq[(LogicalPlan, InnerLike)], conditions: Seq[Expression]): LogicalPlan = { def apply(plan: LogicalPlan): LogicalPlan = plan.transformWithPruning( _.containsPattern(INNER_LIKE_JOIN), ruleId) { case p @ ExtractFiltersAndInnerJoins(input, conditions) if input.size > 2 && conditions.nonEmpty => val reordered = if (conf.starSchemaDetection && !conf.cboEnabled) { val starJoinPlan = StarSchemaDetection.reorderStarJoins(input, conditions) if (starJoinPlan.nonEmpty) { val rest = input.filterNot(starJoinPlan.contains(_)) createOrderedJoin(starJoinPlan ++ rest, conditions) } else { createOrderedJoin(input, conditions) } } else { createOrderedJoin(input, conditions) } if (p.sameOutput(reordered)) { reordered } else { // Reordering the joins have changed the order of the columns. // Inject a projection to make sure we restore to the expected ordering. Project(p.output, reordered) } } org.apache.spark.sql.catalyst.planning.ExtractFiltersAndInnerJoins /** * A pattern that collects the filter and inner joins. * * Filter * | * inner Join * / \\ ----> (Seq(plan0, plan1, plan2), conditions) * Filter plan2 * | * inner join * / \\ * plan0 plan1 * * Note: This pattern currently only works for left-deep trees. */ object ExtractFiltersAndInnerJoins extends PredicateHelper { /** * Flatten all inner joins, which are next to each other. * Return a list of logical plans to be joined with a boolean for each plan indicating if it * was involved in an explicit cross join. Also returns the entire list of join conditions for * the left-deep tree. */ def flattenJoin(plan: LogicalPlan, parentJoinType: InnerLike = Inner) : (Seq[(LogicalPlan, InnerLike)], Seq[Expression]) = plan match { case Join(left, right, joinType: InnerLike, cond, hint) if hint == JoinHint.NONE => val (plans, conditions) = flattenJoin(left, joinType) (plans ++ Seq((right, joinType)), conditions ++ cond.toSeq.flatMap(splitConjunctivePredicates)) case Filter(filterCondition, j @ Join(_, _, _: InnerLike, _, hint)) if hint == JoinHint.NONE => val (plans, conditions) = flattenJoin(j) (plans, conditions ++ splitConjunctivePredicates(filterCondition)) case _ => (Seq((plan, parentJoinType)), Seq.empty) } def unapply(plan: LogicalPlan) : Option[(Seq[(LogicalPlan, InnerLike)], Seq[Expression])] = plan match { case f @ Filter(filterCondition, j @ Join(_, _, joinType: InnerLike, _, hint)) if hint == JoinHint.NONE => Some(flattenJoin(f)) case j @ Join(_, _, joinType, _, hint) if hint == JoinHint.NONE => Some(flattenJoin(j)) case _ => None } }","title":"Code"},{"location":"SparkSQL/Pivot/","text":"Pivot JIRA code Pivot \u00b6 JIRA \u00b6 SPARK-8992 Add Pivot functionality to Spark SQL SPARK-13749 Faster pivot implementation for many distinct values with two phase aggregation code \u00b6 org.apache.spark.sql.catalyst.analysis.Analyzer#batches override def batches: Seq[Batch] = Seq( ... Batch(\"Resolution\", fixedPoint, ResolveTableValuedFunctions(v1SessionCatalog) :: ... ResolvePivot :: ResolveOrdinalInOrderByAndGroupBy :: ResolveAggAliasInGroupBy :: ResolveMissingReferences :: ExtractGenerator :: ResolveGenerate :: ResolveFunctions :: ResolveAliases :: ResolveSubquery :: ... org.apache.spark.sql.catalyst.analysis.Analyzer.ResolvePivot object ResolvePivot extends Rule[LogicalPlan] with AliasHelper { def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsWithPruning( org.apache.spark.sql.catalyst.analysis.Analyzer.ResolveAliases object ResolveAliases extends Rule[LogicalPlan] { def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUpWithPruning( _.containsPattern(UNRESOLVED_ALIAS), ruleId) { case Aggregate(groups, aggs, child) if child.resolved && hasUnresolvedAlias(aggs) => Aggregate(groups, assignAliases(aggs), child) case Pivot(groupByOpt, pivotColumn, pivotValues, aggregates, child) if child.resolved && groupByOpt.isDefined && hasUnresolvedAlias(groupByOpt.get) => Pivot(Some(assignAliases(groupByOpt.get)), pivotColumn, pivotValues, aggregates, child) org.apache.spark.sql.catalyst.plans.logical.Pivot /** * A constructor for creating a pivot, which will later be converted to a [[Project]] * or an [[Aggregate]] during the query analysis. * * @param groupByExprsOpt A sequence of group by expressions. This field should be None if coming * from SQL, in which group by expressions are not explicitly specified. * @param pivotColumn The pivot column. * @param pivotValues A sequence of values for the pivot column. * @param aggregates The aggregation expressions, each with or without an alias. * @param child Child operator */ case class Pivot( groupByExprsOpt: Option[Seq[NamedExpression]], pivotColumn: Expression, pivotValues: Seq[Expression], aggregates: Seq[Expression], child: LogicalPlan) extends UnaryNode { org.apache.spark.sql.RelationalGroupedDataset /** * A set of methods for aggregations on a `DataFrame`, created by [[Dataset#groupBy groupBy]], * [[Dataset#cube cube]] or [[Dataset#rollup rollup]] (and also `pivot`). * * The main method is the `agg` function, which has multiple variants. This class also contains * some first-order statistics such as `mean`, `sum` for convenience. * * @note This class was named `GroupedData` in Spark 1.x. * * @since 2.0.0 */ @Stable class RelationalGroupedDataset protected[sql]( private[sql] val df: DataFrame, private[sql] val groupingExprs: Seq[Expression], groupType: RelationalGroupedDataset.GroupType) { /** * Pivots a column of the current `DataFrame` and performs the specified aggregation. * * There are two versions of `pivot` function: one that requires the caller to specify the list * of distinct values to pivot on, and one that does not. The latter is more concise but less * efficient, because Spark needs to first compute the list of distinct values internally. * * {{{ * // Compute the sum of earnings for each year by course with each course as a separate column * df.groupBy(\"year\").pivot(\"course\", Seq(\"dotNET\", \"Java\")).sum(\"earnings\") * * // Or without specifying column values (less efficient) * df.groupBy(\"year\").pivot(\"course\").sum(\"earnings\") * }}} * * @param pivotColumn Name of the column to pivot. * @since 1.6.0 */ def pivot(pivotColumn: String): RelationalGroupedDataset = pivot(Column(pivotColumn)) /** * Pivots a column of the current `DataFrame` and performs the specified aggregation. * There are two versions of pivot function: one that requires the caller to specify the list * of distinct values to pivot on, and one that does not. The latter is more concise but less * efficient, because Spark needs to first compute the list of distinct values internally. * * {{{ * // Compute the sum of earnings for each year by course with each course as a separate column * df.groupBy(\"year\").pivot(\"course\", Seq(\"dotNET\", \"Java\")).sum(\"earnings\") * * // Or without specifying column values (less efficient) * df.groupBy(\"year\").pivot(\"course\").sum(\"earnings\") * }}} * * From Spark 3.0.0, values can be literal columns, for instance, struct. For pivoting by * multiple columns, use the `struct` function to combine the columns and values: * * {{{ * df.groupBy(\"year\") * .pivot(\"trainingCourse\", Seq(struct(lit(\"java\"), lit(\"Experts\")))) * .agg(sum($\"earnings\")) * }}} * * @param pivotColumn Name of the column to pivot. * @param values List of values that will be translated to columns in the output DataFrame. * @since 1.6.0 */ def pivot(pivotColumn: String, values: Seq[Any]): RelationalGroupedDataset = { pivot(Column(pivotColumn), values) } /** * (Java-specific) Pivots a column of the current `DataFrame` and performs the specified * aggregation. * * There are two versions of pivot function: one that requires the caller to specify the list * of distinct values to pivot on, and one that does not. The latter is more concise but less * efficient, because Spark needs to first compute the list of distinct values internally. * * {{{ * // Compute the sum of earnings for each year by course with each course as a separate column * df.groupBy(\"year\").pivot(\"course\", Arrays.<Object>asList(\"dotNET\", \"Java\")).sum(\"earnings\"); * * // Or without specifying column values (less efficient) * df.groupBy(\"year\").pivot(\"course\").sum(\"earnings\"); * }}} * * @param pivotColumn Name of the column to pivot. * @param values List of values that will be translated to columns in the output DataFrame. * @since 1.6.0 */ def pivot(pivotColumn: String, values: java.util.List[Any]): RelationalGroupedDataset = { pivot(Column(pivotColumn), values) } /** * Pivots a column of the current `DataFrame` and performs the specified aggregation. * This is an overloaded version of the `pivot` method with `pivotColumn` of the `String` type. * * {{{ * // Or without specifying column values (less efficient) * df.groupBy($\"year\").pivot($\"course\").sum($\"earnings\"); * }}} * * @param pivotColumn he column to pivot. * @since 2.4.0 */ def pivot(pivotColumn: Column): RelationalGroupedDataset = { // This is to prevent unintended OOM errors when the number of distinct values is large val maxValues = df.sparkSession.sessionState.conf.dataFramePivotMaxValues // Get the distinct values of the column and sort them so its consistent val values = df.select(pivotColumn) .distinct() .limit(maxValues + 1) .sort(pivotColumn) // ensure that the output columns are in a consistent logical order .collect() .map(_.get(0)) .toSeq if (values.length > maxValues) { throw QueryCompilationErrors.aggregationFunctionAppliedOnNonNumericColumnError( pivotColumn.toString, maxValues) } pivot(pivotColumn, values) } /** * Pivots a column of the current `DataFrame` and performs the specified aggregation. * This is an overloaded version of the `pivot` method with `pivotColumn` of the `String` type. * * {{{ * // Compute the sum of earnings for each year by course with each course as a separate column * df.groupBy($\"year\").pivot($\"course\", Seq(\"dotNET\", \"Java\")).sum($\"earnings\") * }}} * * @param pivotColumn the column to pivot. * @param values List of values that will be translated to columns in the output DataFrame. * @since 2.4.0 */ def pivot(pivotColumn: Column, values: Seq[Any]): RelationalGroupedDataset = { groupType match { case RelationalGroupedDataset.GroupByType => val valueExprs = values.map(_ match { case c: Column => c.expr case v => try { Literal.apply(v) } catch { case _: SparkRuntimeException => throw QueryExecutionErrors.pivotColumnUnsupportedError(v, pivotColumn.expr.dataType) } }) new RelationalGroupedDataset( df, groupingExprs, RelationalGroupedDataset.PivotType(pivotColumn.expr, valueExprs)) case _: RelationalGroupedDataset.PivotType => throw QueryExecutionErrors.repeatedPivotsUnsupportedError() case _ => throw QueryExecutionErrors.pivotNotAfterGroupByUnsupportedError() } } /** * (Java-specific) Pivots a column of the current `DataFrame` and performs the specified * aggregation. This is an overloaded version of the `pivot` method with `pivotColumn` of * the `String` type. * * @param pivotColumn the column to pivot. * @param values List of values that will be translated to columns in the output DataFrame. * @since 2.4.0 */ def pivot(pivotColumn: Column, values: java.util.List[Any]): RelationalGroupedDataset = { pivot(pivotColumn, values.asScala.toSeq) } org.apache.spark.sql.RelationalGroupedDataset.PivotType private[sql] case class PivotType(pivotCol: Expression, values: Seq[Expression]) extends GroupType","title":"Pivot"},{"location":"SparkSQL/Pivot/#pivot","text":"","title":"Pivot"},{"location":"SparkSQL/Pivot/#jira","text":"SPARK-8992 Add Pivot functionality to Spark SQL SPARK-13749 Faster pivot implementation for many distinct values with two phase aggregation","title":"JIRA"},{"location":"SparkSQL/Pivot/#code","text":"org.apache.spark.sql.catalyst.analysis.Analyzer#batches override def batches: Seq[Batch] = Seq( ... Batch(\"Resolution\", fixedPoint, ResolveTableValuedFunctions(v1SessionCatalog) :: ... ResolvePivot :: ResolveOrdinalInOrderByAndGroupBy :: ResolveAggAliasInGroupBy :: ResolveMissingReferences :: ExtractGenerator :: ResolveGenerate :: ResolveFunctions :: ResolveAliases :: ResolveSubquery :: ... org.apache.spark.sql.catalyst.analysis.Analyzer.ResolvePivot object ResolvePivot extends Rule[LogicalPlan] with AliasHelper { def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsWithPruning( org.apache.spark.sql.catalyst.analysis.Analyzer.ResolveAliases object ResolveAliases extends Rule[LogicalPlan] { def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUpWithPruning( _.containsPattern(UNRESOLVED_ALIAS), ruleId) { case Aggregate(groups, aggs, child) if child.resolved && hasUnresolvedAlias(aggs) => Aggregate(groups, assignAliases(aggs), child) case Pivot(groupByOpt, pivotColumn, pivotValues, aggregates, child) if child.resolved && groupByOpt.isDefined && hasUnresolvedAlias(groupByOpt.get) => Pivot(Some(assignAliases(groupByOpt.get)), pivotColumn, pivotValues, aggregates, child) org.apache.spark.sql.catalyst.plans.logical.Pivot /** * A constructor for creating a pivot, which will later be converted to a [[Project]] * or an [[Aggregate]] during the query analysis. * * @param groupByExprsOpt A sequence of group by expressions. This field should be None if coming * from SQL, in which group by expressions are not explicitly specified. * @param pivotColumn The pivot column. * @param pivotValues A sequence of values for the pivot column. * @param aggregates The aggregation expressions, each with or without an alias. * @param child Child operator */ case class Pivot( groupByExprsOpt: Option[Seq[NamedExpression]], pivotColumn: Expression, pivotValues: Seq[Expression], aggregates: Seq[Expression], child: LogicalPlan) extends UnaryNode { org.apache.spark.sql.RelationalGroupedDataset /** * A set of methods for aggregations on a `DataFrame`, created by [[Dataset#groupBy groupBy]], * [[Dataset#cube cube]] or [[Dataset#rollup rollup]] (and also `pivot`). * * The main method is the `agg` function, which has multiple variants. This class also contains * some first-order statistics such as `mean`, `sum` for convenience. * * @note This class was named `GroupedData` in Spark 1.x. * * @since 2.0.0 */ @Stable class RelationalGroupedDataset protected[sql]( private[sql] val df: DataFrame, private[sql] val groupingExprs: Seq[Expression], groupType: RelationalGroupedDataset.GroupType) { /** * Pivots a column of the current `DataFrame` and performs the specified aggregation. * * There are two versions of `pivot` function: one that requires the caller to specify the list * of distinct values to pivot on, and one that does not. The latter is more concise but less * efficient, because Spark needs to first compute the list of distinct values internally. * * {{{ * // Compute the sum of earnings for each year by course with each course as a separate column * df.groupBy(\"year\").pivot(\"course\", Seq(\"dotNET\", \"Java\")).sum(\"earnings\") * * // Or without specifying column values (less efficient) * df.groupBy(\"year\").pivot(\"course\").sum(\"earnings\") * }}} * * @param pivotColumn Name of the column to pivot. * @since 1.6.0 */ def pivot(pivotColumn: String): RelationalGroupedDataset = pivot(Column(pivotColumn)) /** * Pivots a column of the current `DataFrame` and performs the specified aggregation. * There are two versions of pivot function: one that requires the caller to specify the list * of distinct values to pivot on, and one that does not. The latter is more concise but less * efficient, because Spark needs to first compute the list of distinct values internally. * * {{{ * // Compute the sum of earnings for each year by course with each course as a separate column * df.groupBy(\"year\").pivot(\"course\", Seq(\"dotNET\", \"Java\")).sum(\"earnings\") * * // Or without specifying column values (less efficient) * df.groupBy(\"year\").pivot(\"course\").sum(\"earnings\") * }}} * * From Spark 3.0.0, values can be literal columns, for instance, struct. For pivoting by * multiple columns, use the `struct` function to combine the columns and values: * * {{{ * df.groupBy(\"year\") * .pivot(\"trainingCourse\", Seq(struct(lit(\"java\"), lit(\"Experts\")))) * .agg(sum($\"earnings\")) * }}} * * @param pivotColumn Name of the column to pivot. * @param values List of values that will be translated to columns in the output DataFrame. * @since 1.6.0 */ def pivot(pivotColumn: String, values: Seq[Any]): RelationalGroupedDataset = { pivot(Column(pivotColumn), values) } /** * (Java-specific) Pivots a column of the current `DataFrame` and performs the specified * aggregation. * * There are two versions of pivot function: one that requires the caller to specify the list * of distinct values to pivot on, and one that does not. The latter is more concise but less * efficient, because Spark needs to first compute the list of distinct values internally. * * {{{ * // Compute the sum of earnings for each year by course with each course as a separate column * df.groupBy(\"year\").pivot(\"course\", Arrays.<Object>asList(\"dotNET\", \"Java\")).sum(\"earnings\"); * * // Or without specifying column values (less efficient) * df.groupBy(\"year\").pivot(\"course\").sum(\"earnings\"); * }}} * * @param pivotColumn Name of the column to pivot. * @param values List of values that will be translated to columns in the output DataFrame. * @since 1.6.0 */ def pivot(pivotColumn: String, values: java.util.List[Any]): RelationalGroupedDataset = { pivot(Column(pivotColumn), values) } /** * Pivots a column of the current `DataFrame` and performs the specified aggregation. * This is an overloaded version of the `pivot` method with `pivotColumn` of the `String` type. * * {{{ * // Or without specifying column values (less efficient) * df.groupBy($\"year\").pivot($\"course\").sum($\"earnings\"); * }}} * * @param pivotColumn he column to pivot. * @since 2.4.0 */ def pivot(pivotColumn: Column): RelationalGroupedDataset = { // This is to prevent unintended OOM errors when the number of distinct values is large val maxValues = df.sparkSession.sessionState.conf.dataFramePivotMaxValues // Get the distinct values of the column and sort them so its consistent val values = df.select(pivotColumn) .distinct() .limit(maxValues + 1) .sort(pivotColumn) // ensure that the output columns are in a consistent logical order .collect() .map(_.get(0)) .toSeq if (values.length > maxValues) { throw QueryCompilationErrors.aggregationFunctionAppliedOnNonNumericColumnError( pivotColumn.toString, maxValues) } pivot(pivotColumn, values) } /** * Pivots a column of the current `DataFrame` and performs the specified aggregation. * This is an overloaded version of the `pivot` method with `pivotColumn` of the `String` type. * * {{{ * // Compute the sum of earnings for each year by course with each course as a separate column * df.groupBy($\"year\").pivot($\"course\", Seq(\"dotNET\", \"Java\")).sum($\"earnings\") * }}} * * @param pivotColumn the column to pivot. * @param values List of values that will be translated to columns in the output DataFrame. * @since 2.4.0 */ def pivot(pivotColumn: Column, values: Seq[Any]): RelationalGroupedDataset = { groupType match { case RelationalGroupedDataset.GroupByType => val valueExprs = values.map(_ match { case c: Column => c.expr case v => try { Literal.apply(v) } catch { case _: SparkRuntimeException => throw QueryExecutionErrors.pivotColumnUnsupportedError(v, pivotColumn.expr.dataType) } }) new RelationalGroupedDataset( df, groupingExprs, RelationalGroupedDataset.PivotType(pivotColumn.expr, valueExprs)) case _: RelationalGroupedDataset.PivotType => throw QueryExecutionErrors.repeatedPivotsUnsupportedError() case _ => throw QueryExecutionErrors.pivotNotAfterGroupByUnsupportedError() } } /** * (Java-specific) Pivots a column of the current `DataFrame` and performs the specified * aggregation. This is an overloaded version of the `pivot` method with `pivotColumn` of * the `String` type. * * @param pivotColumn the column to pivot. * @param values List of values that will be translated to columns in the output DataFrame. * @since 2.4.0 */ def pivot(pivotColumn: Column, values: java.util.List[Any]): RelationalGroupedDataset = { pivot(pivotColumn, values.asScala.toSeq) } org.apache.spark.sql.RelationalGroupedDataset.PivotType private[sql] case class PivotType(pivotCol: Expression, values: Seq[Expression]) extends GroupType","title":"code"},{"location":"SparkSQL/SparkPlan/","text":"SparkPlan Spark\u2019s Planner Call stack Code of SessionState to create QueryExecution Code of SparkPlan generation Code of prepared SparkPlan generation SparkPlan \u00b6 Spark\u2019s Planner \u00b6 1st Phase: Transforms the logical plan to the physical plan using Strategies QueryExecution.sparkPlan SparkPlanner.plan 2nd Phase: use a Rule Executor to make the Physical Plan ready for execution QueryExecution.prepareForExecution Call stack \u00b6 How is QueryExecution.sparkPlan and QueryExecution.prepareForExecution invoked? QueryExecution.sparkPlan stack at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:141)=====> lazy val sparkPlan: SparkPlan - locked <0x328c> (a org.apache.spark.sql.execution.QueryExecution) at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:138) at org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:158) at org.apache.spark.sql.execution.QueryExecution$$Lambda$1861.30604162.apply(Unknown Source:-1) at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111) at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185) at org.apache.spark.sql.execution.QueryExecution$$Lambda$1461.609375192.apply(Unknown Source:-1) at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:512) at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185) at org.apache.spark.sql.execution.QueryExecution$$Lambda$1460.911201454.apply(Unknown Source:-1) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184) at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:158) at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151) at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:204) at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:251) at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:220) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103) at org.apache.spark.sql.execution.SQLExecution$$$Lambda$1698.2050525584.apply(Unknown Source:-1) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:171) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95) at org.apache.spark.sql.execution.SQLExecution$$$Lambda$1682.2000856156.apply(Unknown Source:-1) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3924) at org.apache.spark.sql.Dataset.collect(Dataset.scala:3188) org.apache.spark.sql.Dataset class Dataset[T] private[sql]( @DeveloperApi @Unstable @transient val queryExecution: QueryExecution, @DeveloperApi @Unstable @transient val encoder: Encoder[T]) extends Serializable { // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure // you wrap it with `withNewExecutionId` if this actions doesn't call other action. def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sparkSession.sessionState.executePlan(logicalPlan), encoder) ====> executePlan() is to create the QueryExecution } def collect(): Array[T] = withAction(\"collect\", queryExecution)(collectFromPlan) private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = { SQLExecution.withNewExecutionId(qe, Some(name)) { QueryExecution.withInternalError(s\"\"\"The \"$name\" action failed.\"\"\") { qe.executedPlan.resetMetrics() action(qe.executedPlan) } } } org.apache.spark.sql.execution.SQLExecution#withNewExecutionId /** * Wrap an action that will execute \"queryExecution\" to track all Spark jobs in the body so that * we can connect them with an execution. */ def withNewExecutionId[T]( queryExecution: QueryExecution, name: Option[String] = None)(body: => T): T = queryExecution.sparkSession.withActive { org.apache.spark.sql.execution.QueryExecution#explainString def explainString( mode: ExplainMode, maxFields: Int = SQLConf.get.maxToStringFields): String = { val concat = new PlanStringConcat() explainString(mode, maxFields, concat.append) withRedaction { concat.toString } } org.apache.spark.sql.execution.QueryExecution#executedPlan // executedPlan should not be used to initialize any SparkPlan. It should be // only used for execution. =====> ??? lazy val executedPlan: SparkPlan = { // We need to materialize the optimizedPlan here, before tracking the planning phase, to ensure // that the optimization time is not counted as part of the planning phase. assertOptimized() executePhase(QueryPlanningTracker.PLANNING) { // clone the plan to avoid sharing the plan instance between different stages like analyzing, // optimizing and planning. QueryExecution.prepareForExecution(preparations, sparkPlan.clone()) =====> sparkPlan is initialized here and then call QueryExecution.prepareForExecution } } Code of SessionState to create QueryExecution \u00b6 org.apache.spark.sql.SparkSession#sessionState /** * State isolated across sessions, including SQL configurations, temporary tables, registered * functions, and everything else that accepts a [[org.apache.spark.sql.internal.SQLConf]]. * If `parentSessionState` is not null, the `SessionState` will be a copy of the parent. * * This is internal to Spark and there is no guarantee on interface stability. * * @since 2.2.0 */ @Unstable @transient lazy val sessionState: SessionState = { parentSessionState .map(_.clone(this)) .getOrElse { val state = SparkSession.instantiateSessionState( SparkSession.sessionStateClassName(sharedState.conf), self) state } } org.apache.spark.sql.SparkSession#instantiateSessionState /** * Helper method to create an instance of `SessionState` based on `className` from conf. * The result is either `SessionState` or a Hive based `SessionState`. */ private def instantiateSessionState( className: String, sparkSession: SparkSession): SessionState = { try { // invoke new [Hive]SessionStateBuilder( // SparkSession, // Option[SessionState]) val clazz = Utils.classForName(className) val ctor = clazz.getConstructors.head ctor.newInstance(sparkSession, None).asInstanceOf[BaseSessionStateBuilder].build() } catch { case NonFatal(e) => throw new IllegalArgumentException(s\"Error while instantiating '$className':\", e) } } org.apache.spark.sql.SparkSession#sessionStateClassName private val HIVE_SESSION_STATE_BUILDER_CLASS_NAME = \"org.apache.spark.sql.hive.HiveSessionStateBuilder\" private def sessionStateClassName(conf: SparkConf): String = { conf.get(CATALOG_IMPLEMENTATION) match { case \"hive\" => HIVE_SESSION_STATE_BUILDER_CLASS_NAME case \"in-memory\" => classOf[SessionStateBuilder].getCanonicalName } } class SessionStateBuilder( session: SparkSession, parentState: Option[SessionState]) extends BaseSessionStateBuilder(session, parentState) { override protected def newBuilder: NewBuilder = new SessionStateBuilder(_, _) } or Hive /** * Builder that produces a Hive-aware `SessionState`. */ class HiveSessionStateBuilder( session: SparkSession, parentState: Option[SessionState]) extends BaseSessionStateBuilder(session, parentState) { org.apache.spark.sql.internal.BaseSessionStateBuilder#build def build(): SessionState = { new SessionState( session.sharedState, conf, experimentalMethods, functionRegistry, tableFunctionRegistry, udfRegistration, () => catalog, sqlParser, () => analyzer, () => optimizer, planner, () => streamingQueryManager, listenerManager, () => resourceLoader, createQueryExecution, createClone, columnarRules, adaptiveRulesHolder) } } org.apache.spark.sql.internal.BaseSessionStateBuilder#createQueryExecution protected def createQueryExecution: (LogicalPlan, CommandExecutionMode.Value) => QueryExecution = (plan, mode) => new QueryExecution(session, plan, mode = mode) org.apache.spark.sql.execution.QueryExecution /** * The primary workflow for executing relational queries using Spark. Designed to allow easy * access to the intermediate phases of query execution for developers. * * While this is not a public class, we should avoid changing the function names for the sake of * changing them, because a lot of developers use the feature for debugging. */ class QueryExecution( val sparkSession: SparkSession, val logical: LogicalPlan, val tracker: QueryPlanningTracker = new QueryPlanningTracker, val mode: CommandExecutionMode.Value = CommandExecutionMode.ALL) extends Logging { Code of SparkPlan generation \u00b6 QueryExecution.sparkPlan => QueryExecution.createSparkPlan SparkPlanner.plan (strategies defined in SparkPlanner) => SparkStrategies.plan => QueryPlanner.plan org.apache.spark.sql.execution.QueryExecution /** * The primary workflow for executing relational queries using Spark. Designed to allow easy * access to the intermediate phases of query execution for developers. * * While this is not a public class, we should avoid changing the function names for the sake of * changing them, because a lot of developers use the feature for debugging. */ class QueryExecution( val sparkSession: SparkSession, val logical: LogicalPlan, val tracker: QueryPlanningTracker = new QueryPlanningTracker, val mode: CommandExecutionMode.Value = CommandExecutionMode.ALL) extends Logging { lazy val sparkPlan: SparkPlan = { // We need to materialize the optimizedPlan here because sparkPlan is also tracked under // the planning phase assertOptimized() executePhase(QueryPlanningTracker.PLANNING) { // Clone the logical plan here, in case the planner rules change the states of the logical // plan. QueryExecution.createSparkPlan(sparkSession, planner, optimizedPlan.clone()) } } /** * Transform a [[LogicalPlan]] into a [[SparkPlan]]. * * Note that the returned physical plan still needs to be prepared for execution. */ def createSparkPlan( sparkSession: SparkSession, planner: SparkPlanner, plan: LogicalPlan): SparkPlan = { // TODO: We use next(), i.e. take the first plan returned by the planner, here for now, // but we will implement to choose the best plan. planner.plan(ReturnAnswer(plan)).next() } org.apache.spark.sql.execution.SparkPlanner class SparkPlanner(val session: SparkSession, val experimentalMethods: ExperimentalMethods) extends SparkStrategies with SQLConfHelper { override def strategies: Seq[Strategy] = experimentalMethods.extraStrategies ++ extraPlanningStrategies ++ ( LogicalQueryStageStrategy :: PythonEvals :: new DataSourceV2Strategy(session) :: FileSourceStrategy :: DataSourceStrategy :: SpecialLimits :: Aggregation :: Window :: JoinSelection :: InMemoryScans :: SparkScripts :: BasicOperators :: Nil) abstract class SparkStrategies extends QueryPlanner[SparkPlan] { self: SparkPlanner => override def plan(plan: LogicalPlan): Iterator[SparkPlan] = { super.plan(plan).map { p => val logicalPlan = plan match { case ReturnAnswer(rootPlan) => rootPlan case _ => plan } p.setLogicalLink(logicalPlan) p } } org.apache.spark.sql.catalyst.planning.QueryPlanner /** * Abstract class for transforming [[LogicalPlan]]s into physical plans. * Child classes are responsible for specifying a list of [[GenericStrategy]] objects that * each of which can return a list of possible physical plan options. * If a given strategy is unable to plan all of the remaining operators in the tree, * it can call [[GenericStrategy#planLater planLater]], which returns a placeholder * object that will be [[collectPlaceholders collected]] and filled in * using other available strategies. * * TODO: RIGHT NOW ONLY ONE PLAN IS RETURNED EVER... * PLAN SPACE EXPLORATION WILL BE IMPLEMENTED LATER. * * @tparam PhysicalPlan The type of physical plan produced by this [[QueryPlanner]] */ abstract class QueryPlanner[PhysicalPlan <: TreeNode[PhysicalPlan]] { /** A list of execution strategies that can be used by the planner */ def strategies: Seq[GenericStrategy[PhysicalPlan]] def plan(plan: LogicalPlan): Iterator[PhysicalPlan] = { // Obviously a lot to do here still... // Collect physical plan candidates. val candidates = strategies.iterator.flatMap(_(plan)) // The candidates may contain placeholders marked as [[planLater]], // so try to replace them by their child plans. val plans = candidates.flatMap { candidate => val placeholders = collectPlaceholders(candidate) if (placeholders.isEmpty) { // Take the candidate as is because it does not contain placeholders. Iterator(candidate) } else { // Plan the logical plan marked as [[planLater]] and replace the placeholders. placeholders.iterator.foldLeft(Iterator(candidate)) { case (candidatesWithPlaceholders, (placeholder, logicalPlan)) => // Plan the logical plan for the placeholder. val childPlans = this.plan(logicalPlan) ====> if there is planLater, recursively apply all the strategies again candidatesWithPlaceholders.flatMap { candidateWithPlaceholders => childPlans.map { childPlan => // Replace the placeholder by the child plan candidateWithPlaceholders.transformUp { case p if p.eq(placeholder) => childPlan } } } } } } val pruned = prunePlans(plans) assert(pruned.hasNext, s\"No plan for $plan\") pruned Code of prepared SparkPlan generation \u00b6 In QueryExecution.prepareForExecution(), rules (Rule[SparkPlan]) are applied => rules are defined in QueryExecution.preparations() org.apache.spark.sql.execution.QueryExecution#prepareForExecution protected def preparations: Seq[Rule[SparkPlan]] = { QueryExecution.preparations(sparkSession, Option(InsertAdaptiveSparkPlan(AdaptiveExecutionContext(sparkSession, this))), false) } /** * Construct a sequence of rules that are used to prepare a planned [[SparkPlan]] for execution. * These rules will make sure subqueries are planned, make use the data partitioning and ordering * are correct, insert whole stage code gen, and try to reduce the work done by reusing exchanges * and subqueries. */ private[execution] def preparations( sparkSession: SparkSession, adaptiveExecutionRule: Option[InsertAdaptiveSparkPlan] = None, subquery: Boolean): Seq[Rule[SparkPlan]] = { // `AdaptiveSparkPlanExec` is a leaf node. If inserted, all the following rules will be no-op // as the original plan is hidden behind `AdaptiveSparkPlanExec`. adaptiveExecutionRule.toSeq ++ Seq( CoalesceBucketsInJoin, PlanDynamicPruningFilters(sparkSession), PlanSubqueries(sparkSession), RemoveRedundantProjects, EnsureRequirements(), // `ReplaceHashWithSortAgg` needs to be added after `EnsureRequirements` to guarantee the // sort order of each node is checked to be valid. ReplaceHashWithSortAgg, // `RemoveRedundantSorts` needs to be added after `EnsureRequirements` to guarantee the same // number of partitions when instantiating PartitioningCollection. RemoveRedundantSorts, DisableUnnecessaryBucketedScan, ApplyColumnarRulesAndInsertTransitions( sparkSession.sessionState.columnarRules, outputsColumnar = false), CollapseCodegenStages()) ++ (if (subquery) { Nil } else { Seq(ReuseExchangeAndSubquery) }) } /** * Prepares a planned [[SparkPlan]] for execution by inserting shuffle operations and internal * row format conversions as needed. */ private[execution] def prepareForExecution( preparations: Seq[Rule[SparkPlan]], plan: SparkPlan): SparkPlan = { val planChangeLogger = new PlanChangeLogger[SparkPlan]() val preparedPlan = preparations.foldLeft(plan) { case (sp, rule) => val result = rule.apply(sp) planChangeLogger.logRule(rule.ruleName, sp, result) result } planChangeLogger.logBatch(\"Preparations\", plan, preparedPlan) preparedPlan }","title":"SparkPlan"},{"location":"SparkSQL/SparkPlan/#sparkplan","text":"","title":"SparkPlan"},{"location":"SparkSQL/SparkPlan/#sparks-planner","text":"1st Phase: Transforms the logical plan to the physical plan using Strategies QueryExecution.sparkPlan SparkPlanner.plan 2nd Phase: use a Rule Executor to make the Physical Plan ready for execution QueryExecution.prepareForExecution","title":"Spark's Planner"},{"location":"SparkSQL/SparkPlan/#call-stack","text":"How is QueryExecution.sparkPlan and QueryExecution.prepareForExecution invoked? QueryExecution.sparkPlan stack at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:141)=====> lazy val sparkPlan: SparkPlan - locked <0x328c> (a org.apache.spark.sql.execution.QueryExecution) at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:138) at org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:158) at org.apache.spark.sql.execution.QueryExecution$$Lambda$1861.30604162.apply(Unknown Source:-1) at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111) at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185) at org.apache.spark.sql.execution.QueryExecution$$Lambda$1461.609375192.apply(Unknown Source:-1) at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:512) at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185) at org.apache.spark.sql.execution.QueryExecution$$Lambda$1460.911201454.apply(Unknown Source:-1) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184) at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:158) at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151) at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:204) at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:251) at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:220) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103) at org.apache.spark.sql.execution.SQLExecution$$$Lambda$1698.2050525584.apply(Unknown Source:-1) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:171) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95) at org.apache.spark.sql.execution.SQLExecution$$$Lambda$1682.2000856156.apply(Unknown Source:-1) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3924) at org.apache.spark.sql.Dataset.collect(Dataset.scala:3188) org.apache.spark.sql.Dataset class Dataset[T] private[sql]( @DeveloperApi @Unstable @transient val queryExecution: QueryExecution, @DeveloperApi @Unstable @transient val encoder: Encoder[T]) extends Serializable { // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure // you wrap it with `withNewExecutionId` if this actions doesn't call other action. def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sparkSession.sessionState.executePlan(logicalPlan), encoder) ====> executePlan() is to create the QueryExecution } def collect(): Array[T] = withAction(\"collect\", queryExecution)(collectFromPlan) private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = { SQLExecution.withNewExecutionId(qe, Some(name)) { QueryExecution.withInternalError(s\"\"\"The \"$name\" action failed.\"\"\") { qe.executedPlan.resetMetrics() action(qe.executedPlan) } } } org.apache.spark.sql.execution.SQLExecution#withNewExecutionId /** * Wrap an action that will execute \"queryExecution\" to track all Spark jobs in the body so that * we can connect them with an execution. */ def withNewExecutionId[T]( queryExecution: QueryExecution, name: Option[String] = None)(body: => T): T = queryExecution.sparkSession.withActive { org.apache.spark.sql.execution.QueryExecution#explainString def explainString( mode: ExplainMode, maxFields: Int = SQLConf.get.maxToStringFields): String = { val concat = new PlanStringConcat() explainString(mode, maxFields, concat.append) withRedaction { concat.toString } } org.apache.spark.sql.execution.QueryExecution#executedPlan // executedPlan should not be used to initialize any SparkPlan. It should be // only used for execution. =====> ??? lazy val executedPlan: SparkPlan = { // We need to materialize the optimizedPlan here, before tracking the planning phase, to ensure // that the optimization time is not counted as part of the planning phase. assertOptimized() executePhase(QueryPlanningTracker.PLANNING) { // clone the plan to avoid sharing the plan instance between different stages like analyzing, // optimizing and planning. QueryExecution.prepareForExecution(preparations, sparkPlan.clone()) =====> sparkPlan is initialized here and then call QueryExecution.prepareForExecution } }","title":"Call stack"},{"location":"SparkSQL/SparkPlan/#code-of-sessionstate-to-create-queryexecution","text":"org.apache.spark.sql.SparkSession#sessionState /** * State isolated across sessions, including SQL configurations, temporary tables, registered * functions, and everything else that accepts a [[org.apache.spark.sql.internal.SQLConf]]. * If `parentSessionState` is not null, the `SessionState` will be a copy of the parent. * * This is internal to Spark and there is no guarantee on interface stability. * * @since 2.2.0 */ @Unstable @transient lazy val sessionState: SessionState = { parentSessionState .map(_.clone(this)) .getOrElse { val state = SparkSession.instantiateSessionState( SparkSession.sessionStateClassName(sharedState.conf), self) state } } org.apache.spark.sql.SparkSession#instantiateSessionState /** * Helper method to create an instance of `SessionState` based on `className` from conf. * The result is either `SessionState` or a Hive based `SessionState`. */ private def instantiateSessionState( className: String, sparkSession: SparkSession): SessionState = { try { // invoke new [Hive]SessionStateBuilder( // SparkSession, // Option[SessionState]) val clazz = Utils.classForName(className) val ctor = clazz.getConstructors.head ctor.newInstance(sparkSession, None).asInstanceOf[BaseSessionStateBuilder].build() } catch { case NonFatal(e) => throw new IllegalArgumentException(s\"Error while instantiating '$className':\", e) } } org.apache.spark.sql.SparkSession#sessionStateClassName private val HIVE_SESSION_STATE_BUILDER_CLASS_NAME = \"org.apache.spark.sql.hive.HiveSessionStateBuilder\" private def sessionStateClassName(conf: SparkConf): String = { conf.get(CATALOG_IMPLEMENTATION) match { case \"hive\" => HIVE_SESSION_STATE_BUILDER_CLASS_NAME case \"in-memory\" => classOf[SessionStateBuilder].getCanonicalName } } class SessionStateBuilder( session: SparkSession, parentState: Option[SessionState]) extends BaseSessionStateBuilder(session, parentState) { override protected def newBuilder: NewBuilder = new SessionStateBuilder(_, _) } or Hive /** * Builder that produces a Hive-aware `SessionState`. */ class HiveSessionStateBuilder( session: SparkSession, parentState: Option[SessionState]) extends BaseSessionStateBuilder(session, parentState) { org.apache.spark.sql.internal.BaseSessionStateBuilder#build def build(): SessionState = { new SessionState( session.sharedState, conf, experimentalMethods, functionRegistry, tableFunctionRegistry, udfRegistration, () => catalog, sqlParser, () => analyzer, () => optimizer, planner, () => streamingQueryManager, listenerManager, () => resourceLoader, createQueryExecution, createClone, columnarRules, adaptiveRulesHolder) } } org.apache.spark.sql.internal.BaseSessionStateBuilder#createQueryExecution protected def createQueryExecution: (LogicalPlan, CommandExecutionMode.Value) => QueryExecution = (plan, mode) => new QueryExecution(session, plan, mode = mode) org.apache.spark.sql.execution.QueryExecution /** * The primary workflow for executing relational queries using Spark. Designed to allow easy * access to the intermediate phases of query execution for developers. * * While this is not a public class, we should avoid changing the function names for the sake of * changing them, because a lot of developers use the feature for debugging. */ class QueryExecution( val sparkSession: SparkSession, val logical: LogicalPlan, val tracker: QueryPlanningTracker = new QueryPlanningTracker, val mode: CommandExecutionMode.Value = CommandExecutionMode.ALL) extends Logging {","title":"Code of SessionState to create QueryExecution"},{"location":"SparkSQL/SparkPlan/#code-of-sparkplan-generation","text":"QueryExecution.sparkPlan => QueryExecution.createSparkPlan SparkPlanner.plan (strategies defined in SparkPlanner) => SparkStrategies.plan => QueryPlanner.plan org.apache.spark.sql.execution.QueryExecution /** * The primary workflow for executing relational queries using Spark. Designed to allow easy * access to the intermediate phases of query execution for developers. * * While this is not a public class, we should avoid changing the function names for the sake of * changing them, because a lot of developers use the feature for debugging. */ class QueryExecution( val sparkSession: SparkSession, val logical: LogicalPlan, val tracker: QueryPlanningTracker = new QueryPlanningTracker, val mode: CommandExecutionMode.Value = CommandExecutionMode.ALL) extends Logging { lazy val sparkPlan: SparkPlan = { // We need to materialize the optimizedPlan here because sparkPlan is also tracked under // the planning phase assertOptimized() executePhase(QueryPlanningTracker.PLANNING) { // Clone the logical plan here, in case the planner rules change the states of the logical // plan. QueryExecution.createSparkPlan(sparkSession, planner, optimizedPlan.clone()) } } /** * Transform a [[LogicalPlan]] into a [[SparkPlan]]. * * Note that the returned physical plan still needs to be prepared for execution. */ def createSparkPlan( sparkSession: SparkSession, planner: SparkPlanner, plan: LogicalPlan): SparkPlan = { // TODO: We use next(), i.e. take the first plan returned by the planner, here for now, // but we will implement to choose the best plan. planner.plan(ReturnAnswer(plan)).next() } org.apache.spark.sql.execution.SparkPlanner class SparkPlanner(val session: SparkSession, val experimentalMethods: ExperimentalMethods) extends SparkStrategies with SQLConfHelper { override def strategies: Seq[Strategy] = experimentalMethods.extraStrategies ++ extraPlanningStrategies ++ ( LogicalQueryStageStrategy :: PythonEvals :: new DataSourceV2Strategy(session) :: FileSourceStrategy :: DataSourceStrategy :: SpecialLimits :: Aggregation :: Window :: JoinSelection :: InMemoryScans :: SparkScripts :: BasicOperators :: Nil) abstract class SparkStrategies extends QueryPlanner[SparkPlan] { self: SparkPlanner => override def plan(plan: LogicalPlan): Iterator[SparkPlan] = { super.plan(plan).map { p => val logicalPlan = plan match { case ReturnAnswer(rootPlan) => rootPlan case _ => plan } p.setLogicalLink(logicalPlan) p } } org.apache.spark.sql.catalyst.planning.QueryPlanner /** * Abstract class for transforming [[LogicalPlan]]s into physical plans. * Child classes are responsible for specifying a list of [[GenericStrategy]] objects that * each of which can return a list of possible physical plan options. * If a given strategy is unable to plan all of the remaining operators in the tree, * it can call [[GenericStrategy#planLater planLater]], which returns a placeholder * object that will be [[collectPlaceholders collected]] and filled in * using other available strategies. * * TODO: RIGHT NOW ONLY ONE PLAN IS RETURNED EVER... * PLAN SPACE EXPLORATION WILL BE IMPLEMENTED LATER. * * @tparam PhysicalPlan The type of physical plan produced by this [[QueryPlanner]] */ abstract class QueryPlanner[PhysicalPlan <: TreeNode[PhysicalPlan]] { /** A list of execution strategies that can be used by the planner */ def strategies: Seq[GenericStrategy[PhysicalPlan]] def plan(plan: LogicalPlan): Iterator[PhysicalPlan] = { // Obviously a lot to do here still... // Collect physical plan candidates. val candidates = strategies.iterator.flatMap(_(plan)) // The candidates may contain placeholders marked as [[planLater]], // so try to replace them by their child plans. val plans = candidates.flatMap { candidate => val placeholders = collectPlaceholders(candidate) if (placeholders.isEmpty) { // Take the candidate as is because it does not contain placeholders. Iterator(candidate) } else { // Plan the logical plan marked as [[planLater]] and replace the placeholders. placeholders.iterator.foldLeft(Iterator(candidate)) { case (candidatesWithPlaceholders, (placeholder, logicalPlan)) => // Plan the logical plan for the placeholder. val childPlans = this.plan(logicalPlan) ====> if there is planLater, recursively apply all the strategies again candidatesWithPlaceholders.flatMap { candidateWithPlaceholders => childPlans.map { childPlan => // Replace the placeholder by the child plan candidateWithPlaceholders.transformUp { case p if p.eq(placeholder) => childPlan } } } } } } val pruned = prunePlans(plans) assert(pruned.hasNext, s\"No plan for $plan\") pruned","title":"Code of SparkPlan generation"},{"location":"SparkSQL/SparkPlan/#code-of-prepared-sparkplan-generation","text":"In QueryExecution.prepareForExecution(), rules (Rule[SparkPlan]) are applied => rules are defined in QueryExecution.preparations() org.apache.spark.sql.execution.QueryExecution#prepareForExecution protected def preparations: Seq[Rule[SparkPlan]] = { QueryExecution.preparations(sparkSession, Option(InsertAdaptiveSparkPlan(AdaptiveExecutionContext(sparkSession, this))), false) } /** * Construct a sequence of rules that are used to prepare a planned [[SparkPlan]] for execution. * These rules will make sure subqueries are planned, make use the data partitioning and ordering * are correct, insert whole stage code gen, and try to reduce the work done by reusing exchanges * and subqueries. */ private[execution] def preparations( sparkSession: SparkSession, adaptiveExecutionRule: Option[InsertAdaptiveSparkPlan] = None, subquery: Boolean): Seq[Rule[SparkPlan]] = { // `AdaptiveSparkPlanExec` is a leaf node. If inserted, all the following rules will be no-op // as the original plan is hidden behind `AdaptiveSparkPlanExec`. adaptiveExecutionRule.toSeq ++ Seq( CoalesceBucketsInJoin, PlanDynamicPruningFilters(sparkSession), PlanSubqueries(sparkSession), RemoveRedundantProjects, EnsureRequirements(), // `ReplaceHashWithSortAgg` needs to be added after `EnsureRequirements` to guarantee the // sort order of each node is checked to be valid. ReplaceHashWithSortAgg, // `RemoveRedundantSorts` needs to be added after `EnsureRequirements` to guarantee the same // number of partitions when instantiating PartitioningCollection. RemoveRedundantSorts, DisableUnnecessaryBucketedScan, ApplyColumnarRulesAndInsertTransitions( sparkSession.sessionState.columnarRules, outputsColumnar = false), CollapseCodegenStages()) ++ (if (subquery) { Nil } else { Seq(ReuseExchangeAndSubquery) }) } /** * Prepares a planned [[SparkPlan]] for execution by inserting shuffle operations and internal * row format conversions as needed. */ private[execution] def prepareForExecution( preparations: Seq[Rule[SparkPlan]], plan: SparkPlan): SparkPlan = { val planChangeLogger = new PlanChangeLogger[SparkPlan]() val preparedPlan = preparations.foldLeft(plan) { case (sp, rule) => val result = rule.apply(sp) planChangeLogger.logRule(rule.ruleName, sp, result) result } planChangeLogger.logBatch(\"Preparations\", plan, preparedPlan) preparedPlan }","title":"Code of prepared SparkPlan generation"}]}