<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>MicroBatchExecution - SparkCodeInternal</title>
        <link href="../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css">

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../..">SparkCodeInternal</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href="../.." class="nav-link">Introduction</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Spark Core <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../SparkCore/TaskScheduling/" class="dropdown-item">TaskScheduling</a>
</li>
                                    
<li>
    <a href="../../SparkCore/YarnAllocator/" class="dropdown-item">YarnAllocator</a>
</li>
                                    
<li>
    <a href="../../SparkCore/ShuffleService/" class="dropdown-item">ShuffleService</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Spark SQL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../SparkSQL/Analyzer/" class="dropdown-item">Analyzer</a>
</li>
                                    
<li>
    <a href="../../SparkSQL/SparkPlan/" class="dropdown-item">SparkPlan</a>
</li>
                                    
<li>
    <a href="../../SparkSQL/AQE/" class="dropdown-item">AQE</a>
</li>
                                    
<li>
    <a href="../../SparkSQL/Aggregation/" class="dropdown-item">Aggregation</a>
</li>
                                    
<li>
    <a href="../../SparkSQL/Join/" class="dropdown-item">Join</a>
</li>
                                    
<li>
    <a href="../../SparkSQL/Pivot/" class="dropdown-item">Pivot</a>
</li>
                                    
<li>
    <a href="../../SparkSQL/Hive/" class="dropdown-item">Hive</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Spark SparkStructuredStreaming <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="./" class="dropdown-item active">MicroBatchExecution</a>
</li>
                                    
<li>
    <a href="../RocksDBFileManager/" class="dropdown-item">RocksDBFileManager</a>
</li>
                                    
<li>
    <a href="../KafkaSqlConnector/" class="dropdown-item">KafkaSqlConnector</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../../SparkSQL/Hive/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../RocksDBFileManager/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/hwanghw/SparkCodeInternal/tree/main/docs/SparkStructuredStreaming/MicroBatchExecution.md" class="nav-link">Edit on hwanghw/SparkCodeInternal</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#streamingquerymanager" class="nav-link">StreamingQueryManager</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#microbatchexecution" class="nav-link">MicroBatchExecution</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#watermark-support" class="nav-link">Watermark Support</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#removekeysolderthanwatermark" class="nav-link">removeKeysOlderThanWatermark</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<div class="toc">
<ul>
<li><a href="#streamingquerymanager">StreamingQueryManager</a></li>
<li><a href="#microbatchexecution">MicroBatchExecution</a></li>
<li><a href="#watermark-support">Watermark Support</a><ul>
<li><a href="#removekeysolderthanwatermark">removeKeysOlderThanWatermark</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="streamingquerymanager">StreamingQueryManager<a class="headerlink" href="#streamingquerymanager" title="Permanent link">&para;</a></h1>
<p>SparkSession -&gt; SessionState -&gt; StreamingQueryManager</p>
<pre><code class="language-scala">private[sql] class SessionState(
  ......
  // The streamingQueryManager is lazy to avoid creating a StreamingQueryManager for each session
  // when connecting to ThriftServer.
  lazy val streamingQueryManager: StreamingQueryManager = streamingQueryManagerBuilder()
</code></pre>
<p>=&gt;</p>
<p>org.apache.spark.sql.streaming.DataStreamWriter#startQuery</p>
<pre><code class="language-scala">  private def startQuery(
      sink: Table,
      newOptions: CaseInsensitiveMap[String],
      recoverFromCheckpoint: Boolean = true,
      catalogAndIdent: Option[(TableCatalog, Identifier)] = None,
      catalogTable: Option[CatalogTable] = None): StreamingQuery = {
    val useTempCheckpointLocation = SOURCES_ALLOW_ONE_TIME_QUERY.contains(source)    

    df.sparkSession.sessionState.streamingQueryManager.startQuery(
      newOptions.get(&quot;queryName&quot;),
      newOptions.get(&quot;checkpointLocation&quot;),
      df,
      newOptions.originalMap,
      sink,
      outputMode,
      useTempCheckpointLocation = useTempCheckpointLocation,
      recoverFromCheckpointLocation = recoverFromCheckpoint,
      trigger = trigger,
      catalogAndIdent = catalogAndIdent,
      catalogTable = catalogTable)
  }
</code></pre>
<p>org.apache.spark.sql.streaming.StreamingQueryManager#startQuery</p>
<pre><code class="language-scala">  /**
   * Start a [[StreamingQuery]].
   *
   * @param userSpecifiedName Query name optionally specified by the user.
   * @param userSpecifiedCheckpointLocation  Checkpoint location optionally specified by the user.
   * @param df Streaming DataFrame.
   * @param sink  Sink to write the streaming outputs.
   * @param outputMode  Output mode for the sink.
   * @param useTempCheckpointLocation  Whether to use a temporary checkpoint location when the user
   *                                   has not specified one. If false, then error will be thrown.
   * @param recoverFromCheckpointLocation  Whether to recover query from the checkpoint location.
   *                                       If false and the checkpoint location exists, then error
   *                                       will be thrown.
   * @param trigger [[Trigger]] for the query.
   * @param triggerClock [[Clock]] to use for the triggering.
   * @param catalogAndIdent Catalog and identifier for the sink, set when it is a V2 catalog table
   */
  @throws[TimeoutException]
  private[sql] def startQuery(
      userSpecifiedName: Option[String],
      userSpecifiedCheckpointLocation: Option[String],
      df: DataFrame,
      extraOptions: Map[String, String],
      sink: Table,
      outputMode: OutputMode,
      useTempCheckpointLocation: Boolean = false,
      recoverFromCheckpointLocation: Boolean = true,
      trigger: Trigger = Trigger.ProcessingTime(0),
      triggerClock: Clock = new SystemClock(),
      catalogAndIdent: Option[(TableCatalog, Identifier)] = None,
      catalogTable: Option[CatalogTable] = None): StreamingQuery = {
    val query = createQuery(
      userSpecifiedName,
      userSpecifiedCheckpointLocation,
      df,
      extraOptions,
      sink,
      outputMode,
      useTempCheckpointLocation,
      recoverFromCheckpointLocation,
      trigger,
      triggerClock,
      catalogAndIdent,
      catalogTable)
    // scalastyle:on argcount

    // The following code block checks if a stream with the same name or id is running. Then it
    // returns an Option of an already active stream to stop outside of the lock
    // to avoid a deadlock.
    val activeRunOpt = activeQueriesSharedLock.synchronized {
      // Make sure no other query with same name is active
      userSpecifiedName.foreach { name =&gt;
        if (activeQueries.values.exists(_.name == name)) {
          throw new IllegalArgumentException(s&quot;Cannot start query with name $name as a query &quot; +
            s&quot;with that name is already active in this SparkSession&quot;)
        }
      }

      // Make sure no other query with same id is active across all sessions
      val activeOption = Option(sparkSession.sharedState.activeStreamingQueries.get(query.id))
        .orElse(activeQueries.get(query.id)) // shouldn't be needed but paranoia ...

      val shouldStopActiveRun =
        sparkSession.conf.get(SQLConf.STREAMING_STOP_ACTIVE_RUN_ON_RESTART)
      if (activeOption.isDefined) {
        if (shouldStopActiveRun) {
          val oldQuery = activeOption.get
          logWarning(s&quot;Stopping existing streaming query [id=${query.id}, &quot; +
            s&quot;runId=${oldQuery.runId}], as a new run is being started.&quot;)
          Some(oldQuery)
        } else {
          throw new IllegalStateException(
            s&quot;Cannot start query with id ${query.id} as another query with same id is &quot; +
              s&quot;already active. Perhaps you are attempting to restart a query from checkpoint &quot; +
              s&quot;that is already active. You may stop the old query by setting the SQL &quot; +
              &quot;configuration: &quot; +
              s&quot;&quot;&quot;spark.conf.set(&quot;${SQLConf.STREAMING_STOP_ACTIVE_RUN_ON_RESTART.key}&quot;, true) &quot;&quot;&quot; +
              &quot;and retry.&quot;)
        }
      } else {
        // nothing to stop so, no-op
        None
      }
    }

    // stop() will clear the queryId from activeStreamingQueries as well as activeQueries
    activeRunOpt.foreach(_.stop())

    activeQueriesSharedLock.synchronized {
      // We still can have a race condition when two concurrent instances try to start the same
      // stream, while a third one was already active and stopped above. In this case, we throw a
      // ConcurrentModificationException.
      val oldActiveQuery = sparkSession.sharedState.activeStreamingQueries.put(
        query.id, query.streamingQuery) // we need to put the StreamExecution, not the wrapper
      if (oldActiveQuery != null) {
        throw QueryExecutionErrors.concurrentQueryInstanceError()
      }
      activeQueries.put(query.id, query)
    }

    try {
      // When starting a query, it will call `StreamingQueryListener.onQueryStarted` synchronously.
      // As it's provided by the user and can run arbitrary codes, we must not hold any lock here.
      // Otherwise, it's easy to cause dead-lock, or block too long if the user codes take a long
      // time to finish.
      query.streamingQuery.start()   ===&gt; org.apache.spark.sql.execution.streaming.StreamExecution#start
    } catch {
      case e: Throwable =&gt;
        unregisterTerminatedStream(query)
        throw e
    }
    query
  }
</code></pre>
<p>org.apache.spark.sql.execution.streaming.StreamExecution#start</p>
<pre><code class="language-scala">/**
 * Manages the execution of a streaming Spark SQL query that is occurring in a separate thread.
 * Unlike a standard query, a streaming query executes repeatedly each time new data arrives at any
 * [[Source]] present in the query plan. Whenever new data arrives, a [[QueryExecution]] is created
 * and the results are committed transactionally to the given [[Sink]].
 *
 * @param deleteCheckpointOnStop whether to delete the checkpoint if the query is stopped without
 *                               errors. Checkpoint deletion can be forced with the appropriate
 *                               Spark configuration.
 */
abstract class StreamExecution(
    override val sparkSession: SparkSession,
    override val name: String,
    val resolvedCheckpointRoot: String,
    val analyzedPlan: LogicalPlan,
    val sink: Table,
    val trigger: Trigger,
    val triggerClock: Clock,
    val outputMode: OutputMode,
    deleteCheckpointOnStop: Boolean)
  extends StreamingQuery with ProgressReporter with Logging {

  /**
   * Starts the execution. This returns only after the thread has started and [[QueryStartedEvent]]
   * has been posted to all the listeners.
   */
  def start(): Unit = {
    logInfo(s&quot;Starting $prettyIdString. Use $resolvedCheckpointRoot to store the query checkpoint.&quot;)
    queryExecutionThread.setDaemon(true)
    queryExecutionThread.start()
    startLatch.await()  // Wait until thread started and QueryStart event has been posted
  }


  /**
   * The thread that runs the micro-batches of this stream. Note that this thread must be
   * [[org.apache.spark.util.UninterruptibleThread]] to workaround KAFKA-1894: interrupting a
   * running `KafkaConsumer` may cause endless loop.
   */
  val queryExecutionThread: QueryExecutionThread =
    new QueryExecutionThread(s&quot;stream execution thread for $prettyIdString&quot;) {
      override def run(): Unit = {
        // To fix call site like &quot;run at &lt;unknown&gt;:0&quot;, we bridge the call site from the caller
        // thread to this micro batch thread
        sparkSession.sparkContext.setCallSite(callSite)
        runStream()
      }
    }

  /**
   * Activate the stream and then wrap a callout to runActivatedStream, handling start and stop.
   *
   * Note that this method ensures that [[QueryStartedEvent]] and [[QueryTerminatedEvent]] are
   * posted such that listeners are guaranteed to get a start event before a termination.
   * Furthermore, this method also ensures that [[QueryStartedEvent]] event is posted before the
   * `start()` method returns.
   */
  private def   runStream(): Unit = {
    try {
          ...
        if (state.compareAndSet(INITIALIZING, ACTIVE)) {
          // Unblock `awaitInitialization`
          initializationLatch.countDown()
          runActivatedStream(sparkSessionForStream)  ===&gt; implemented by MicroBatchExecution or ContinuousExecution
          updateStatusMessage(&quot;Stopped&quot;)
        } else {
          // `stop()` is already called. Let `finally` finish the cleanup.
        }
     ......

    } catch {
      case e if isInterruptedByStop(e, sparkSession.sparkContext) =&gt;
        // interrupted by stop()
        updateStatusMessage(&quot;Stopped&quot;)
      case e: IOException if e.getMessage != null
        &amp;&amp; e.getMessage.startsWith(classOf[InterruptedException].getName)
        &amp;&amp; state.get == TERMINATED =&gt;
        // This is a workaround for HADOOP-12074: `Shell.runCommand` converts `InterruptedException`
        // to `new IOException(ie.toString())` before Hadoop 2.8.
        updateStatusMessage(&quot;Stopped&quot;)
      case e: Throwable =&gt;
        val message = if (e.getMessage == null) &quot;&quot; else e.getMessage
        streamDeathCause = new StreamingQueryException(
          toDebugString(includeLogicalPlan = isInitialized),
          cause = e,
          committedOffsets.toOffsetSeq(sources, offsetSeqMetadata).toString,
          availableOffsets.toOffsetSeq(sources, offsetSeqMetadata).toString,
          errorClass = &quot;STREAM_FAILED&quot;,
          messageParameters = Map(
            &quot;id&quot; -&gt; id.toString,
            &quot;runId&quot; -&gt; runId.toString,
            &quot;message&quot; -&gt; message))
        logError(s&quot;Query $prettyIdString terminated with error&quot;, e)
        updateStatusMessage(s&quot;Terminated with exception: $message&quot;)
        // Rethrow the fatal errors to allow the user using `Thread.UncaughtExceptionHandler` to
        // handle them
        if (!NonFatal(e)) {
          throw e
        }
    } finally queryExecutionThread.runUninterruptibly {
      // The whole `finally` block must run inside `runUninterruptibly` to avoid being interrupted
      // when a query is stopped by the user. We need to make sure the following codes finish
      // otherwise it may throw `InterruptedException` to `UncaughtExceptionHandler` (SPARK-21248).

      // Release latches to unblock the user codes since exception can happen in any place and we
      // may not get a chance to release them
      startLatch.countDown()
      initializationLatch.countDown()

      try {
        stopSources()
        cleanup()
        state.set(TERMINATED)
        currentStatus = status.copy(isTriggerActive = false, isDataAvailable = false)

        // Update metrics and status
        sparkSession.sparkContext.env.metricsSystem.removeSource(streamMetrics)

        // Notify others
        sparkSession.streams.notifyQueryTermination(StreamExecution.this)
        postEvent(
          new QueryTerminatedEvent(id, runId, exception.map(_.cause).map(Utils.exceptionString)))

        // Delete the temp checkpoint when either force delete enabled or the query didn't fail
        if (deleteCheckpointOnStop &amp;&amp;
            (sparkSession.sessionState.conf
              .getConf(SQLConf.FORCE_DELETE_TEMP_CHECKPOINT_LOCATION) || exception.isEmpty)) {
          val checkpointPath = new Path(resolvedCheckpointRoot)
          try {
            logInfo(s&quot;Deleting checkpoint $checkpointPath.&quot;)
            fileManager.delete(checkpointPath)
          } catch {
            case NonFatal(e) =&gt;
              // Deleting temp checkpoint folder is best effort, don't throw non fatal exceptions
              // when we cannot delete them.
              logWarning(s&quot;Cannot delete $checkpointPath&quot;, e)
          }
        }
      } finally {
        awaitProgressLock.lock()
        try {
          // Wake up any threads that are waiting for the stream to progress.
          awaitProgressLockCondition.signalAll()
        } finally {
          awaitProgressLock.unlock()
        }
        terminationLatch.countDown()
      }
    }
  }
</code></pre>
<h1 id="microbatchexecution">MicroBatchExecution<a class="headerlink" href="#microbatchexecution" title="Permanent link">&para;</a></h1>
<pre><code class="language-scala">class MicroBatchExecution(
    sparkSession: SparkSession,
    trigger: Trigger,
    triggerClock: Clock,
    extraOptions: Map[String, String],
    plan: WriteToStream)
  extends StreamExecution(
    sparkSession, plan.name, plan.resolvedCheckpointLocation, plan.inputQuery, plan.sink, trigger,
    triggerClock, plan.outputMode, plan.deleteCheckpointOnStop) with AsyncLogPurge {


  /**
   * Repeatedly attempts to run batches as data arrives.
   */
  protected def runActivatedStream(sparkSessionForStream: SparkSession): Unit = {

    val noDataBatchesEnabled =
      sparkSessionForStream.sessionState.conf.streamingNoDataMicroBatchesEnabled

    triggerExecutor.execute(() =&gt; {
      if (isActive) {

        // check if there are any previous errors and bubble up any existing async operations
        errorNotifier.throwErrorIfExists

        var currentBatchHasNewData = false // Whether the current batch had new data

        startTrigger()

        reportTimeTaken(&quot;triggerExecution&quot;) {
          // We'll do this initialization only once every start / restart
          if (currentBatchId &lt; 0) {
            AcceptsLatestSeenOffsetHandler.setLatestSeenOffsetOnSources(
              offsetLog.getLatest().map(_._2), sources)
            populateStartOffsets(sparkSessionForStream)
            logInfo(s&quot;Stream started from $committedOffsets&quot;)
          }

          // Set this before calling constructNextBatch() so any Spark jobs executed by sources
          // while getting new data have the correct description
          sparkSession.sparkContext.setJobDescription(getBatchDescriptionString)

          // Try to construct the next batch. This will return true only if the next batch is
          // ready and runnable. Note that the current batch may be runnable even without
          // new data to process as `constructNextBatch` may decide to run a batch for
          // state cleanup, etc. `isNewDataAvailable` will be updated to reflect whether new data
          // is available or not.
          if (!isCurrentBatchConstructed) {
            isCurrentBatchConstructed = constructNextBatch(noDataBatchesEnabled)
          }

          // Record the trigger offset range for progress reporting *before* processing the batch
          recordTriggerOffsets(
            from = committedOffsets,
            to = availableOffsets,
            latest = latestOffsets)

          // Remember whether the current batch has data or not. This will be required later
          // for bookkeeping after running the batch, when `isNewDataAvailable` will have changed
          // to false as the batch would have already processed the available data.
          currentBatchHasNewData = isNewDataAvailable

          currentStatus = currentStatus.copy(isDataAvailable = isNewDataAvailable)
          if (isCurrentBatchConstructed) {
            if (currentBatchHasNewData) updateStatusMessage(&quot;Processing new data&quot;)
            else updateStatusMessage(&quot;No new data but cleaning up state&quot;)
            runBatch(sparkSessionForStream)
          } else {
            updateStatusMessage(&quot;Waiting for data to arrive&quot;)
          }
        }

        // Must be outside reportTimeTaken so it is recorded
        finishTrigger(currentBatchHasNewData, isCurrentBatchConstructed)

        // Signal waiting threads. Note this must be after finishTrigger() to ensure all
        // activities (progress generation, etc.) have completed before signaling.
        withProgressLocked { awaitProgressLockCondition.signalAll() }

        // If the current batch has been executed, then increment the batch id and reset flag.
        // Otherwise, there was no data to execute the batch and sleep for some time
        if (isCurrentBatchConstructed) {
          currentBatchId += 1
          isCurrentBatchConstructed = false
        } else if (triggerExecutor.isInstanceOf[MultiBatchExecutor]) {
          logInfo(&quot;Finished processing all available data for the trigger, terminating this &quot; +
            &quot;Trigger.AvailableNow query&quot;)
          state.set(TERMINATED)
        } else Thread.sleep(pollingDelayMs)
      }
      updateStatusMessage(&quot;Waiting for next trigger&quot;)
      isActive
    })
  }

</code></pre>
<h1 id="watermark-support">Watermark Support<a class="headerlink" href="#watermark-support" title="Permanent link">&para;</a></h1>
<h2 id="removekeysolderthanwatermark">removeKeysOlderThanWatermark<a class="headerlink" href="#removekeysolderthanwatermark" title="Permanent link">&para;</a></h2>
<p><strong>org.apache.spark.sql.execution.streaming.WatermarkSupport#removeKeysOlderThanWatermark</strong></p>
<pre><code class="language-scala">  protected def removeKeysOlderThanWatermark(store: StateStore): Unit = {
    if (watermarkPredicateForKeysForEviction.nonEmpty) {
      val numRemovedStateRows = longMetric(&quot;numRemovedStateRows&quot;)
      store.iterator().foreach { rowPair =&gt;
        if (watermarkPredicateForKeysForEviction.get.eval(rowPair.key)) {
          store.remove(rowPair.key)
          numRemovedStateRows += 1
        }
      }
    }
  }

protected def removeKeysOlderThanWatermark(
      storeManager: StreamingAggregationStateManager,
      store: StateStore): Unit = {
    if (watermarkPredicateForKeysForEviction.nonEmpty) {
      val numRemovedStateRows = longMetric(&quot;numRemovedStateRows&quot;)
      storeManager.keys(store).foreach { keyRow =&gt;
        if (watermarkPredicateForKeysForEviction.get.eval(keyRow)) {
          storeManager.remove(store, keyRow)
          numRemovedStateRows += 1
        }
      }
    }
  }
</code></pre>
<p><strong>org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider.RocksDBStateStore#iterator</strong></p>
<pre><code class="language-scala">    override def iterator(): Iterator[UnsafeRowPair] = {
      rocksDB.iterator().map { kv =&gt;
        val rowPair = encoder.decode(kv)
        if (!isValidated &amp;&amp; rowPair.value != null) {
          StateStoreProvider.validateStateRowFormat(
            rowPair.key, keySchema, rowPair.value, valueSchema, storeConf)
          isValidated = true
        }
        rowPair
      }
    }
</code></pre>
<p><strong>org.apache.spark.sql.execution.streaming.state.RocksDB#iterator</strong></p>
<pre><code class="language-scala">  def iterator(): Iterator[ByteArrayPair] = {
    val iter = writeBatch.newIteratorWithBase(db.newIterator())
    logInfo(s&quot;Getting iterator from version $loadedVersion&quot;)
    iter.seekToFirst()

    // Attempt to close this iterator if there is a task failure, or a task interruption.
    // This is a hack because it assumes that the RocksDB is running inside a task.
    Option(TaskContext.get()).foreach { tc =&gt;
      tc.addTaskCompletionListener[Unit] { _ =&gt; iter.close() }
    }

    new NextIterator[ByteArrayPair] {
      override protected def getNext(): ByteArrayPair = {
        if (iter.isValid) {
          byteArrayPair.set(iter.key, iter.value)
          iter.next()
          byteArrayPair
        } else {
          finished = true
          iter.close()
          null
        }
      }
      override protected def close(): Unit = { iter.close() }
    }
  }
</code></pre></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
