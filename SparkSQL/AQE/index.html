<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>AQE - SparkCodeInternal</title>
        <link href="../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css">

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../..">SparkCodeInternal</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href="../.." class="nav-link">Introduction</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Spark Core <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../SparkCore/YarnAllocator/" class="dropdown-item">YarnAllocator</a>
</li>
                                    
<li>
    <a href="../../SparkCore/ShuffleService/" class="dropdown-item">ShuffleService</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Spark SQL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../Analyzer/" class="dropdown-item">Analyzer</a>
</li>
                                    
<li>
    <a href="../SparkPlan/" class="dropdown-item">SparkPlan</a>
</li>
                                    
<li>
    <a href="./" class="dropdown-item active">AQE</a>
</li>
                                    
<li>
    <a href="../Aggregation/" class="dropdown-item">Aggregation</a>
</li>
                                    
<li>
    <a href="../Join/" class="dropdown-item">Join</a>
</li>
                                    
<li>
    <a href="../Pivot/" class="dropdown-item">Pivot</a>
</li>
                                    
<li>
    <a href="../Hive/" class="dropdown-item">Hive</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../SparkPlan/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../Aggregation/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/hwanghw/SparkCodeInternal/tree/main/docs/SparkSQL/AQE.md" class="nav-link">Edit on hwanghw/SparkCodeInternal</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#adaptive-execution-in-spark" class="nav-link">Adaptive execution in Spark</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#jira" class="nav-link">Jira</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#queryexecutionpreparations" class="nav-link">QueryExecution#preparations</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#insertadaptivesparkplan" class="nav-link">InsertAdaptiveSparkPlan</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#adaptivesparkplanexec" class="nav-link">AdaptiveSparkPlanExec</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#querystageexec" class="nav-link">QueryStageExec</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#reusequerystage" class="nav-link">reuseQueryStage</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#adaptive-coalesce-partitions" class="nav-link">Adaptive coalesce partitions</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<div class="toc">
<ul>
<li><a href="#adaptive-execution-in-spark">Adaptive execution in Spark</a><ul>
<li><a href="#jira">Jira</a></li>
<li><a href="#queryexecutionpreparations">QueryExecution#preparations</a></li>
<li><a href="#insertadaptivesparkplan">InsertAdaptiveSparkPlan</a></li>
<li><a href="#adaptivesparkplanexec">AdaptiveSparkPlanExec</a></li>
<li><a href="#querystageexec">QueryStageExec</a></li>
<li><a href="#reusequerystage">reuseQueryStage</a></li>
<li><a href="#adaptive-coalesce-partitions">Adaptive coalesce partitions</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="adaptive-execution-in-spark">Adaptive execution in Spark<a class="headerlink" href="#adaptive-execution-in-spark" title="Permanent link">&para;</a></h1>
<h2 id="jira">Jira<a class="headerlink" href="#jira" title="Permanent link">&para;</a></h2>
<p><a href="https://issues.apache.org/jira/browse/SPARK-31412">SPARK-31412 Feature requirement (with subtasks list)</a><br />
<a href="https://docs.google.com/document/d/1mpVjvQZRAkD-Ggy6-hcjXtBPiQoVbZGe3dLnAKgtJ4k/edit?usp=sharing">Design Doc</a></p>
<p><a href="https://issues.apache.org/jira/browse/SPARK-23128">SPARK-23128 The basic framework for the new Adaptive Query Execution</a></p>
<p><a href="https://issues.apache.org/jira/browse/SPARK-28177">SPARK-28177 Adjust post shuffle partition number in adaptive execution</a></p>
<p><a href="https://issues.apache.org/jira/browse/SPARK-29544">SPARK-29544 Optimize skewed join at runtime with new Adaptive Execution</a></p>
<p><a href="https://issues.apache.org/jira/browse/SPARK-31865">SPARK-31865 Fix complex AQE query stage not reused</a></p>
<p><a href="https://issues.apache.org/jira/browse/SPARK-35552">SPARK-35552 Make query stage materialized more readable</a></p>
<p><a href="https://issues.apache.org/jira/browse/SPARK-9850">SPARK-9850 Adaptive execution in Spark (original idea)</a><br />
<a href="https://issues.apache.org/jira/secure/attachment/12749984/AdaptiveExecutionInSpark.pdf">Design Doc</a></p>
<p><a href="https://issues.apache.org/jira/browse/SPARK-9851">SPARK-9851 Support submitting map stages individually in DAGScheduler</a></p>
<h2 id="queryexecutionpreparations"><strong>QueryExecution#preparations</strong><a class="headerlink" href="#queryexecutionpreparations" title="Permanent link">&para;</a></h2>
<p>org.apache.spark.sql.execution.QueryExecution#preparations</p>
<pre><code>private[execution] def preparations(
   sparkSession: SparkSession,
   adaptiveExecutionRule: Option[InsertAdaptiveSparkPlan] = None,
   subquery: Boolean): Seq[Rule[SparkPlan]] = {
 // `AdaptiveSparkPlanExec` is a leaf node. If inserted, all the following rules will be no-op
 // as the original plan is hidden behind `AdaptiveSparkPlanExec`.
 adaptiveExecutionRule.toSeq ++
</code></pre>
<h2 id="insertadaptivesparkplan"><strong>InsertAdaptiveSparkPlan</strong><a class="headerlink" href="#insertadaptivesparkplan" title="Permanent link">&para;</a></h2>
<p>org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan</p>
<pre><code>/**
* This rule wraps the query plan with an [[AdaptiveSparkPlanExec]], which executes the query plan
* and re-optimize the plan during execution based on runtime data statistics.
*
* Note that this rule is stateful and thus should not be reused across query executions.
*/
case class InsertAdaptiveSparkPlan(
   adaptiveExecutionContext: AdaptiveExecutionContext) extends Rule[SparkPlan] {

  override def apply(plan: SparkPlan): SparkPlan = applyInternal(plan, false)

  private def applyInternal(plan: SparkPlan, isSubquery: Boolean): SparkPlan = plan match {
    case _ if !conf.adaptiveExecutionEnabled =&gt; plan
    case _: ExecutedCommandExec =&gt; plan
    case _: CommandResultExec =&gt; plan
    case c: DataWritingCommandExec =&gt; c.copy(child = apply(c.child))
    case c: V2CommandExec =&gt; c.withNewChildren(c.children.map(apply))
    case _ if shouldApplyAQE(plan, isSubquery) =&gt;
      if (supportAdaptive(plan)) {
        try {
          // Plan sub-queries recursively and pass in the shared stage cache for exchange reuse.
          // Fall back to non-AQE mode if AQE is not supported in any of the sub-queries.
          val subqueryMap = buildSubqueryMap(plan)
          val planSubqueriesRule = PlanAdaptiveSubqueries(subqueryMap)
          val preprocessingRules = Seq(
            planSubqueriesRule)
          // Run pre-processing rules.
          val newPlan = AdaptiveSparkPlanExec.applyPhysicalRules(plan, preprocessingRules)
          logDebug(s&quot;Adaptive execution enabled for plan: $plan&quot;)
          AdaptiveSparkPlanExec(newPlan, adaptiveExecutionContext, preprocessingRules, isSubquery)
        } catch {
          case SubqueryAdaptiveNotSupportedException(subquery) =&gt;
            logWarning(s&quot;${SQLConf.ADAPTIVE_EXECUTION_ENABLED.key} is enabled &quot; +
              s&quot;but is not supported for sub-query: $subquery.&quot;)
            plan
        }
      } else {
        logDebug(s&quot;${SQLConf.ADAPTIVE_EXECUTION_ENABLED.key} is enabled &quot; +
          s&quot;but is not supported for query: $plan.&quot;)
        plan
      }

    case _ =&gt; plan
  }

  // AQE is only useful when the query has exchanges or sub-queries. This method returns true if
  // one of the following conditions is satisfied:
  //   - The config ADAPTIVE_EXECUTION_FORCE_APPLY is true.
  //   - The input query is from a sub-query. When this happens, it means we've already decided to
  //     apply AQE for the main query and we must continue to do it.
  //   - The query contains exchanges.
  //   - The query may need to add exchanges. It's an overkill to run `EnsureRequirements` here, so
  //     we just check `SparkPlan.requiredChildDistribution` and see if it's possible that the
  //     the query needs to add exchanges later.
  //   - The query contains sub-query.
  private def shouldApplyAQE(plan: SparkPlan, isSubquery: Boolean): Boolean = {
    conf.getConf(SQLConf.ADAPTIVE_EXECUTION_FORCE_APPLY) || isSubquery || {
      plan.exists {
        case _: Exchange =&gt; true
        case p if !p.requiredChildDistribution.forall(_ == UnspecifiedDistribution) =&gt; true
        case p =&gt; p.expressions.exists(_.exists {
          case _: SubqueryExpression =&gt; true
          case _ =&gt; false
        })
      }
    }
  } 
</code></pre>
<h2 id="adaptivesparkplanexec"><strong>AdaptiveSparkPlanExec</strong><a class="headerlink" href="#adaptivesparkplanexec" title="Permanent link">&para;</a></h2>
<p>org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec</p>
<pre><code>
/**
 * A root node to execute the query plan adaptively. It splits the query plan into independent
 * stages and executes them in order according to their dependencies. The query stage
 * materializes its output at the end. When one stage completes, the data statistics of the
 * materialized output will be used to optimize the remainder of the query.
 *
 * To create query stages, we traverse the query tree bottom up. When we hit an exchange node,
 * and if all the child query stages of this exchange node are materialized, we create a new
 * query stage for this exchange node. The new stage is then materialized asynchronously once it
 * is created.
 *
 * When one query stage finishes materialization, the rest query is re-optimized and planned based
 * on the latest statistics provided by all materialized stages. Then we traverse the query plan
 * again and create more stages if possible. After all stages have been materialized, we execute
 * the rest of the plan.
 */
case class AdaptiveSparkPlanExec(
    inputPlan: SparkPlan,
    @transient context: AdaptiveExecutionContext,
    @transient preprocessingRules: Seq[Rule[SparkPlan]],
    @transient isSubquery: Boolean,
    @transient override val supportsColumnar: Boolean = false)
  extends LeafExecNode {

  override def doExecute(): RDD[InternalRow] = {
    withFinalPlanUpdate(_.execute())
  }

  private def withFinalPlanUpdate[T](fun: SparkPlan =&gt; T): T = {
    val plan = getFinalPhysicalPlan()
    val result = fun(plan)
    finalPlanUpdate
    result
  }

  private def getFinalPhysicalPlan(): SparkPlan = lock.synchronized {
    if (isFinalPlan) return currentPhysicalPlan

    // In case of this adaptive plan being executed out of `withActive` scoped functions, e.g.,
    // `plan.queryExecution.rdd`, we need to set active session here as new plan nodes can be
    // created in the middle of the execution.
    context.session.withActive {
      val executionId = getExecutionId
      // Use inputPlan logicalLink here in case some top level physical nodes may be removed
      // during `initialPlan`
      var currentLogicalPlan = inputPlan.logicalLink.get
      var result = createQueryStages(currentPhysicalPlan)
      val events = new LinkedBlockingQueue[StageMaterializationEvent]()
      val errors = new mutable.ArrayBuffer[Throwable]()
      var stagesToReplace = Seq.empty[QueryStageExec]
      while (!result.allChildStagesMaterialized) {
        currentPhysicalPlan = result.newPlan
        if (result.newStages.nonEmpty) {
          stagesToReplace = result.newStages ++ stagesToReplace
          executionId.foreach(onUpdatePlan(_, result.newStages.map(_.plan)))

          // SPARK-33933: we should submit tasks of broadcast stages first, to avoid waiting
          // for tasks to be scheduled and leading to broadcast timeout.
          // This partial fix only guarantees the start of materialization for BroadcastQueryStage
          // is prior to others, but because the submission of collect job for broadcasting is
          // running in another thread, the issue is not completely resolved.
          val reorderedNewStages = result.newStages
            .sortWith {
              case (_: BroadcastQueryStageExec, _: BroadcastQueryStageExec) =&gt; false
              case (_: BroadcastQueryStageExec, _) =&gt; true
              case _ =&gt; false
            }


          ====================  stage.materialize() is run as Future async =========================
          // Start materialization of all new stages and fail fast if any stages failed eagerly
          reorderedNewStages.foreach { stage =&gt;
            try {
              stage.materialize().onComplete { res =&gt;
                if (res.isSuccess) {
                  events.offer(StageSuccess(stage, res.get))
                } else {
                  events.offer(StageFailure(stage, res.failed.get))
                }
              }(AdaptiveSparkPlanExec.executionContext)
            } catch {
              case e: Throwable =&gt;
                cleanUpAndThrowException(Seq(e), Some(stage.id))
            }
          }
          ==========================================================================================

        }

        // Wait on the next completed stage, which indicates new stats are available and probably
        // new stages can be created. There might be other stages that finish at around the same
        // time, so we process those stages too in order to reduce re-planning.
        val nextMsg = events.take()
        val rem = new util.ArrayList[StageMaterializationEvent]()
        events.drainTo(rem)
        (Seq(nextMsg) ++ rem.asScala).foreach {
          case StageSuccess(stage, res) =&gt;
            stage.resultOption.set(Some(res))
          case StageFailure(stage, ex) =&gt;
            errors.append(ex)
        }

        // In case of errors, we cancel all running stages and throw exception.
        if (errors.nonEmpty) {
          cleanUpAndThrowException(errors.toSeq, None)
        }

        // Try re-optimizing and re-planning. Adopt the new plan if its cost is equal to or less
        // than that of the current plan; otherwise keep the current physical plan together with
        // the current logical plan since the physical plan's logical links point to the logical
        // plan it has originated from.
        // Meanwhile, we keep a list of the query stages that have been created since last plan
        // update, which stands for the &quot;semantic gap&quot; between the current logical and physical
        // plans. And each time before re-planning, we replace the corresponding nodes in the
        // current logical plan with logical query stages to make it semantically in sync with
        // the current physical plan. Once a new plan is adopted and both logical and physical
        // plans are updated, we can clear the query stage list because at this point the two plans
        // are semantically and physically in sync again.
        val logicalPlan = replaceWithQueryStagesInLogicalPlan(currentLogicalPlan, stagesToReplace)
        val afterReOptimize = reOptimize(logicalPlan)
        if (afterReOptimize.isDefined) {
          val (newPhysicalPlan, newLogicalPlan) = afterReOptimize.get
          val origCost = costEvaluator.evaluateCost(currentPhysicalPlan)
          val newCost = costEvaluator.evaluateCost(newPhysicalPlan)
          if (newCost &lt; origCost ||
            (newCost == origCost &amp;&amp; currentPhysicalPlan != newPhysicalPlan)) {
            logOnLevel(&quot;Plan changed:\n&quot; +
              sideBySide(currentPhysicalPlan.treeString, newPhysicalPlan.treeString).mkString(&quot;\n&quot;))
            cleanUpTempTags(newPhysicalPlan)
            currentPhysicalPlan = newPhysicalPlan
            currentLogicalPlan = newLogicalPlan
            stagesToReplace = Seq.empty[QueryStageExec]
          }
        }
        // Now that some stages have finished, we can try creating new stages.
        result = createQueryStages(currentPhysicalPlan)
      }

      // Run the final plan when there's no more unfinished stages.
      currentPhysicalPlan = applyPhysicalRules(
        optimizeQueryStage(result.newPlan, isFinalStage = true),
        postStageCreationRules(supportsColumnar),
        Some((planChangeLogger, &quot;AQE Post Stage Creation&quot;)))
      isFinalPlan = true
      executionId.foreach(onUpdatePlan(_, Seq(currentPhysicalPlan)))
      currentPhysicalPlan
    }
  }


  /**
   * This method is called recursively to traverse the plan tree bottom-up and create a new query
   * stage or try reusing an existing stage if the current node is an [[Exchange]] node and all of
   * its child stages have been materialized.
   *
   * With each call, it returns:
   * 1) The new plan replaced with [[QueryStageExec]] nodes where new stages are created.
   * 2) Whether the child query stages (if any) of the current node have all been materialized.
   * 3) A list of the new query stages that have been created.
   */
  private def createQueryStages(plan: SparkPlan): CreateStageResult = plan match {
    case e: Exchange =&gt;
      // First have a quick check in the `stageCache` without having to traverse down the node.
      context.stageCache.get(e.canonicalized) match {
        case Some(existingStage) if conf.exchangeReuseEnabled =&gt;
          val stage = reuseQueryStage(existingStage, e)
          val isMaterialized = stage.isMaterialized
          CreateStageResult(
            newPlan = stage,
            allChildStagesMaterialized = isMaterialized,
            newStages = if (isMaterialized) Seq.empty else Seq(stage))

        case _ =&gt;
          val result = createQueryStages(e.child)
          val newPlan = e.withNewChildren(Seq(result.newPlan)).asInstanceOf[Exchange]
          // Create a query stage only when all the child query stages are ready.
          if (result.allChildStagesMaterialized) {
            var newStage = newQueryStage(newPlan)
            if (conf.exchangeReuseEnabled) {
              // Check the `stageCache` again for reuse. If a match is found, ditch the new stage
              // and reuse the existing stage found in the `stageCache`, otherwise update the
              // `stageCache` with the new stage.
              val queryStage = context.stageCache.getOrElseUpdate(
                newStage.plan.canonicalized, newStage)
              if (queryStage.ne(newStage)) {
                newStage = reuseQueryStage(queryStage, e)
              }
            }
            val isMaterialized = newStage.isMaterialized
            CreateStageResult(
              newPlan = newStage,
              allChildStagesMaterialized = isMaterialized,
              newStages = if (isMaterialized) Seq.empty else Seq(newStage))
          } else {
            CreateStageResult(newPlan = newPlan,
              allChildStagesMaterialized = false, newStages = result.newStages)
          }
      }

    case q: QueryStageExec =&gt;
      CreateStageResult(newPlan = q,
        allChildStagesMaterialized = q.isMaterialized, newStages = Seq.empty)

    case _ =&gt;
      if (plan.children.isEmpty) {
        CreateStageResult(newPlan = plan, allChildStagesMaterialized = true, newStages = Seq.empty)
      } else {
        val results = plan.children.map(createQueryStages)
        CreateStageResult(
          newPlan = plan.withNewChildren(results.map(_.newPlan)),
          allChildStagesMaterialized = results.forall(_.allChildStagesMaterialized),
          newStages = results.flatMap(_.newStages))
      }
  }

  private def newQueryStage(e: Exchange): QueryStageExec = {
    val optimizedPlan = optimizeQueryStage(e.child, isFinalStage = false)
    val queryStage = e match {
      case s: ShuffleExchangeLike =&gt;
        val newShuffle = applyPhysicalRules(
          s.withNewChildren(Seq(optimizedPlan)),
          postStageCreationRules(outputsColumnar = s.supportsColumnar),
          Some((planChangeLogger, &quot;AQE Post Stage Creation&quot;)))
        if (!newShuffle.isInstanceOf[ShuffleExchangeLike]) {
          throw new IllegalStateException(
            &quot;Custom columnar rules cannot transform shuffle node to something else.&quot;)
        }
        ShuffleQueryStageExec(currentStageId, newShuffle, s.canonicalized)
      case b: BroadcastExchangeLike =&gt;
        val newBroadcast = applyPhysicalRules(
          b.withNewChildren(Seq(optimizedPlan)),
          postStageCreationRules(outputsColumnar = b.supportsColumnar),
          Some((planChangeLogger, &quot;AQE Post Stage Creation&quot;)))
        if (!newBroadcast.isInstanceOf[BroadcastExchangeLike]) {
          throw new IllegalStateException(
            &quot;Custom columnar rules cannot transform broadcast node to something else.&quot;)
        }
        BroadcastQueryStageExec(currentStageId, newBroadcast, b.canonicalized)
    }
    currentStageId += 1
    setLogicalLinkForNewQueryStage(queryStage, e)
    queryStage
  }





</code></pre>
<p><strong>rules</strong></p>
<pre><code>  @transient private val costEvaluator =
    conf.getConf(SQLConf.ADAPTIVE_CUSTOM_COST_EVALUATOR_CLASS) match {
      case Some(className) =&gt; CostEvaluator.instantiate(className, session.sparkContext.getConf)
      case _ =&gt; SimpleCostEvaluator(conf.getConf(SQLConf.ADAPTIVE_FORCE_OPTIMIZE_SKEWED_JOIN))
    }

  // A list of physical plan rules to be applied before creation of query stages. The physical
  // plan should reach a final status of query stages (i.e., no more addition or removal of
  // Exchange nodes) after running these rules.
  @transient private val queryStagePreparationRules: Seq[Rule[SparkPlan]] = {
    // For cases like `df.repartition(a, b).select(c)`, there is no distribution requirement for
    // the final plan, but we do need to respect the user-specified repartition. Here we ask
    // `EnsureRequirements` to not optimize out the user-specified repartition-by-col to work
    // around this case.
    val ensureRequirements =
      EnsureRequirements(requiredDistribution.isDefined, requiredDistribution)
    Seq(
      RemoveRedundantProjects,
      ensureRequirements,
      ValidateSparkPlan,
      ReplaceHashWithSortAgg,
      RemoveRedundantSorts,
      DisableUnnecessaryBucketedScan,
      OptimizeSkewedJoin(ensureRequirements)
    ) ++ context.session.sessionState.adaptiveRulesHolder.queryStagePrepRules
  }

  // A list of physical optimizer rules to be applied to a new stage before its execution. These
  // optimizations should be stage-independent.
  @transient private val queryStageOptimizerRules: Seq[Rule[SparkPlan]] = Seq(
    PlanAdaptiveDynamicPruningFilters(this),
    ReuseAdaptiveSubquery(context.subqueryCache),
    OptimizeSkewInRebalancePartitions,
    CoalesceShufflePartitions(context.session),
    // `OptimizeShuffleWithLocalRead` needs to make use of 'AQEShuffleReadExec.partitionSpecs'
    // added by `CoalesceShufflePartitions`, and must be executed after it.
    OptimizeShuffleWithLocalRead
  )

  // This rule is stateful as it maintains the codegen stage ID. We can't create a fresh one every
  // time and need to keep it in a variable.
  @transient private val collapseCodegenStagesRule: Rule[SparkPlan] =
    CollapseCodegenStages()

  // A list of physical optimizer rules to be applied right after a new stage is created. The input
  // plan to these rules has exchange as its root node.
  private def postStageCreationRules(outputsColumnar: Boolean) = Seq(
    ApplyColumnarRulesAndInsertTransitions(
      context.session.sessionState.columnarRules, outputsColumnar),
    collapseCodegenStagesRule
  )

  @transient val initialPlan = context.session.withActive {
    applyPhysicalRules(
      inputPlan, queryStagePreparationRules, Some((planChangeLogger, &quot;AQE Preparations&quot;)))
  }

  @volatile private var currentPhysicalPlan = initialPlan

  // The logical plan optimizer for re-optimizing the current logical plan.
  @transient private val optimizer = new AQEOptimizer(conf,
    session.sessionState.adaptiveRulesHolder.runtimeOptimizerRules)

  private def optimizeQueryStage(plan: SparkPlan, isFinalStage: Boolean): SparkPlan = {
    val optimized = queryStageOptimizerRules.foldLeft(plan) { case (latestPlan, rule) =&gt;
      val applied = rule.apply(latestPlan)
      val result = rule match {
        case _: AQEShuffleReadRule if !applied.fastEquals(latestPlan) =&gt;
          val distribution = if (isFinalStage) {
            // If `requiredDistribution` is None, it means `EnsureRequirements` will not optimize
            // out the user-specified repartition, thus we don't have a distribution requirement
            // for the final plan.
            requiredDistribution.getOrElse(UnspecifiedDistribution)
          } else {
            UnspecifiedDistribution
          }
          if (ValidateRequirements.validate(applied, distribution)) {
            applied
          } else {
            logDebug(s&quot;Rule ${rule.ruleName} is not applied as it breaks the &quot; +
              &quot;distribution requirement of the query plan.&quot;)
            latestPlan
          }
        case _ =&gt; applied
      }
      planChangeLogger.logRule(rule.ruleName, latestPlan, result)
      result
    }
    planChangeLogger.logBatch(&quot;AQE Query Stage Optimization&quot;, plan, optimized)
    optimized
  }

  /**
   * Re-optimize and run physical planning on the current logical plan based on the latest stats.
   */
  private def reOptimize(logicalPlan: LogicalPlan): Option[(SparkPlan, LogicalPlan)] = {
    try {
      logicalPlan.invalidateStatsCache()
      val optimized = optimizer.execute(logicalPlan)
      val sparkPlan = context.session.sessionState.planner.plan(ReturnAnswer(optimized)).next()
      val newPlan = applyPhysicalRules(
        sparkPlan,
        preprocessingRules ++ queryStagePreparationRules,
        Some((planChangeLogger, &quot;AQE Replanning&quot;)))

      // When both enabling AQE and DPP, `PlanAdaptiveDynamicPruningFilters` rule will
      // add the `BroadcastExchangeExec` node manually in the DPP subquery,
      // not through `EnsureRequirements` rule. Therefore, when the DPP subquery is complicated
      // and need to be re-optimized, AQE also need to manually insert the `BroadcastExchangeExec`
      // node to prevent the loss of the `BroadcastExchangeExec` node in DPP subquery.
      // Here, we also need to avoid to insert the `BroadcastExchangeExec` node when the newPlan is
      // already the `BroadcastExchangeExec` plan after apply the `LogicalQueryStageStrategy` rule.
      val finalPlan = inputPlan match {
        case b: BroadcastExchangeLike
          if (!newPlan.isInstanceOf[BroadcastExchangeLike]) =&gt; b.withNewChildren(Seq(newPlan))
        case _ =&gt; newPlan
      }

      Some((finalPlan, optimized))
    } catch {
      case e: InvalidAQEPlanException[_] =&gt;
        logOnLevel(s&quot;Re-optimize - ${e.getMessage()}:\n${e.plan}&quot;)
        None
    }
  }

</code></pre>
<h2 id="querystageexec"><strong>QueryStageExec</strong><a class="headerlink" href="#querystageexec" title="Permanent link">&para;</a></h2>
<p>org.apache.spark.sql.execution.adaptive.QueryStageExec</p>
<pre><code>
/**
 * A query stage is an independent subgraph of the query plan. Query stage materializes its output
 * before proceeding with further operators of the query plan. The data statistics of the
 * materialized output can be used to optimize subsequent query stages.
 *
 * There are 2 kinds of query stages:
 *   1. Shuffle query stage. This stage materializes its output to shuffle files, and Spark launches
 *      another job to execute the further operators.
 *   2. Broadcast query stage. This stage materializes its output to an array in driver JVM. Spark
 *      broadcasts the array before executing the further operators.
 */
abstract class QueryStageExec extends LeafExecNode {

  @transient
  @volatile
  protected var _resultOption = new AtomicReference[Option[Any]](None)

  private[adaptive] def resultOption: AtomicReference[Option[Any]] = _resultOption
  def isMaterialized: Boolean = resultOption.get().isDefined

  /**
   * Compute the statistics of the query stage if executed, otherwise None.
   */
  def computeStats(): Option[Statistics] = if (isMaterialized) {
    val runtimeStats = getRuntimeStatistics
    val dataSize = runtimeStats.sizeInBytes.max(0)
    val numOutputRows = runtimeStats.rowCount.map(_.max(0))
    Some(Statistics(dataSize, numOutputRows, isRuntime = true))
  } else {
    None
  }


  /**
   * Materialize this query stage, to prepare for the execution, like submitting map stages,
   * broadcasting data, etc. The caller side can use the returned [[Future]] to wait until this
   * stage is ready.
   */
  def doMaterialize(): Future[Any]

====== materialize() is called by org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#getFinalPhysicalPlan  ===
  /**
   * Materialize this query stage, to prepare for the execution, like submitting map stages,
   * broadcasting data, etc. The caller side can use the returned [[Future]] to wait until this
   * stage is ready.
   */
  final def materialize(): Future[Any] = {  
    logDebug(s&quot;Materialize query stage ${this.getClass.getSimpleName}: $id&quot;)
    doMaterialize()
  }
=========================================================================================================  
</code></pre>
<pre><code>/**
 * A shuffle query stage whose child is a [[ShuffleExchangeLike]] or [[ReusedExchangeExec]].
 *
 * @param id the query stage id.
 * @param plan the underlying plan.
 * @param _canonicalized the canonicalized plan before applying query stage optimizer rules.
 */
case class ShuffleQueryStageExec(
    override val id: Int,
    override val plan: SparkPlan,
    override val _canonicalized: SparkPlan) extends QueryStageExec {

  @transient val shuffle = plan match {
    case s: ShuffleExchangeLike =&gt; s
    case ReusedExchangeExec(_, s: ShuffleExchangeLike) =&gt; s
    case _ =&gt;
      throw new IllegalStateException(s&quot;wrong plan for shuffle stage:\n ${plan.treeString}&quot;)
  }

  @transient private lazy val shuffleFuture = shuffle.submitShuffleJob

  override def doMaterialize(): Future[Any] = shuffleFuture


  override def getRuntimeStatistics: Statistics = shuffle.runtimeStatistics
</code></pre>
<pre><code>
/**
 * A broadcast query stage whose child is a [[BroadcastExchangeLike]] or [[ReusedExchangeExec]].
 *
 * @param id the query stage id.
 * @param plan the underlying plan.
 * @param _canonicalized the canonicalized plan before applying query stage optimizer rules.
 */
case class BroadcastQueryStageExec(
    override val id: Int,
    override val plan: SparkPlan,
    override val _canonicalized: SparkPlan) extends QueryStageExec {

  @transient val broadcast = plan match {
    case b: BroadcastExchangeLike =&gt; b
    case ReusedExchangeExec(_, b: BroadcastExchangeLike) =&gt; b
    case _ =&gt;
      throw new IllegalStateException(s&quot;wrong plan for broadcast stage:\n ${plan.treeString}&quot;)
  }

  override def doMaterialize(): Future[Any] = {
    broadcast.submitBroadcastJob
  }

  override def getRuntimeStatistics: Statistics = broadcast.runtimeStatistics
</code></pre>
<h2 id="reusequerystage"><strong>reuseQueryStage</strong><a class="headerlink" href="#reusequerystage" title="Permanent link">&para;</a></h2>
<p>org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#createQueryStages =&gt; reuseQueryStage  </p>
<p>org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#reuseQueryStage</p>
<pre><code>  private def reuseQueryStage(existing: QueryStageExec, exchange: Exchange): QueryStageExec = {
    val queryStage = existing.newReuseInstance(currentStageId, exchange.output)
    currentStageId += 1
    setLogicalLinkForNewQueryStage(queryStage, exchange)
    queryStage
  }
</code></pre>
<p>org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec#newReuseInstance</p>
<pre><code>  override def newReuseInstance(newStageId: Int, newOutput: Seq[Attribute]): QueryStageExec = {
    val reuse = BroadcastQueryStageExec(
      newStageId,
      ReusedExchangeExec(newOutput, broadcast),
      _canonicalized)
    reuse._resultOption = this._resultOption
    reuse
  }
</code></pre>
<p>org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec#newReuseInstance</p>
<pre><code>  override def newReuseInstance(newStageId: Int, newOutput: Seq[Attribute]): QueryStageExec = {
    val reuse = ShuffleQueryStageExec(
      newStageId,
      ReusedExchangeExec(newOutput, shuffle),
      _canonicalized)
    reuse._resultOption = this._resultOption
    reuse
  }
</code></pre>
<h2 id="adaptive-coalesce-partitions">Adaptive coalesce partitions<a class="headerlink" href="#adaptive-coalesce-partitions" title="Permanent link">&para;</a></h2>
<p>SQLConf</p>
<pre><code>  val COALESCE_PARTITIONS_ENABLED =
    buildConf(&quot;spark.sql.adaptive.coalescePartitions.enabled&quot;)
      .doc(s&quot;When true and '${ADAPTIVE_EXECUTION_ENABLED.key}' is true, Spark will coalesce &quot; +
        &quot;contiguous shuffle partitions according to the target size (specified by &quot; +
        s&quot;'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}'), to avoid too many small tasks.&quot;)
      .version(&quot;3.0.0&quot;)
      .booleanConf
      .createWithDefault(true)

  val COALESCE_PARTITIONS_PARALLELISM_FIRST =
    buildConf(&quot;spark.sql.adaptive.coalescePartitions.parallelismFirst&quot;)
      .doc(&quot;When true, Spark does not respect the target size specified by &quot; +
        s&quot;'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}' (default 64MB) when coalescing contiguous &quot; +
        &quot;shuffle partitions, but adaptively calculate the target size according to the default &quot; +
        &quot;parallelism of the Spark cluster. The calculated size is usually smaller than the &quot; +
        &quot;configured target size. This is to maximize the parallelism and avoid performance &quot; +
        &quot;regression when enabling adaptive query execution. It's recommended to set this config &quot; +
        &quot;to false and respect the configured target size.&quot;)
      .version(&quot;3.2.0&quot;)
      .booleanConf
      .createWithDefault(true)

  val COALESCE_PARTITIONS_MIN_PARTITION_SIZE =
    buildConf(&quot;spark.sql.adaptive.coalescePartitions.minPartitionSize&quot;)
      .doc(&quot;The minimum size of shuffle partitions after coalescing. This is useful when the &quot; +
        &quot;adaptively calculated target size is too small during partition coalescing.&quot;)
      .version(&quot;3.2.0&quot;)
      .bytesConf(ByteUnit.BYTE)
      .checkValue(_ &gt; 0, &quot;minPartitionSize must be positive&quot;)
      .createWithDefaultString(&quot;1MB&quot;)

  val COALESCE_PARTITIONS_MIN_PARTITION_NUM =
    buildConf(&quot;spark.sql.adaptive.coalescePartitions.minPartitionNum&quot;)
      .internal()
      .doc(&quot;(deprecated) The suggested (not guaranteed) minimum number of shuffle partitions &quot; +
        &quot;after coalescing. If not set, the default value is the default parallelism of the &quot; +
        &quot;Spark cluster. This configuration only has an effect when &quot; +
        s&quot;'${ADAPTIVE_EXECUTION_ENABLED.key}' and &quot; +
        s&quot;'${COALESCE_PARTITIONS_ENABLED.key}' are both true.&quot;)
      .version(&quot;3.0.0&quot;)
      .intConf
      .checkValue(_ &gt; 0, &quot;The minimum number of partitions must be positive.&quot;)
      .createOptional

  val COALESCE_PARTITIONS_INITIAL_PARTITION_NUM =
    buildConf(&quot;spark.sql.adaptive.coalescePartitions.initialPartitionNum&quot;)
      .doc(&quot;The initial number of shuffle partitions before coalescing. If not set, it equals to &quot; +
        s&quot;${SHUFFLE_PARTITIONS.key}. This configuration only has an effect when &quot; +
        s&quot;'${ADAPTIVE_EXECUTION_ENABLED.key}' and '${COALESCE_PARTITIONS_ENABLED.key}' &quot; +
        &quot;are both true.&quot;)
      .version(&quot;3.0.0&quot;)
      .intConf
      .checkValue(_ &gt; 0, &quot;The initial number of partitions must be positive.&quot;)
      .createOptional
</code></pre>
<p>org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec#queryStageOptimizerRules</p>
<pre><code>  // A list of physical optimizer rules to be applied to a new stage before its execution. These
  // optimizations should be stage-independent.
  @transient private val queryStageOptimizerRules: Seq[Rule[SparkPlan]] = Seq(
    PlanAdaptiveDynamicPruningFilters(this),
    ReuseAdaptiveSubquery(context.subqueryCache),
    OptimizeSkewInRebalancePartitions,

    ====== rule for coalesce partitions ==========
    CoalesceShufflePartitions(context.session),
    ==============================================
    // `OptimizeShuffleWithLocalRead` needs to make use of 'AQEShuffleReadExec.partitionSpecs'
    // added by `CoalesceShufflePartitions`, and must be executed after it.
    OptimizeShuffleWithLocalRead
  )
</code></pre>
<p>org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions</p>
<pre><code>/**
 * A rule to coalesce the shuffle partitions based on the map output statistics, which can
 * avoid many small reduce tasks that hurt performance.
 */
case class CoalesceShufflePartitions(session: SparkSession) extends AQEShuffleReadRule {
  override def apply(plan: SparkPlan): SparkPlan = {
    if (!conf.coalesceShufflePartitionsEnabled) {
      return plan
    }

    // Ideally, this rule should simply coalesce partitions w.r.t. the target size specified by
    // ADVISORY_PARTITION_SIZE_IN_BYTES (default 64MB). To avoid perf regression in AQE, this
    // rule by default tries to maximize the parallelism and set the target size to
    // `total shuffle size / Spark default parallelism`. In case the `Spark default parallelism`
    // is too big, this rule also respect the minimum partition size specified by
    // COALESCE_PARTITIONS_MIN_PARTITION_SIZE (default 1MB).
    // For history reason, this rule also need to support the config
    // COALESCE_PARTITIONS_MIN_PARTITION_NUM. We should remove this config in the future.
    val minNumPartitions = conf.getConf(SQLConf.COALESCE_PARTITIONS_MIN_PARTITION_NUM).getOrElse {
      if (conf.getConf(SQLConf.COALESCE_PARTITIONS_PARALLELISM_FIRST)) {
        // We fall back to Spark default parallelism if the minimum number of coalesced partitions
        // is not set, so to avoid perf regressions compared to no coalescing.
        session.sparkContext.defaultParallelism
      } else {
        // If we don't need to maximize the parallelism, we set `minPartitionNum` to 1, so that
        // the specified advisory partition size will be respected.
        1
      }
    }
    val advisoryTargetSize = conf.getConf(SQLConf.ADVISORY_PARTITION_SIZE_IN_BYTES)
    val minPartitionSize = if (Utils.isTesting) {
      // In the tests, we usually set the target size to a very small value that is even smaller
      // than the default value of the min partition size. Here we also adjust the min partition
      // size to be not larger than 20% of the target size, so that the tests don't need to set
      // both configs all the time to check the coalescing behavior.
      conf.getConf(SQLConf.COALESCE_PARTITIONS_MIN_PARTITION_SIZE).min(advisoryTargetSize / 5)
    } else {
      conf.getConf(SQLConf.COALESCE_PARTITIONS_MIN_PARTITION_SIZE)
    }

    // Sub-plans under the Union operator can be coalesced independently, so we can divide them
    // into independent &quot;coalesce groups&quot;, and all shuffle stages within each group have to be
    // coalesced together.
    val coalesceGroups = collectCoalesceGroups(plan)

    // Divide minimum task parallelism among coalesce groups according to their data sizes.
    val minNumPartitionsByGroup = if (coalesceGroups.length == 1) {
      Seq(math.max(minNumPartitions, 1))
    } else {
      val sizes =
        coalesceGroups.map(_.flatMap(_.shuffleStage.mapStats.map(_.bytesByPartitionId.sum)).sum)
      val totalSize = sizes.sum
      sizes.map { size =&gt;
        val num = if (totalSize &gt; 0) {
          math.round(minNumPartitions * 1.0 * size / totalSize)
        } else {
          minNumPartitions
        }
        math.max(num.toInt, 1)
      }
    }

    val specsMap = mutable.HashMap.empty[Int, Seq[ShufflePartitionSpec]]
    // Coalesce partitions for each coalesce group independently.
    coalesceGroups.zip(minNumPartitionsByGroup).foreach { case (shuffleStages, minNumPartitions) =&gt;
      val newPartitionSpecs = ShufflePartitionsUtil.coalescePartitions(
        shuffleStages.map(_.shuffleStage.mapStats),
        shuffleStages.map(_.partitionSpecs),
        advisoryTargetSize = advisoryTargetSize,
        minNumPartitions = minNumPartitions,
        minPartitionSize = minPartitionSize)

      if (newPartitionSpecs.nonEmpty) {
        shuffleStages.zip(newPartitionSpecs).map { case (stageInfo, partSpecs) =&gt;
          specsMap.put(stageInfo.shuffleStage.id, partSpecs)
        }
      }
    }

    if (specsMap.nonEmpty) {
      updateShuffleReads(plan, specsMap.toMap)
    } else {
      plan
    }
  }

  private def updateShuffleReads(
      plan: SparkPlan, specsMap: Map[Int, Seq[ShufflePartitionSpec]]): SparkPlan = plan match {
    // Even for shuffle exchange whose input RDD has 0 partition, we should still update its
    // `partitionStartIndices`, so that all the leaf shuffles in a stage have the same
    // number of output partitions.
    case ShuffleStageInfo(stage, _) =&gt;
      specsMap.get(stage.id).map { specs =&gt;
        AQEShuffleReadExec(stage, specs)
      }.getOrElse(plan)
    case other =&gt; other.mapChildren(updateShuffleReads(_, specsMap))
  }

</code></pre>
<p>org.apache.spark.sql.execution.adaptive.AQEShuffleReadRule</p>
<pre><code>
/**
 * A rule that may create [[AQEShuffleReadExec]] on top of [[ShuffleQueryStageExec]] and change the
 * plan output partitioning. The AQE framework will skip the rule if it leads to extra shuffles.
 */
trait AQEShuffleReadRule extends Rule[SparkPlan] {
  /**
   * Returns the list of [[ShuffleOrigin]]s supported by this rule.
   */
  protected def supportedShuffleOrigins: Seq[ShuffleOrigin]

  protected def isSupported(shuffle: ShuffleExchangeLike): Boolean = {
    supportedShuffleOrigins.contains(shuffle.shuffleOrigin)
  }
}
</code></pre>
<p>org.apache.spark.sql.execution.adaptive.AQEShuffleReadExec</p>
<pre><code>/**
 * A wrapper of shuffle query stage, which follows the given partition arrangement.
 *
 * @param child           It is usually `ShuffleQueryStageExec`, but can be the shuffle exchange
 *                        node during canonicalization.
 * @param partitionSpecs  The partition specs that defines the arrangement, requires at least one
 *                        partition.
 */
case class AQEShuffleReadExec private(
    child: SparkPlan,
    partitionSpecs: Seq[ShufflePartitionSpec]) extends UnaryExecNode {

  private def shuffleStage = child match {
    case stage: ShuffleQueryStageExec =&gt; Some(stage)
    case _ =&gt; None
  }

  private lazy val shuffleRDD: RDD[_] = {
    shuffleStage match {
      case Some(stage) =&gt;
        sendDriverMetrics()
        stage.shuffle.getShuffleRDD(partitionSpecs.toArray)
      case _ =&gt;
        throw new IllegalStateException(&quot;operating on canonicalized plan&quot;)
    }
  }

  override protected def doExecute(): RDD[InternalRow] = {
    shuffleRDD.asInstanceOf[RDD[InternalRow]]
  }
</code></pre>
<p>org.apache.spark.sql.execution.exchange.ShuffleExchangeExec#getShuffleRDD</p>
<pre><code>  override def getShuffleRDD(partitionSpecs: Array[ShufflePartitionSpec]): RDD[InternalRow] = {
    new ShuffledRowRDD(shuffleDependency, readMetrics, partitionSpecs)
  }
</code></pre></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
